{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BliS0CfKOQW",
        "outputId": "669eab36-6b08-4d4c-f3ec-adb948320663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-682badff4e2a>:373: UserWarning: Overwriting CSWin_64_12211_tiny_224 in registry with __main__.CSWin_64_12211_tiny_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_64_12211_tiny_224(pretrained=False, **kwargs):\n",
            "<ipython-input-9-682badff4e2a>:380: UserWarning: Overwriting CSWin_64_24322_small_224 in registry with __main__.CSWin_64_24322_small_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_64_24322_small_224(pretrained=False, **kwargs):\n",
            "<ipython-input-9-682badff4e2a>:387: UserWarning: Overwriting CSWin_96_24322_base_224 in registry with __main__.CSWin_96_24322_base_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_96_24322_base_224(pretrained=False, **kwargs):\n",
            "<ipython-input-9-682badff4e2a>:394: UserWarning: Overwriting CSWin_144_24322_large_224 in registry with __main__.CSWin_144_24322_large_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_144_24322_large_224(pretrained=False, **kwargs):\n",
            "<ipython-input-9-682badff4e2a>:403: UserWarning: Overwriting CSWin_96_24322_base_384 in registry with __main__.CSWin_96_24322_base_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_96_24322_base_384(pretrained=False, **kwargs):\n",
            "<ipython-input-9-682badff4e2a>:410: UserWarning: Overwriting CSWin_144_24322_large_384 in registry with __main__.CSWin_144_24322_large_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_144_24322_large_384(pretrained=False, **kwargs):\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "!pip install einops\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.helpers import load_pretrained\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    'cswin_224': _cfg(),\n",
        "    'cswin_384': _cfg(\n",
        "        crop_pct=1.0\n",
        "    ),\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class LePEAttention(nn.Module):\n",
        "    def __init__(self, dim, resolution, idx, split_size=7, dim_out=None, num_heads=8, attn_drop=0., proj_drop=0., qk_scale=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_out = dim_out or dim\n",
        "        self.resolution = resolution\n",
        "        self.split_size = split_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        if idx == -1:\n",
        "            H_sp, W_sp = self.resolution, self.resolution\n",
        "        elif idx == 0:\n",
        "            H_sp, W_sp = self.resolution, self.split_size\n",
        "        elif idx == 1:\n",
        "            W_sp, H_sp = self.resolution, self.split_size\n",
        "        else:\n",
        "            print (\"ERROR MODE\", idx)\n",
        "            exit(0)\n",
        "        self.H_sp = H_sp\n",
        "        self.W_sp = W_sp\n",
        "        stride = 1\n",
        "        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1,groups=dim)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "    def im2cswin(self, x):\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(np.sqrt(N))\n",
        "        x = x.transpose(-2,-1).contiguous().view(B, C, H, W)\n",
        "        x = img2windows(x, self.H_sp, self.W_sp)\n",
        "        x = x.reshape(-1, self.H_sp* self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
        "        return x\n",
        "\n",
        "    def get_lepe(self, x, func):\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(np.sqrt(N))\n",
        "        x = x.transpose(-2,-1).contiguous().view(B, C, H, W)\n",
        "\n",
        "        H_sp, W_sp = self.H_sp, self.W_sp\n",
        "        x = x.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous().reshape(-1, C, H_sp, W_sp) ### B', C, H', W'\n",
        "\n",
        "        lepe = func(x) ### B', C, H', W'\n",
        "        lepe = lepe.reshape(-1, self.num_heads, C // self.num_heads, H_sp * W_sp).permute(0, 1, 3, 2).contiguous()\n",
        "\n",
        "        x = x.reshape(-1, self.num_heads, C // self.num_heads, self.H_sp* self.W_sp).permute(0, 1, 3, 2).contiguous()\n",
        "        return x, lepe\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        x: B L C\n",
        "        \"\"\"\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        ### Img2Window\n",
        "        H = W = self.resolution\n",
        "        B, L, C = q.shape\n",
        "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
        "\n",
        "        q = self.im2cswin(q)\n",
        "        k = self.im2cswin(k)\n",
        "        v, lepe = self.get_lepe(v, self.get_v)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N\n",
        "        attn = nn.functional.softmax(attn, dim=-1, dtype=attn.dtype)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v) + lepe\n",
        "        x = x.transpose(1, 2).reshape(-1, self.H_sp* self.W_sp, C)  # B head N N @ B head N C\n",
        "\n",
        "        ### Window2Img\n",
        "        x = windows2img(x, self.H_sp, self.W_sp, H, W).view(B, -1, C)  # B H' W' C\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CSWinBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, reso, num_heads,\n",
        "                 split_size=7, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 last_stage=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.patches_resolution = reso\n",
        "        self.split_size = split_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.reso = reso\n",
        "\n",
        "\n",
        "        if self.patches_resolution == split_size:\n",
        "            last_stage = True\n",
        "        if last_stage:\n",
        "            self.branch_num = 1\n",
        "        else:\n",
        "            self.branch_num = 2\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(drop)\n",
        "\n",
        "        if last_stage:\n",
        "            self.attns = nn.ModuleList([\n",
        "                LePEAttention(\n",
        "                    dim, resolution=self.patches_resolution, idx = -1,\n",
        "                    split_size=split_size, num_heads=num_heads, dim_out=dim,\n",
        "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "                for i in range(self.branch_num)])\n",
        "        else:\n",
        "            self.attns = nn.ModuleList([\n",
        "                LePEAttention(\n",
        "                    dim//2, resolution=self.patches_resolution, idx = i,\n",
        "                    split_size=split_size, num_heads=num_heads//2, dim_out=dim//2,\n",
        "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "                for i in range(self.branch_num)])\n",
        "\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "\n",
        "        H = W = self.patches_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
        "        img = self.norm1(x)\n",
        "        qkv = self.qkv(img).reshape(B, -1, 3, C).permute(2, 0, 1, 3)\n",
        "\n",
        "        if self.branch_num == 2:\n",
        "            x1 = self.attns[0](qkv[:,:,:,:C//2])\n",
        "            x2 = self.attns[1](qkv[:,:,:,C//2:])\n",
        "            attened_x = torch.cat([x1,x2], dim=2)\n",
        "        else:\n",
        "            attened_x = self.attns[0](qkv)\n",
        "        attened_x = self.proj(attened_x)\n",
        "        x = x + self.drop_path(attened_x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "def img2windows(img, H_sp, W_sp):\n",
        "    \"\"\"\n",
        "    img: B C H W\n",
        "    \"\"\"\n",
        "    B, C, H, W = img.shape\n",
        "    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
        "    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp* W_sp, C)\n",
        "    return img_perm\n",
        "\n",
        "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n",
        "    \"\"\"\n",
        "    img_splits_hw: B' H W C\n",
        "    \"\"\"\n",
        "    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n",
        "\n",
        "    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n",
        "    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return img\n",
        "\n",
        "class Merge_Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(dim, dim_out, 3, 2, 1)\n",
        "        self.norm = norm_layer(dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, new_HW, C = x.shape\n",
        "        H = W = int(np.sqrt(new_HW))\n",
        "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
        "        x = self.conv(x)\n",
        "        B, C = x.shape[:2]\n",
        "        x = x.view(B, C, -1).transpose(-2, -1).contiguous()\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CSWinTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=96, depth=[2,2,6,2], split_size = [3,5,7],\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, use_chk=False):\n",
        "        super().__init__()\n",
        "        self.use_chk = use_chk\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        heads=num_heads\n",
        "\n",
        "        self.stage1_conv_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, 7, 4, 2),\n",
        "            Rearrange('b c h w -> b (h w) c', h = img_size//4, w = img_size//4),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "\n",
        "        curr_dim = embed_dim\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))]  # stochastic depth decay rule\n",
        "        self.stage1 = nn.ModuleList([\n",
        "            CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[0], reso=img_size//4, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[0],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth[0])])\n",
        "\n",
        "        self.merge1 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        self.stage2 = nn.ModuleList(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[1], reso=img_size//8, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[1],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:1])+i], norm_layer=norm_layer)\n",
        "            for i in range(depth[1])])\n",
        "\n",
        "        self.merge2 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        temp_stage3 = []\n",
        "        temp_stage3.extend(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[2], reso=img_size//16, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[2],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:2])+i], norm_layer=norm_layer)\n",
        "            for i in range(depth[2])])\n",
        "\n",
        "        self.stage3 = nn.ModuleList(temp_stage3)\n",
        "\n",
        "        self.merge3 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        self.stage4 = nn.ModuleList(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[3], reso=img_size//32, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[-1],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:-1])+i], norm_layer=norm_layer, last_stage=True)\n",
        "            for i in range(depth[-1])])\n",
        "\n",
        "        self.norm = norm_layer(curr_dim)\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(curr_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.head.weight, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        if self.num_classes != num_classes:\n",
        "            print ('reset head to', num_classes)\n",
        "            self.num_classes = num_classes\n",
        "            self.head = nn.Linear(self.out_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "            self.head = self.head.cuda()\n",
        "            trunc_normal_(self.head.weight, std=.02)\n",
        "            if self.head.bias is not None:\n",
        "                nn.init.constant_(self.head.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.stage1_conv_embed(x)\n",
        "        for blk in self.stage1:\n",
        "            if self.use_chk:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        for pre, blocks in zip([self.merge1, self.merge2, self.merge3],\n",
        "                               [self.stage2, self.stage3, self.stage4]):\n",
        "            x = pre(x)\n",
        "            for blk in blocks:\n",
        "                if self.use_chk:\n",
        "                    x = checkpoint.checkpoint(blk, x)\n",
        "                else:\n",
        "                    x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return torch.mean(x, dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=16):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict\n",
        "\n",
        "### 224 models\n",
        "\n",
        "@register_model\n",
        "def CSWin_64_12211_tiny_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=64, depth=[1,2,21,1],\n",
        "        split_size=[1,2,7,7], num_heads=[2,4,8,16], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_64_24322_small_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=64, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[2,4,8,16], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_96_24322_base_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=96, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[4,8,16,32], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_144_24322_large_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=144, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[6,12,24,24], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "### 384 models\n",
        "\n",
        "@register_model\n",
        "def CSWin_96_24322_base_384(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=96, depth=[2,4,32,2],\n",
        "        split_size=[1,2,12,12], num_heads=[4,8,16,32], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_384']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_144_24322_large_384(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=144, depth=[2,4,32,2],\n",
        "        split_size=[1,2,12,12], num_heads=[6,12,24,24], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_384']\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the saved model\n",
        "model = CSWin_64_12211_tiny_224()\n",
        "\n",
        "# Load the saved model\n",
        "model = torch.load(\"/content/drive/MyDrive/cswin_tiny_224.pth\", map_location=torch.device('cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVUMHE3kLmOO",
        "outputId": "d25d10b5-37d1-4e83-ee94-e180ffe174af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath, to_2tuple\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., expansion_ratio=3):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., expansion_ratio=3):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.expansion = expansion_ratio\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * self.expansion, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, atten=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "class ReAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    It is observed that similarity along same batch of data is extremely large.\n",
        "    Thus can reduce the bs dimension when calculating the attention map.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.,expansion_ratio = 3, apply_transform=True, transform_scale=False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.apply_transform = apply_transform\n",
        "\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        if apply_transform:\n",
        "            self.reatten_matrix = nn.Conv2d(self.num_heads,self.num_heads, 1, 1)\n",
        "            self.var_norm = nn.BatchNorm2d(self.num_heads)\n",
        "            self.qkv = nn.Linear(dim, dim * expansion_ratio, bias=qkv_bias)\n",
        "            self.reatten_scale = self.scale if transform_scale else 1.0\n",
        "        else:\n",
        "            self.qkv = nn.Linear(dim, dim * expansion_ratio, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    def forward(self, x, atten=None):\n",
        "        B, N, C = x.shape\n",
        "        # x = self.fc(x)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        if self.apply_transform:\n",
        "            attn = self.var_norm(self.reatten_matrix(attn)) * self.reatten_scale\n",
        "        attn_next = attn\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn_next\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, expansion=3,\n",
        "                 group = False, share = False, re_atten=False, bs=False, apply_transform=False,\n",
        "                 scale_adjustment=1.0, transform_scale=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.re_atten = re_atten\n",
        "\n",
        "        self.adjust_ratio = scale_adjustment\n",
        "        self.dim = dim\n",
        "        if  self.re_atten:\n",
        "            self.attn = ReAttention(\n",
        "                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
        "                expansion_ratio = expansion, apply_transform=apply_transform, transform_scale=transform_scale)\n",
        "        else:\n",
        "            self.attn = Attention(\n",
        "                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
        "                expansion_ratio = expansion)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "    def forward(self, x, atten=None):\n",
        "        if self.re_atten:\n",
        "            x_new, atten = self.attn(self.norm1(x * self.adjust_ratio), atten)\n",
        "            x = x + self.drop_path(x_new/self.adjust_ratio)\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x * self.adjust_ratio))) / self.adjust_ratio\n",
        "            return x, atten\n",
        "        else:\n",
        "            x_new, atten = self.attn(self.norm1(x), atten)\n",
        "            x= x + self.drop_path(x_new)\n",
        "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "            return x, atten\n",
        "\n",
        "class PatchEmbed_CNN(nn.Module):\n",
        "    \"\"\"\n",
        "        Following T2T, we use 3 layers of CNN for comparison with other methods.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768,spp=32):\n",
        "        super().__init__()\n",
        "\n",
        "        new_patch_size = to_2tuple(patch_size // 2)\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 112x112\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)  # 112x112\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.proj = nn.Conv2d(64, embed_dim, kernel_size=new_patch_size, stride=new_patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)  # [B, C, W, H]\n",
        "\n",
        "        return x\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "        Same embedding as timm lib.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HybridEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "        Same embedding as timm lib.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        assert isinstance(backbone, nn.Module)\n",
        "        img_size = to_2tuple(img_size)\n",
        "        self.img_size = img_size\n",
        "        self.backbone = backbone\n",
        "        if feature_size is None:\n",
        "            with torch.no_grad():\n",
        "                training = backbone.training\n",
        "                if training:\n",
        "                    backbone.eval()\n",
        "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
        "                feature_size = o.shape[-2:]\n",
        "                feature_dim = o.shape[1]\n",
        "                backbone.train(training)\n",
        "        else:\n",
        "            feature_size = to_2tuple(feature_size)\n",
        "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
        "        self.num_patches = feature_size[0] * feature_size[1]\n",
        "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)[-1]\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EkUTNsSTbQhL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code for DeepViT. The implementation has heavy reference to timm.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import pickle\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.helpers import load_pretrained\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.resnet import resnet26d, resnet50d\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    # patch models\n",
        "    'Deepvit_base_patch16_224_16B': _cfg(\n",
        "        url='',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'Deepvit_base_patch16_224_24B': _cfg(\n",
        "        url='',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'Deepvit_base_patch16_224_32B': _cfg(\n",
        "        url='',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'Deepvit_L_384': _cfg(\n",
        "        url='',\n",
        "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class DeepVisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, group = False, re_atten=True, cos_reg = False,\n",
        "                 use_cnn_embed=False, apply_transform=None, transform_scale=False, scale_adjustment=1.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        # use cosine similarity as a regularization term\n",
        "        self.cos_reg = cos_reg\n",
        "\n",
        "        if hybrid_backbone is not None:\n",
        "            self.patch_embed = HybridEmbed(\n",
        "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        else:\n",
        "            if use_cnn_embed:\n",
        "                self.patch_embed = PatchEmbed_CNN(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "            else:\n",
        "                self.patch_embed = PatchEmbed(\n",
        "                    img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        d = depth if isinstance(depth, int) else len(depth)\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, d)]  # stochastic depth decay rule\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, share=depth[i], num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, group = group,\n",
        "                re_atten=re_atten, apply_transform=apply_transform[i], transform_scale=transform_scale, scale_adjustment=scale_adjustment)\n",
        "            for i in range(len(depth))])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        if self.cos_reg:\n",
        "            atten_list = []\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        attn = None\n",
        "        for blk in self.blocks:\n",
        "            x, attn = blk(x, attn)\n",
        "            if self.cos_reg:\n",
        "                atten_list.append(attn)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        if self.cos_reg and self.training:\n",
        "            return x[:, 0], atten_list\n",
        "        else:\n",
        "            return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.cos_reg and self.training:\n",
        "            x, atten = self.forward_features(x)\n",
        "            x = self.head(x)\n",
        "            return x, atten\n",
        "        else:\n",
        "            x = self.forward_features(x)\n",
        "            x = self.head(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "@register_model\n",
        "def deepvit_patch16_224_re_attn_16b(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 0 + [True] * 16\n",
        "    model = DeepVisionTransformer(\n",
        "        patch_size=16, embed_dim=384, depth=[False] * 16, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_base_patch16_224_16B']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def deepvit_patch16_224_re_attn_24b(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 0 + [True] * 24\n",
        "    model = DeepVisionTransformer(\n",
        "        patch_size=16, embed_dim=384, depth=[False] * 24, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_base_patch16_224_24B']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def deepvit_patch16_224_re_attn_32b(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 0 + [True] * 32\n",
        "    model = DeepVisionTransformer(\n",
        "        patch_size=16, embed_dim=384, depth=[False] * 32, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_base_patch16_224_32B']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "@register_model\n",
        "def deepvit_S(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 11 + [True] * 5\n",
        "    model = DeepVisionTransformer(\n",
        "        patch_size=16, embed_dim=396, depth=[False] * 16, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  transform_scale=True, use_cnn_embed = True, scale_adjustment=0.5, **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_base_patch16_224_32B']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "@register_model\n",
        "def deepvit_L(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 20 + [True] * 12\n",
        "    model = DeepVisionTransformer(\n",
        "        patch_size=16, embed_dim=420, depth=[False] * 32, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), use_cnn_embed = True, scale_adjustment=0.5, **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_base_patch16_224_32B']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def deepvit_L_384(pretrained=False, **kwargs):\n",
        "    apply_transform = [False] * 20 + [True] * 12\n",
        "    model = DeepVisionTransformer(\n",
        "        img_size=384, patch_size=16, embed_dim=420, depth=[False] * 32, apply_transform=apply_transform, num_heads=12, mlp_ratio=3, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), use_cnn_embed = True, scale_adjustment=0.5, **kwargs)\n",
        "    # We following the same settings for original ViT\n",
        "    model.default_cfg = default_cfgs['Deepvit_L_384']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhJGBhmLbV6M",
        "outputId": "9d5ecf19-e3eb-4bd3-a655-ec9c20ca02a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-751cf56dc5c5>:153: UserWarning: Overwriting deepvit_patch16_224_re_attn_16b in registry with __main__.deepvit_patch16_224_re_attn_16b. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_patch16_224_re_attn_16b(pretrained=False, **kwargs):\n",
            "<ipython-input-12-751cf56dc5c5>:166: UserWarning: Overwriting deepvit_patch16_224_re_attn_24b in registry with __main__.deepvit_patch16_224_re_attn_24b. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_patch16_224_re_attn_24b(pretrained=False, **kwargs):\n",
            "<ipython-input-12-751cf56dc5c5>:179: UserWarning: Overwriting deepvit_patch16_224_re_attn_32b in registry with __main__.deepvit_patch16_224_re_attn_32b. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_patch16_224_re_attn_32b(pretrained=False, **kwargs):\n",
            "<ipython-input-12-751cf56dc5c5>:191: UserWarning: Overwriting deepvit_S in registry with __main__.deepvit_S. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_S(pretrained=False, **kwargs):\n",
            "<ipython-input-12-751cf56dc5c5>:203: UserWarning: Overwriting deepvit_L in registry with __main__.deepvit_L. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_L(pretrained=False, **kwargs):\n",
            "<ipython-input-12-751cf56dc5c5>:216: UserWarning: Overwriting deepvit_L_384 in registry with __main__.deepvit_L_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def deepvit_L_384(pretrained=False, **kwargs):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# CSWin Transformer\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# Licensed under the MIT License.\n",
        "# written By Xiaoyi Dong\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.helpers import load_pretrained\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    'cswin_224': _cfg(),\n",
        "    'cswin_384': _cfg(\n",
        "        crop_pct=1.0\n",
        "    ),\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class LePEAttention(nn.Module):\n",
        "    def __init__(self, dim, resolution, idx, split_size=7, dim_out=None, num_heads=8, attn_drop=0., proj_drop=0., qk_scale=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dim_out = dim_out or dim\n",
        "        self.resolution = resolution\n",
        "        self.split_size = split_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        if idx == -1:\n",
        "            H_sp, W_sp = self.resolution, self.resolution\n",
        "        elif idx == 0:\n",
        "            H_sp, W_sp = self.resolution, self.split_size\n",
        "        elif idx == 1:\n",
        "            W_sp, H_sp = self.resolution, self.split_size\n",
        "        else:\n",
        "            print (\"ERROR MODE\", idx)\n",
        "            exit(0)\n",
        "        self.H_sp = H_sp\n",
        "        self.W_sp = W_sp\n",
        "        stride = 1\n",
        "        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1,groups=dim)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "    def im2cswin(self, x):\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(np.sqrt(N))\n",
        "        x = x.transpose(-2,-1).contiguous().view(B, C, H, W)\n",
        "        x = img2windows(x, self.H_sp, self.W_sp)\n",
        "        x = x.reshape(-1, self.H_sp* self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
        "        return x\n",
        "\n",
        "    def get_lepe(self, x, func):\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(np.sqrt(N))\n",
        "        x = x.transpose(-2,-1).contiguous().view(B, C, H, W)\n",
        "\n",
        "        H_sp, W_sp = self.H_sp, self.W_sp\n",
        "        x = x.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous().reshape(-1, C, H_sp, W_sp) ### B', C, H', W'\n",
        "\n",
        "        lepe = func(x) ### B', C, H', W'\n",
        "        lepe = lepe.reshape(-1, self.num_heads, C // self.num_heads, H_sp * W_sp).permute(0, 1, 3, 2).contiguous()\n",
        "\n",
        "        x = x.reshape(-1, self.num_heads, C // self.num_heads, self.H_sp* self.W_sp).permute(0, 1, 3, 2).contiguous()\n",
        "        return x, lepe\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        x: B L C\n",
        "        \"\"\"\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        ### Img2Window\n",
        "        H = W = self.resolution\n",
        "        B, L, C = q.shape\n",
        "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
        "\n",
        "        q = self.im2cswin(q)\n",
        "        k = self.im2cswin(k)\n",
        "        v, lepe = self.get_lepe(v, self.get_v)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N\n",
        "        attn = nn.functional.softmax(attn, dim=-1, dtype=attn.dtype)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v) + lepe\n",
        "        x = x.transpose(1, 2).reshape(-1, self.H_sp* self.W_sp, C)  # B head N N @ B head N C\n",
        "\n",
        "        ### Window2Img\n",
        "        x = windows2img(x, self.H_sp, self.W_sp, H, W).view(B, -1, C)  # B H' W' C\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CSWinBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, reso, num_heads,\n",
        "                 split_size=7, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 last_stage=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.patches_resolution = reso\n",
        "        self.split_size = split_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.norm1 = norm_layer(dim)\n",
        "\n",
        "        if self.patches_resolution == split_size:\n",
        "            last_stage = True\n",
        "        if last_stage:\n",
        "            self.branch_num = 1\n",
        "        else:\n",
        "            self.branch_num = 2\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(drop)\n",
        "\n",
        "        if last_stage:\n",
        "            self.attns = nn.ModuleList([\n",
        "                LePEAttention(\n",
        "                    dim, resolution=self.patches_resolution, idx = -1,\n",
        "                    split_size=split_size, num_heads=num_heads, dim_out=dim,\n",
        "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "                for i in range(self.branch_num)])\n",
        "        else:\n",
        "            self.attns = nn.ModuleList([\n",
        "                LePEAttention(\n",
        "                    dim//2, resolution=self.patches_resolution, idx = i,\n",
        "                    split_size=split_size, num_heads=num_heads//2, dim_out=dim//2,\n",
        "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "                for i in range(self.branch_num)])\n",
        "\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "\n",
        "        H = W = self.patches_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
        "        img = self.norm1(x)\n",
        "        qkv = self.qkv(img).reshape(B, -1, 3, C).permute(2, 0, 1, 3)\n",
        "\n",
        "        if self.branch_num == 2:\n",
        "            x1 = self.attns[0](qkv[:,:,:,:C//2])\n",
        "            x2 = self.attns[1](qkv[:,:,:,C//2:])\n",
        "            attened_x = torch.cat([x1,x2], dim=2)\n",
        "        else:\n",
        "            attened_x = self.attns[0](qkv)\n",
        "        attened_x = self.proj(attened_x)\n",
        "        x = x + self.drop_path(attened_x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "def img2windows(img, H_sp, W_sp):\n",
        "    \"\"\"\n",
        "    img: B C H W\n",
        "    \"\"\"\n",
        "    B, C, H, W = img.shape\n",
        "    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
        "    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp* W_sp, C)\n",
        "    return img_perm\n",
        "\n",
        "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n",
        "    \"\"\"\n",
        "    img_splits_hw: B' H W C\n",
        "    \"\"\"\n",
        "    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n",
        "\n",
        "    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n",
        "    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return img\n",
        "\n",
        "class Merge_Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(dim, dim_out, 3, 2, 1)\n",
        "        self.norm = norm_layer(dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, new_HW, C = x.shape\n",
        "        H = W = int(np.sqrt(new_HW))\n",
        "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
        "        x = self.conv(x)\n",
        "        B, C = x.shape[:2]\n",
        "        x = x.view(B, C, -1).transpose(-2, -1).contiguous()\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CSWinTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=96, depth=[2,2,6,2], split_size = [3,5,7],\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, use_chk=False):\n",
        "        super().__init__()\n",
        "        self.use_chk = use_chk\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        heads=num_heads\n",
        "\n",
        "        self.stage1_conv_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, 7, 4, 2),\n",
        "            Rearrange('b c h w -> b (h w) c', h = img_size//4, w = img_size//4),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "\n",
        "        curr_dim = embed_dim\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))]  # stochastic depth decay rule\n",
        "        self.stage1 = nn.ModuleList([\n",
        "            CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[0], reso=img_size//4, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[0],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth[0])])\n",
        "\n",
        "        self.merge1 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        self.stage2 = nn.ModuleList(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[1], reso=img_size//8, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[1],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:1])+i], norm_layer=norm_layer)\n",
        "            for i in range(depth[1])])\n",
        "\n",
        "        self.merge2 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        temp_stage3 = []\n",
        "        temp_stage3.extend(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[2], reso=img_size//16, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[2],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:2])+i], norm_layer=norm_layer)\n",
        "            for i in range(depth[2])])\n",
        "\n",
        "        self.stage3 = nn.ModuleList(temp_stage3)\n",
        "\n",
        "        self.merge3 = Merge_Block(curr_dim, curr_dim*2)\n",
        "        curr_dim = curr_dim*2\n",
        "        self.stage4 = nn.ModuleList(\n",
        "            [CSWinBlock(\n",
        "                dim=curr_dim, num_heads=heads[3], reso=img_size//32, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[-1],\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[np.sum(depth[:-1])+i], norm_layer=norm_layer, last_stage=True)\n",
        "            for i in range(depth[-1])])\n",
        "\n",
        "        self.norm = norm_layer(curr_dim)\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(curr_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.head.weight, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        if self.num_classes != num_classes:\n",
        "            print ('reset head to', num_classes)\n",
        "            self.num_classes = num_classes\n",
        "            self.head = nn.Linear(self.out_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "            self.head = self.head.cuda()\n",
        "            trunc_normal_(self.head.weight, std=.02)\n",
        "            if self.head.bias is not None:\n",
        "                nn.init.constant_(self.head.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.stage1_conv_embed(x)\n",
        "        for blk in self.stage1:\n",
        "            if self.use_chk:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        for pre, blocks in zip([self.merge1, self.merge2, self.merge3],\n",
        "                               [self.stage2, self.stage3, self.stage4]):\n",
        "            x = pre(x)\n",
        "            for blk in blocks:\n",
        "                if self.use_chk:\n",
        "                    x = checkpoint.checkpoint(blk, x)\n",
        "                else:\n",
        "                    x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return torch.mean(x, dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=16):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict\n",
        "\n",
        "### 224 models\n",
        "\n",
        "@register_model\n",
        "def CSWin_64_12211_tiny_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=64, depth=[1,2,21,1],\n",
        "        split_size=[1,2,7,7], num_heads=[2,4,8,16], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_64_24322_small_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=64, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[2,4,8,16], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_96_24322_base_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=96, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[4,8,16,32], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_144_24322_large_224(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=144, depth=[2,4,32,2],\n",
        "        split_size=[1,2,7,7], num_heads=[6,12,24,24], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_224']\n",
        "    return model\n",
        "\n",
        "### 384 models\n",
        "\n",
        "@register_model\n",
        "def CSWin_96_24322_base_384(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=96, depth=[2,4,32,2],\n",
        "        split_size=[1,2,12,12], num_heads=[4,8,16,32], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_384']\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def CSWin_144_24322_large_384(pretrained=False, **kwargs):\n",
        "    model = CSWinTransformer(patch_size=4, embed_dim=144, depth=[2,4,32,2],\n",
        "        split_size=[1,2,12,12], num_heads=[6,12,24,24], mlp_ratio=4., **kwargs)\n",
        "    model.default_cfg = default_cfgs['cswin_384']\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6c8R3PfPfW",
        "outputId": "904f6078-5d8a-45f5-af9f-2662c5dca56b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-20936ac6e71c>:376: UserWarning: Overwriting CSWin_64_12211_tiny_224 in registry with __main__.CSWin_64_12211_tiny_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_64_12211_tiny_224(pretrained=False, **kwargs):\n",
            "<ipython-input-14-20936ac6e71c>:383: UserWarning: Overwriting CSWin_64_24322_small_224 in registry with __main__.CSWin_64_24322_small_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_64_24322_small_224(pretrained=False, **kwargs):\n",
            "<ipython-input-14-20936ac6e71c>:390: UserWarning: Overwriting CSWin_96_24322_base_224 in registry with __main__.CSWin_96_24322_base_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_96_24322_base_224(pretrained=False, **kwargs):\n",
            "<ipython-input-14-20936ac6e71c>:397: UserWarning: Overwriting CSWin_144_24322_large_224 in registry with __main__.CSWin_144_24322_large_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_144_24322_large_224(pretrained=False, **kwargs):\n",
            "<ipython-input-14-20936ac6e71c>:406: UserWarning: Overwriting CSWin_96_24322_base_384 in registry with __main__.CSWin_96_24322_base_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_96_24322_base_384(pretrained=False, **kwargs):\n",
            "<ipython-input-14-20936ac6e71c>:413: UserWarning: Overwriting CSWin_144_24322_large_384 in registry with __main__.CSWin_144_24322_large_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def CSWin_144_24322_large_384(pretrained=False, **kwargs):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/cswin_tiny_224.pth\", map_location=torch.device('cuda'))  # Assuming the model file is in the same directory and named cswin_tiny_224.pth\n",
        "\n",
        "# Create an instance of the model\n",
        "model = CSWin_64_12211_tiny_224()  # Assuming CSWinTransformer is your model class\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "_UON736CWQar",
        "outputId": "aa3396fd-bdff-455a-a73b-1fa70b810f9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for CSWinTransformer:\n\tMissing key(s) in state_dict: \"stage1_conv_embed.0.weight\", \"stage1_conv_embed.0.bias\", \"stage1_conv_embed.2.weight\", \"stage1_conv_embed.2.bias\", \"stage1.0.qkv.weight\", \"stage1.0.qkv.bias\", \"stage1.0.norm1.weight\", \"stage1.0.norm1.bias\", \"stage1.0.proj.weight\", \"stage1.0.proj.bias\", \"stage1.0.attns.0.get_v.weight\", \"stage1.0.attns.0.get_v.bias\", \"stage1.0.attns.1.get_v.weight\", \"stage1.0.attns.1.get_v.bias\", \"stage1.0.mlp.fc1.weight\", \"stage1.0.mlp.fc1.bias\", \"stage1.0.mlp.fc2.weight\", \"stage1.0.mlp.fc2.bias\", \"stage1.0.norm2.weight\", \"stage1.0.norm2.bias\", \"merge1.conv.weight\", \"merge1.conv.bias\", \"merge1.norm.weight\", \"merge1.norm.bias\", \"stage2.0.qkv.weight\", \"stage2.0.qkv.bias\", \"stage2.0.norm1.weight\", \"stage2.0.norm1.bias\", \"stage2.0.proj.weight\", \"stage2.0.proj.bias\", \"stage2.0.attns.0.get_v.weight\", \"stage2.0.attns.0.get_v.bias\", \"stage2.0.attns.1.get_v.weight\", \"stage2.0.attns.1.get_v.bias\", \"stage2.0.mlp.fc1.weight\", \"stage2.0.mlp.fc1.bias\", \"stage2.0.mlp.fc2.weight\", \"stage2.0.mlp.fc2.bias\", \"stage2.0.norm2.weight\", \"stage2.0.norm2.bias\", \"stage2.1.qkv.weight\", \"stage2.1.qkv.bias\", \"stage2.1.norm1.weight\", \"stage2.1.norm1.bias\", \"stage2.1.proj.weight\", \"stage2.1.proj.bias\", \"stage2.1.attns.0.get_v.weight\", \"stage2.1.attns.0.get_v.bias\", \"stage2.1.attns.1.get_v.weight\", \"stage2.1.attns.1.get_v.bias\", \"stage2.1.mlp.fc1.weight\", \"stage2.1.mlp.fc1.bias\", \"stage2.1.mlp.fc2.weight\", \"stage2.1.mlp.fc2.bias\", \"stage2.1.norm2.weight\", \"stage2.1.norm2.bias\", \"merge2.conv.weight\", \"merge2.conv.bias\", \"merge2.norm.weight\", \"merge2.norm.bias\", \"stage3.0.qkv.weight\", \"stage3.0.qkv.bias\", \"stage3.0.norm1.weight\", \"stage3.0.norm1.bias\", \"stage3.0.proj.weight\", \"stage3.0.proj.bias\", \"stage3.0.attns.0.get_v.weight\", \"stage3.0.attns.0.get_v.bias\", \"stage3.0.attns.1.get_v.weight\", \"stage3.0.attns.1.get_v.bias\", \"stage3.0.mlp.fc1.weight\", \"stage3.0.mlp.fc1.bias\", \"stage3.0.mlp.fc2.weight\", \"stage3.0.mlp.fc2.bias\", \"stage3.0.norm2.weight\", \"stage3.0.norm2.bias\", \"stage3.1.qkv.weight\", \"stage3.1.qkv.bias\", \"stage3.1.norm1.weight\", \"stage3.1.norm1.bias\", \"stage3.1.proj.weight\", \"stage3.1.proj.bias\", \"stage3.1.attns.0.get_v.weight\", \"stage3.1.attns.0.get_v.bias\", \"stage3.1.attns.1.get_v.weight\", \"stage3.1.attns.1.get_v.bias\", \"stage3.1.mlp.fc1.weight\", \"stage3.1.mlp.fc1.bias\", \"stage3.1.mlp.fc2.weight\", \"stage3.1.mlp.fc2.bias\", \"stage3.1.norm2.weight\", \"stage3.1.norm2.bias\", \"stage3.2.qkv.weight\", \"stage3.2.qkv.bias\", \"stage3.2.norm1.weight\", \"stage3.2.norm1.bias\", \"stage3.2.proj.weight\", \"stage3.2.proj.bias\", \"stage3.2.attns.0.get_v.weight\", \"stage3.2.attns.0.get_v.bias\", \"stage3.2.attns.1.get_v.weight\", \"stage3.2.attns.1.get_v.bias\", \"stage3.2.mlp.fc1.weight\", \"stage3.2.mlp.fc1.bias\", \"stage3.2.mlp.fc2.weight\", \"stage3.2.mlp.fc2.bias\", \"stage3.2.norm2.weight\", \"stage3.2.norm2.bias\", \"stage3.3.qkv.weight\", \"stage3.3.qkv.bias\", \"stage3.3.norm1.weight\", \"stage3.3.norm1.bias\", \"stage3.3.proj.weight\", \"stage3.3.proj.bias\", \"stage3.3.attns.0.get_v.weight\", \"stage3.3.attns.0.get_v.bias\", \"stage3.3.attns.1.get_v.weight\", \"stage3.3.attns.1.get_v.bias\", \"stage3.3.mlp.fc1.weight\", \"stage3.3.mlp.fc1.bias\", \"stage3.3.mlp.fc2.weight\", \"stage3.3.mlp.fc2.bias\", \"stage3.3.norm2.weight\", \"stage3.3.norm2.bias\", \"stage3.4.qkv.weight\", \"stage3.4.qkv.bias\", \"stage3.4.norm1.weight\", \"stage3.4.norm1.bias\", \"stage3.4.proj.weight\", \"stage3.4.proj.bias\", \"stage3.4.attns.0.get_v.weight\", \"stage3.4.attns.0.get_v.bias\", \"stage3.4.attns.1.get_v.weight\", \"stage3.4.attns.1.get_v.bias\", \"stage3.4.mlp.fc1.weight\", \"stage3.4.mlp.fc1.bias\", \"stage3.4.mlp.fc2.weight\", \"stage3.4.mlp.fc2.bias\", \"stage3.4.norm2.weight\", \"stage3.4.norm2.bias\", \"stage3.5.qkv.weight\", \"stage3.5.qkv.bias\", \"stage3.5.norm1.weight\", \"stage3.5.norm1.bias\", \"stage3.5.proj.weight\", \"stage3.5.proj.bias\", \"stage3.5.attns.0.get_v.weight\", \"stage3.5.attns.0.get_v.bias\", \"stage3.5.attns.1.get_v.weight\", \"stage3.5.attns.1.get_v.bias\", \"stage3.5.mlp.fc1.weight\", \"stage3.5.mlp.fc1.bias\", \"stage3.5.mlp.fc2.weight\", \"stage3.5.mlp.fc2.bias\", \"stage3.5.norm2.weight\", \"stage3.5.norm2.bias\", \"stage3.6.qkv.weight\", \"stage3.6.qkv.bias\", \"stage3.6.norm1.weight\", \"stage3.6.norm1.bias\", \"stage3.6.proj.weight\", \"stage3.6.proj.bias\", \"stage3.6.attns.0.get_v.weight\", \"stage3.6.attns.0.get_v.bias\", \"stage3.6.attns.1.get_v.weight\", \"stage3.6.attns.1.get_v.bias\", \"stage3.6.mlp.fc1.weight\", \"stage3.6.mlp.fc1.bias\", \"stage3.6.mlp.fc2.weight\", \"stage3.6.mlp.fc2.bias\", \"stage3.6.norm2.weight\", \"stage3.6.norm2.bias\", \"stage3.7.qkv.weight\", \"stage3.7.qkv.bias\", \"stage3.7.norm1.weight\", \"stage3.7.norm1.bias\", \"stage3.7.proj.weight\", \"stage3.7.proj.bias\", \"stage3.7.attns.0.get_v.weight\", \"stage3.7.attns.0.get_v.bias\", \"stage3.7.attns.1.get_v.weight\", \"stage3.7.attns.1.get_v.bias\", \"stage3.7.mlp.fc1.weight\", \"stage3.7.mlp.fc1.bias\", \"stage3.7.mlp.fc2.weight\", \"stage3.7.mlp.fc2.bias\", \"stage3.7.norm2.weight\", \"stage3.7.norm2.bias\", \"stage3.8.qkv.weight\", \"stage3.8.qkv.bias\", \"stage3.8.norm1.weight\", \"stage3.8.norm1.bias\", \"stage3.8.proj.weight\", \"stage3.8.proj.bias\", \"stage3.8.attns.0.get_v.weight\", \"stage3.8.attns.0.get_v.bias\", \"stage3.8.attns.1.get_v.weight\", \"stage3.8.attns.1.get_v.bias\", \"stage3.8.mlp.fc1.weight\", \"stage3.8.mlp.fc1.bias\", \"stage3.8.mlp.fc2.weight\", \"stage3.8.mlp.fc2.bias\", \"stage3.8.norm2.weight\", \"stage3.8.norm2.bias\", \"stage3.9.qkv.weight\", \"stage3.9.qkv.bias\", \"stage3.9.norm1.weight\", \"stage3.9.norm1.bias\", \"stage3.9.proj.weight\", \"stage3.9.proj.bias\", \"stage3.9.attns.0.get_v.weight\", \"stage3.9.attns.0.get_v.bias\", \"stage3.9.attns.1.get_v.weight\", \"stage3.9.attns.1.get_v.bias\", \"stage3.9.mlp.fc1.weight\", \"stage3.9.mlp.fc1.bias\", \"stage3.9.mlp.fc2.weight\", \"stage3.9.mlp.fc2.bias\", \"stage3.9.norm2.weight\", \"stage3.9.norm2.bias\", \"stage3.10.qkv.weight\", \"stage3.10.qkv.bias\", \"stage3.10.norm1.weight\", \"stage3.10.norm1.bias\", \"stage3.10.proj.weight\", \"stage3.10.proj.bias\", \"stage3.10.attns.0.get_v.weight\", \"stage3.10.attns.0.get_v.bias\", \"stage3.10.attns.1.get_v.weight\", \"stage3.10.attns.1.get_v.bias\", \"stage3.10.mlp.fc1.weight\", \"stage3.10.mlp.fc1.bias\", \"stage3.10.mlp.fc2.weight\", \"stage3.10.mlp.fc2.bias\", \"stage3.10.norm2.weight\", \"stage3.10.norm2.bias\", \"stage3.11.qkv.weight\", \"stage3.11.qkv.bias\", \"stage3.11.norm1.weight\", \"stage3.11.norm1.bias\", \"stage3.11.proj.weight\", \"stage3.11.proj.bias\", \"stage3.11.attns.0.get_v.weight\", \"stage3.11.attns.0.get_v.bias\", \"stage3.11.attns.1.get_v.weight\", \"stage3.11.attns.1.get_v.bias\", \"stage3.11.mlp.fc1.weight\", \"stage3.11.mlp.fc1.bias\", \"stage3.11.mlp.fc2.weight\", \"stage3.11.mlp.fc2.bias\", \"stage3.11.norm2.weight\", \"stage3.11.norm2.bias\", \"stage3.12.qkv.weight\", \"stage3.12.qkv.bias\", \"stage3.12.norm1.weight\", \"stage3.12.norm1.bias\", \"stage3.12.proj.weight\", \"stage3.12.proj.bias\", \"stage3.12.attns.0.get_v.weight\", \"stage3.12.attns.0.get_v.bias\", \"stage3.12.attns.1.get_v.weight\", \"stage3.12.attns.1.get_v.bias\", \"stage3.12.mlp.fc1.weight\", \"stage3.12.mlp.fc1.bias\", \"stage3.12.mlp.fc2.weight\", \"stage3.12.mlp.fc2.bias\", \"stage3.12.norm2.weight\", \"stage3.12.norm2.bias\", \"stage3.13.qkv.weight\", \"stage3.13.qkv.bias\", \"stage3.13.norm1.weight\", \"stage3.13.norm1.bias\", \"stage3.13.proj.weight\", \"stage3.13.proj.bias\", \"stage3.13.attns.0.get_v.weight\", \"stage3.13.attns.0.get_v.bias\", \"stage3.13.attns.1.get_v.weight\", \"stage3.13.attns.1.get_v.bias\", \"stage3.13.mlp.fc1.weight\", \"stage3.13.mlp.fc1.bias\", \"stage3.13.mlp.fc2.weight\", \"stage3.13.mlp.fc2.bias\", \"stage3.13.norm2.weight\", \"stage3.13.norm2.bias\", \"stage3.14.qkv.weight\", \"stage3.14.qkv.bias\", \"stage3.14.norm1.weight\", \"stage3.14.norm1.bias\", \"stage3.14.proj.weight\", \"stage3.14.proj.bias\", \"stage3.14.attns.0.get_v.weight\", \"stage3.14.attns.0.get_v.bias\", \"stage3.14.attns.1.get_v.weight\", \"stage3.14.attns.1.get_v.bias\", \"stage3.14.mlp.fc1.weight\", \"stage3.14.mlp.fc1.bias\", \"stage3.14.mlp.fc2.weight\", \"stage3.14.mlp.fc2.bias\", \"stage3.14.norm2.weight\", \"stage3.14.norm2.bias\", \"stage3.15.qkv.weight\", \"stage3.15.qkv.bias\", \"stage3.15.norm1.weight\", \"stage3.15.norm1.bias\", \"stage3.15.proj.weight\", \"stage3.15.proj.bias\", \"stage3.15.attns.0.get_v.weight\", \"stage3.15.attns.0.get_v.bias\", \"stage3.15.attns.1.get_v.weight\", \"stage3.15.attns.1.get_v.bias\", \"stage3.15.mlp.fc1.weight\", \"stage3.15.mlp.fc1.bias\", \"stage3.15.mlp.fc2.weight\", \"stage3.15.mlp.fc2.bias\", \"stage3.15.norm2.weight\", \"stage3.15.norm2.bias\", \"stage3.16.qkv.weight\", \"stage3.16.qkv.bias\", \"stage3.16.norm1.weight\", \"stage3.16.norm1.bias\", \"stage3.16.proj.weight\", \"stage3.16.proj.bias\", \"stage3.16.attns.0.get_v.weight\", \"stage3.16.attns.0.get_v.bias\", \"stage3.16.attns.1.get_v.weight\", \"stage3.16.attns.1.get_v.bias\", \"stage3.16.mlp.fc1.weight\", \"stage3.16.mlp.fc1.bias\", \"stage3.16.mlp.fc2.weight\", \"stage3.16.mlp.fc2.bias\", \"stage3.16.norm2.weight\", \"stage3.16.norm2.bias\", \"stage3.17.qkv.weight\", \"stage3.17.qkv.bias\", \"stage3.17.norm1.weight\", \"stage3.17.norm1.bias\", \"stage3.17.proj.weight\", \"stage3.17.proj.bias\", \"stage3.17.attns.0.get_v.weight\", \"stage3.17.attns.0.get_v.bias\", \"stage3.17.attns.1.get_v.weight\", \"stage3.17.attns.1.get_v.bias\", \"stage3.17.mlp.fc1.weight\", \"stage3.17.mlp.fc1.bias\", \"stage3.17.mlp.fc2.weight\", \"stage3.17.mlp.fc2.bias\", \"stage3.17.norm2.weight\", \"stage3.17.norm2.bias\", \"stage3.18.qkv.weight\", \"stage3.18.qkv.bias\", \"stage3.18.norm1.weight\", \"stage3.18.norm1.bias\", \"stage3.18.proj.weight\", \"stage3.18.proj.bias\", \"stage3.18.attns.0.get_v.weight\", \"stage3.18.attns.0.get_v.bias\", \"stage3.18.attns.1.get_v.weight\", \"stage3.18.attns.1.get_v.bias\", \"stage3.18.mlp.fc1.weight\", \"stage3.18.mlp.fc1.bias\", \"stage3.18.mlp.fc2.weight\", \"stage3.18.mlp.fc2.bias\", \"stage3.18.norm2.weight\", \"stage3.18.norm2.bias\", \"stage3.19.qkv.weight\", \"stage3.19.qkv.bias\", \"stage3.19.norm1.weight\", \"stage3.19.norm1.bias\", \"stage3.19.proj.weight\", \"stage3.19.proj.bias\", \"stage3.19.attns.0.get_v.weight\", \"stage3.19.attns.0.get_v.bias\", \"stage3.19.attns.1.get_v.weight\", \"stage3.19.attns.1.get_v.bias\", \"stage3.19.mlp.fc1.weight\", \"stage3.19.mlp.fc1.bias\", \"stage3.19.mlp.fc2.weight\", \"stage3.19.mlp.fc2.bias\", \"stage3.19.norm2.weight\", \"stage3.19.norm2.bias\", \"stage3.20.qkv.weight\", \"stage3.20.qkv.bias\", \"stage3.20.norm1.weight\", \"stage3.20.norm1.bias\", \"stage3.20.proj.weight\", \"stage3.20.proj.bias\", \"stage3.20.attns.0.get_v.weight\", \"stage3.20.attns.0.get_v.bias\", \"stage3.20.attns.1.get_v.weight\", \"stage3.20.attns.1.get_v.bias\", \"stage3.20.mlp.fc1.weight\", \"stage3.20.mlp.fc1.bias\", \"stage3.20.mlp.fc2.weight\", \"stage3.20.mlp.fc2.bias\", \"stage3.20.norm2.weight\", \"stage3.20.norm2.bias\", \"merge3.conv.weight\", \"merge3.conv.bias\", \"merge3.norm.weight\", \"merge3.norm.bias\", \"stage4.0.qkv.weight\", \"stage4.0.qkv.bias\", \"stage4.0.norm1.weight\", \"stage4.0.norm1.bias\", \"stage4.0.proj.weight\", \"stage4.0.proj.bias\", \"stage4.0.attns.0.get_v.weight\", \"stage4.0.attns.0.get_v.bias\", \"stage4.0.mlp.fc1.weight\", \"stage4.0.mlp.fc1.bias\", \"stage4.0.mlp.fc2.weight\", \"stage4.0.mlp.fc2.bias\", \"stage4.0.norm2.weight\", \"stage4.0.norm2.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"state_dict_ema\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-75421511a85c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the state dictionary into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2154\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CSWinTransformer:\n\tMissing key(s) in state_dict: \"stage1_conv_embed.0.weight\", \"stage1_conv_embed.0.bias\", \"stage1_conv_embed.2.weight\", \"stage1_conv_embed.2.bias\", \"stage1.0.qkv.weight\", \"stage1.0.qkv.bias\", \"stage1.0.norm1.weight\", \"stage1.0.norm1.bias\", \"stage1.0.proj.weight\", \"stage1.0.proj.bias\", \"stage1.0.attns.0.get_v.weight\", \"stage1.0.attns.0.get_v.bias\", \"stage1.0.attns.1.get_v.weight\", \"stage1.0.attns.1.get_v.bias\", \"stage1.0.mlp.fc1.weight\", \"stage1.0.mlp.fc1.bias\", \"stage1.0.mlp.fc2.weight\", \"stage1.0.mlp.fc2.bias\", \"stage1.0.norm2.weight\", \"stage1.0.norm2.bias\", \"merge1.conv.weight\", \"merge1.conv.bias\", \"merge1.norm.weight\", \"merge1.norm.bias\", \"stage2.0.qkv.weight\", \"stage2.0.qkv.bias\", \"stage2.0.norm1.weight\", \"stage2.0.norm1.bias\", \"stage2.0.proj.weight\", \"stage2.0.proj.bias\", \"stage2.0.attns.0.get_v.weight\", \"stage2.0.attns.0.get_v.bias\", \"stage2.0.attns.1.get_v.weight\", \"stage2.0.attns.1.get_v.bias\", \"stage2.0.mlp.fc1.weight\", \"stage2.0.mlp.fc1.bias\", \"stage2.0.mlp.fc2.weight\", \"stage2.0.mlp.fc2.bias\", \"stage2.0.norm2.weight\", \"stage2.0.norm2.bias\", \"stage2.1.qkv.weight\", \"stage2.1.qkv.bias\", \"stage2.1.norm1.weight\", \"stage2.1.norm1.bias\", \"stage2.1.proj.weight\", \"stage2.1.proj.bias\", \"stage2.1.attns.0.get_v.weight\", \"stage2.1.attns.0.get_v.bias\", \"stage2.1.attns.1.get_v.weight\", \"stage2.1.attns.1.get_v.bias\", \"stage2.1.mlp.fc1.weight\", \"stage2.1.mlp.fc1.bias\", \"stage2.1.mlp.fc2.weight\", \"stage2.1.mlp.fc2.bias\", \"stage2.1.norm2.weight\", \"stage2.1.norm2.bias\", \"merge2.conv...\n\tUnexpected key(s) in state_dict: \"state_dict_ema\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
        "import timm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define constants\n",
        "data_dirs = [\"TA\", \"PT\", \"PC\", \"MC\", \"LC\", \"F\", \"DC\", \"A\"]\n",
        "data_root = \"/content/drive/MyDrive/Breast Cancer Project/IW/40\"  # Replace with the root directory of your data\n",
        "train_split = 0.7\n",
        "\n",
        "# Create a list to store the paths and labels of all images\n",
        "all_data = []\n",
        "\n",
        "# Populate the list with paths and labels\n",
        "for label, folder in enumerate(data_dirs):\n",
        "    folder_path = os.path.join(data_root, folder)\n",
        "    image_files = os.listdir(folder_path)\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        all_data.append((image_path, label))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = train_test_split(all_data, train_size=train_split, shuffle=True, random_state=42)\n",
        "\n",
        "# Define custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        img = Image.open(img_path).convert('RGB')  # Open image and convert to RGB mode\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
        "        return img, label_tensor\n",
        "\n",
        "# Image preprocessing with augmentation for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Image preprocessing without augmentation for testing and validation\n",
        "test_val_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create custom datasets\n",
        "train_dataset = CustomDataset(train_data, transform=train_transform)\n",
        "test_dataset = CustomDataset(test_data, transform=test_val_transform)\n",
        "\n",
        "# DataLoaders for batching and shuffling\n",
        "batch_size = 10  # Define the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        # Ensure the input tensor is passed correctly\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        progress_bar.set_postfix({'Loss': train_loss / total, 'Accuracy': 100 * correct / total})\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(test_loader.dataset)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "v6d59bYaV7d-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}