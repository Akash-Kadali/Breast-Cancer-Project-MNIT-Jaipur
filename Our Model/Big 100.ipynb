{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8146556,
          "sourceType": "datasetVersion",
          "datasetId": 4817492
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 8158384,
          "datasetId": 4826407
        }
      ],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow>=1.7.0\"\n",
        "!pip install tensorflow-addons\n",
        "!pip install tensorflow-hub\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vq5qLS4d2LmQ",
        "outputId": "c03ff3ff-413d-4fa4-e208-730e898d235c",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=1.7.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.7.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.7.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.7.0) (3.2.2)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras>=2.14.1->tensorflow-hub) (3.2.2)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, utils\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "vmaszhvR2LmY",
        "outputId": "a1b879f9-cd69-4c4d-ead9-d5dd3eb36d14",
        "scrolled": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ht481_PZ2LmZ",
        "outputId": "23aac273-a72a-43b6-cd08-e43e3bf0e04b",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import statistics\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Dense, DepthwiseConv2D,AveragePooling2D, Concatenate, Dropout, Permute,Reshape,Lambda,Activation, Add,Multiply, MaxPooling2D, Conv2D, Flatten, BatchNormalization, GlobalAveragePooling2D,LayerNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16,ConvNeXtTiny,ResNet50, MobileNet, Xception, EfficientNetB0 , DenseNet169, DenseNet201, DenseNet121, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\n",
        "from tensorflow.keras.optimizers.legacy import Adam, SGD\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import gc\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "\n",
        "from keras.models import load_model\n",
        "#from keras.models import Sequential\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "o91Tp0x_sHTl",
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.version.VERSION, tf.config.list_physical_devices()"
      ],
      "metadata": {
        "id": "N-4YPF2w26ll",
        "outputId": "a3bb7416-b9cb-45ba-b740-cf1d981c7cd6",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.15.0',\n",
              " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              "  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = '/content/drive/MyDrive/Breast Cancer Project/IW/100/A'\n",
        "F = '/content/drive/MyDrive/Breast Cancer Project/IW/100/F'\n",
        "PT ='/content/drive/MyDrive/Breast Cancer Project/IW/100/PT'\n",
        "TA ='/content/drive/MyDrive/Breast Cancer Project/IW/100/TA'\n",
        "DC ='/content/drive/MyDrive/Breast Cancer Project/IW/100/DC'\n",
        "LC ='/content/drive/MyDrive/Breast Cancer Project/IW/100/LC'\n",
        "MC ='/content/drive/MyDrive/Breast Cancer Project/IW/100/MC'\n",
        "PC ='/content/drive/MyDrive/Breast Cancer Project/IW/100/PC'"
      ],
      "metadata": {
        "id": "4EWX3Gd8r32a",
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirlist=[A, F, PT, TA, DC, LC, MC, PC]\n",
        "classes=['A','F', 'PT', 'TA', 'DC', 'LC', 'MC', 'PC']\n",
        "filepaths=[]\n",
        "labels=[]\n",
        "for i,j in zip(dirlist, classes):\n",
        "    filelist=os.listdir(i)\n",
        "    for f in filelist:\n",
        "        filepath=os.path.join (i,f)\n",
        "        filepaths.append(filepath)\n",
        "        labels.append(j)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ],
      "metadata": {
        "id": "QNZkBeucr9Mh",
        "outputId": "851f478e-4041-409b-dc66-35dc41fedb57",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepaths:  2081    labels:  2081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Files=pd.Series(filepaths, name='filepaths')\n",
        "Label=pd.Series(labels, name='labels')\n",
        "df=pd.concat([Files,Label], axis=1)\n",
        "df=pd.DataFrame(np.array(df).reshape(len(filepaths),2), columns = ['filepaths', 'labels'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QVgOHhygsAlZ",
        "outputId": "763275b9-c2a8-4bd6-9ca1-8cecc0c3ffcf",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           filepaths labels\n",
              "0  /content/drive/MyDrive/Breast Cancer Project/I...      A\n",
              "1  /content/drive/MyDrive/Breast Cancer Project/I...      A\n",
              "2  /content/drive/MyDrive/Breast Cancer Project/I...      A\n",
              "3  /content/drive/MyDrive/Breast Cancer Project/I...      A\n",
              "4  /content/drive/MyDrive/Breast Cancer Project/I...      A"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a4681fa-212b-4684-80a7-e2b4a83529cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepaths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Breast Cancer Project/I...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Breast Cancer Project/I...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Breast Cancer Project/I...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Breast Cancer Project/I...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Breast Cancer Project/I...</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a4681fa-212b-4684-80a7-e2b4a83529cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a4681fa-212b-4684-80a7-e2b4a83529cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a4681fa-212b-4684-80a7-e2b4a83529cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3381de7e-8901-4ba7-992c-8ca116f0b6d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3381de7e-8901-4ba7-992c-8ca116f0b6d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3381de7e-8901-4ba7-992c-8ca116f0b6d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2081,\n  \"fields\": [\n    {\n      \"column\": \"filepaths\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2081,\n        \"samples\": [\n          \"/content/drive/MyDrive/Breast Cancer Project/IW/100/LC/SOB_M_LC-14-15570C-100-023.png\",\n          \"/content/drive/MyDrive/Breast Cancer Project/IW/100/DC/SOB_M_DC-14-5694-100-010.png\",\n          \"/content/drive/MyDrive/Breast Cancer Project/IW/100/MC/SOB_M_MC-14-18842-100-019.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"F\",\n          \"LC\",\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['labels'].value_counts())"
      ],
      "metadata": {
        "id": "1uQkt3MTsD_o",
        "outputId": "52e14269-4b1c-429f-f649-223b273da030",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels\n",
            "DC    903\n",
            "F     260\n",
            "MC    222\n",
            "LC    170\n",
            "TA    150\n",
            "PC    142\n",
            "PT    121\n",
            "A     113\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, train_size=0.70)\n",
        "#train_new, valid = train_test_split(train, train_size=0.90, random_state=0)\n",
        "\n",
        "print(f\"train set shape: {train.shape}\")\n",
        "print(f\"test set shape: {test.shape}\")\n",
        "print(f\"validation set shape: {test.shape}\")"
      ],
      "metadata": {
        "id": "pfI-lh99sGr7",
        "outputId": "312c6304-805a-42af-aff1-e9b725c87cc7",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set shape: (1456, 2)\n",
            "test set shape: (625, 2)\n",
            "validation set shape: (625, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = '/content/drive/MyDrive/fold5/40/train'\n",
        "# test = '/content/drive/MyDrive/fold5/40/test'"
      ],
      "metadata": {
        "id": "G5LJCbZ2IooS",
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, vertical_flip =True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1.0/255.)"
      ],
      "metadata": {
        "id": "BF_mNPBOsQPL",
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tand get the number os devices.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print('DEVICES AVAILABLE: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "train_gen = train_datagen.flow_from_dataframe(dataframe=train,\n",
        "                                              x_col = 'filepaths', y_col ='labels',\n",
        "                                              target_size = (224,224), batch_size = 4 * strategy.num_replicas_in_sync,\n",
        "                                              class_mode = 'categorical', shuffle = True)\n",
        "\n",
        "val_gen = test_datagen.flow_from_dataframe(test,\n",
        "                                            target_size = (224,224),   x_col = 'filepaths', y_col ='labels',\n",
        "                                             class_mode = 'categorical',\n",
        "                                            batch_size = 4 * strategy.num_replicas_in_sync, shuffle = False)\n",
        "\n",
        "\n",
        "test_gen = test_datagen.flow_from_dataframe(test,\n",
        "                                            target_size = (224,224),   x_col = 'filepaths', y_col ='labels',\n",
        "                                             class_mode = 'categorical',\n",
        "                                            batch_size = 4, shuffle = False)"
      ],
      "metadata": {
        "id": "4Gl5R4EJsRbN",
        "outputId": "2cc3f0f3-417c-49b1-8ba0-81624755b9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICES AVAILABLE: 1\n",
            "Found 1456 validated image filenames belonging to 8 classes.\n",
            "Found 625 validated image filenames belonging to 8 classes.\n",
            "Found 625 validated image filenames belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "    smoothed_points = []\n",
        "    for point in points:\n",
        "        if smoothed_points:\n",
        "            previous = smoothed_points[-1]\n",
        "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "        else:\n",
        "            smoothed_points.append(point)\n",
        "    return smoothed_points\n",
        "\n",
        "def plotmodel(history,name):\n",
        "\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(epochs,smooth_curve(acc))\n",
        "    plt.plot(epochs,smooth_curve(val_acc))\n",
        "    plt.ylabel('acc')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
        "    #plt.savefig('acc_'+name+'_'+mag+'_'+fold+'.png')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(epochs,smooth_curve(loss))\n",
        "    plt.plot(epochs,smooth_curve(val_loss))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
        "   # plt.savefig('loss_'+name+'_'+mag+'_'+fold+'.png')\n",
        "\n",
        "def label_smooth(y_true, y_pred):\n",
        "    y_true=((1-0.1)*y_true+0.05)\n",
        "    return K.categorical_crossentropy(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "CJBdug_F2Lmc",
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 3.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"green\" if cm[i, j] > thresh else \"red\", fontdict={'fontsize':'x-large'})\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "LeIVRsgY2Lmc",
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DefConv_full(input, filters, kernel_size, strides=1):\n",
        "    \"\"\"\n",
        "    Using DefConv_reduced to implement full DC layer.\n",
        "    \"\"\"\n",
        "    offsets = layers.Conv2D(filters=2 * kernel_size ** 2,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=strides,\n",
        "                            padding='same',\n",
        "                            kernel_initializer='random_normal'\n",
        "                            )(input)\n",
        "    X = DefConvLayer_red(filters=filters,\n",
        "                         kernel_size=kernel_size,\n",
        "                         strides=strides\n",
        "                         )(input, offsets)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "class DefConvLayer_red(Layer):\n",
        "\n",
        "    def __init__(self, filters,strides, kernel_size=3, **kwargs):\n",
        "        assert type(kernel_size) == int, \"expect kernel_size to be of type 'int'\"\n",
        "        assert type(strides) == int, \"expect strides to be of type int\"\n",
        "        self.N = kernel_size ** 2\n",
        "        self.filters = filters\n",
        "        self.strides = strides\n",
        "\n",
        "        super(DefConvLayer_red, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], self.N, self.filters),\n",
        "                                 # Wdc is of shape [n_C=input_channels, lxl=N, filters=output_channels]\n",
        "                                 initializer='RandomNormal',\n",
        "                                 dtype='float32',\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, input, offsets):\n",
        "        # input of shape: (m=batch_size, n_H, n_W, n_C)\n",
        "        # offsets of shape: (m, n_H, n_W, 2*N)\n",
        "        # m, n_H, n_W, n_C = input.shape\n",
        "        # offsets = super(DefConvLayer, self).call(input) # Conv2D to learn offsets (m, n_H, n_W, 2*N)\n",
        "\n",
        "        input_offsets = self.BLIN(input, offsets)  # (m, n_H, n_W, n_C, N)\n",
        "        # BLIN returns N interpolated values of input at the offsets, for each spatial pixel\n",
        "        # replicate the offset input to each of the output channels\n",
        "        input_offsets = tf.expand_dims(input_offsets, axis=-1)\n",
        "        input_offsets = tf.tile(input_offsets, [1, 1, 1, 1, 1, self.filters])  # (m, n_H, n_W, n_C, N, filters)\n",
        "\n",
        "        new_shape = (1, 1, 1,) + self.W.shape\n",
        "        W = tf.reshape(self.W, shape=new_shape)  # (1, 1, 1, n_C, N, filters) to be broadcastable to input_offsets\n",
        "\n",
        "        output = tf.multiply(input_offsets, W)  # (m, n_H, n_W, n_C, N, filters)\n",
        "        output = tf.math.reduce_sum(output, axis=-2)  # (m, n_H, n_W, n_C, filters) reduce along each channel kernel\n",
        "        output = tf.math.reduce_sum(output, axis=-2)  # (m, n_H, n_W, filters) reduce along input channels\n",
        "        return output\n",
        "\n",
        "    @tf.function\n",
        "    def BLIN(self, input, offsets_in):  # Bi-Linear Interpolation of input feature map values at offset locations\n",
        "        \"\"\"\n",
        "        'input' shape: (m, n_Hi, n_Wi, n_C)\n",
        "        'offsets_in' shape: (m, n_Ho, n_Wo, 2*N)\n",
        "        'offsets_in' is the output of the Conv2D layer step aimed at learning the offsets,\n",
        "                     possibly smaller spatial size than input's, if strides>1\n",
        "        \"\"\"\n",
        "        offsets = offsets_in\n",
        "        m    = tf.shape(input)[0]\n",
        "        n_Hi = tf.shape(input)[1]\n",
        "        n_Wi = tf.shape(input)[2]\n",
        "        n_C  = tf.shape(input)[3]\n",
        "\n",
        "        n_Ho = tf.shape(offsets)[1] # also the output spatial shape\n",
        "        n_Wo = tf.shape(offsets)[2]\n",
        "        N    = tf.shape(offsets)[3] // 2\n",
        "\n",
        "        # expand the input into (m, n_Hi, n_Wi, n_C, N). this will also be the output shape of this function\n",
        "        input_offsets = tf.expand_dims(input, axis=-1) # (m, n_Hi, n_Wi, n_C, N, 1)\n",
        "        # replicate N times, to be compatible with the kernel operation later\n",
        "        input_offsets = tf.tile(input_offsets, [1, 1, 1, 1, N])  # (m, n_Hi, n_Wi, n_C, N)\n",
        "\n",
        "        # the offset metrices will be replicated n_C times: same (spatial) offsets for each of the input *channels*.\n",
        "        offsets = tf.reshape(offsets, (m, n_Ho, n_Wo, 1, N, 2))  # (m, n_Ho, n_Wo, 1, N, 2) add a \"channel\" axis\n",
        "        offsets = tf.tile(offsets, [1, 1, 1, n_C, 1, 1])  # (m, n_Ho, n_Wo, n_C, N, 2) replicate for each of the input channels\n",
        "\n",
        "        # construct a full index grid to be applied onto \"input_offsets\" of size (m, n_H, n_W, n_C, N)\n",
        "        (grid_m, grid_i, grid_j, grid_c, grid_N) = tf.meshgrid(tf.range(m), tf.range(n_Hi),\n",
        "                                                               tf.range(n_Wi), tf.range(n_C), tf.range(N),\n",
        "                                                               indexing='ij')  # (m, n_Hi, n_Wi, n_C, N) a list of 5 metrices with index-like values\n",
        "\n",
        "        # adjust indices to 'strides' down-sample, and\n",
        "        # unroll indices to fit into tf.gather_nd later. (unroll offsets also)\n",
        "        ur_grid_m = tf.reshape(grid_m[:, ::self.strides, ::self.strides, :, :], [-1])  # (m*n_Ho*n_Wo*n_C*N, 1); integers\n",
        "        ur_grid_i = tf.reshape(grid_i[:, ::self.strides, ::self.strides, :, :], [-1])\n",
        "        ur_grid_j = tf.reshape(grid_j[:, ::self.strides, ::self.strides, :, :], [-1])\n",
        "        ur_grid_c = tf.reshape(grid_c[:, ::self.strides, ::self.strides, :, :], [-1])\n",
        "        ur_grid_N = tf.reshape(grid_N[:, ::self.strides, ::self.strides, :, :], [-1])\n",
        "        ur_offsets = tf.reshape(offsets, (-1, 2))  # (m*n_Ho*n_Wo*n_C*N, 2) both i, j\n",
        "\n",
        "        # spatial indices will be adjusted using 'offsets'\n",
        "        coords_i = tf.cast(ur_grid_i, dtype='float32') + ur_offsets[..., 0]\n",
        "        coords_j = tf.cast(ur_grid_j, dtype='float32') + ur_offsets[..., 1]\n",
        "\n",
        "        # Need to think further on how to handle edges,\n",
        "        # perhaps assume outside of index values can be zeros instead of hard-clipping.\n",
        "        coords_i = tf.clip_by_value(coords_i, 0, tf.cast(n_Hi, dtype='float32')-1)\n",
        "        coords_j = tf.clip_by_value(coords_j, 0, tf.cast(n_Wi, dtype='float32')-1)\n",
        "        coords_2d = tf.stack([coords_i, coords_j], axis=-1)  # (m*n_Ho*n_Wo*n_C*N, 2); float32\n",
        "\n",
        "        # generate top and bottom, left and right, nearest \"real\" indices\n",
        "        # assuming coords represents (p,q) values where i<=p<=i+1, and j<=q<=j+1:\n",
        "        # shape: (m*n_Ho*n_Wo*n_C*N, 2)\n",
        "        # note the coordinates themselves (values in coords) are [i,j] within [0:n_Hi-1, 0:n_Wi] range\n",
        "        coords_lt = tf.cast(tf.math.floor(coords_2d), dtype='int32')  # nearest (i,j)\n",
        "        coords_rb = tf.cast(tf.math.ceil(coords_2d), dtype='int32')  # nearest (i+1, j+1)\n",
        "\n",
        "        coords_lb = tf.stack((coords_rb[..., 0], coords_lt[..., 1]), axis=-1)  # nearest (i+1, j)\n",
        "        coords_rt = tf.stack((coords_lt[..., 0], coords_rb[..., 1]), axis=-1)  # nearest (i, j+1)\n",
        "\n",
        "        # use the replicated input tensor \"input_offsets\" which holds the input values, to get these values at the specific locations:\n",
        "        # these type of Tensors doesn't allow for conversion into numpy-like arrays. to use tf.gather_nd, need to unroll indices\n",
        "        # unroll all grid tensors to be used with tf.gather_nd()\n",
        "\n",
        "        indices_lt = tf.stack([ur_grid_m, coords_lt[..., 0], coords_lt[..., 1], ur_grid_c, ur_grid_N], axis=-1)\n",
        "        indices_rb = tf.stack([ur_grid_m, coords_rb[..., 0], coords_rb[..., 1], ur_grid_c, ur_grid_N], axis=-1)\n",
        "        indices_lb = tf.stack([ur_grid_m, coords_lb[..., 0], coords_lb[..., 1], ur_grid_c, ur_grid_N], axis=-1)\n",
        "        indices_rt = tf.stack([ur_grid_m, coords_rt[..., 0], coords_rt[..., 1], ur_grid_c, ur_grid_N], axis=-1)\n",
        "\n",
        "        vals_lt = tf.gather_nd(input_offsets, indices_lt)\n",
        "        vals_rb = tf.gather_nd(input_offsets, indices_rb)\n",
        "        vals_lb = tf.gather_nd(input_offsets, indices_lb)\n",
        "        vals_rt = tf.gather_nd(input_offsets, indices_rt)\n",
        "\n",
        "        # calculate the offset from the left-top (i,j) position\n",
        "        ur_coords_offset_lt = coords_2d - tf.cast(coords_lt, dtype='float32')  # (m*n_Ho*n_Wo*n_C*N, 2)\n",
        "\n",
        "        # first linear interpolation (m*n_H*n_W*n_C*N)\n",
        "        vals_t = vals_lt + (vals_rt - vals_lt) * ur_coords_offset_lt[..., 1]  # along the j axis (n_Wi), top\n",
        "        vals_b = vals_lb + (vals_rb - vals_lb) * ur_coords_offset_lt[..., 1]  # along the j axis (n_Wi), bottom\n",
        "\n",
        "        # second linear interpolation\n",
        "        input_offsets = vals_t + (vals_b - vals_t) * ur_coords_offset_lt[..., 0]  # along the i axis (n_Hi)\n",
        "\n",
        "        # reshape back to output shape\n",
        "        input_offsets = tf.reshape(input_offsets, (m, n_Ho, n_Wo, n_C, N))\n",
        "\n",
        "        return input_offsets"
      ],
      "metadata": {
        "id": "hwUptDC7WJHS",
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def train_model(model, train_gen, val_gen, test_gen, mag, image_size, save_name, lr1, lr2, Epochs1, Epochs2):\n",
        "    from keras.callbacks import ModelCheckpoint\n",
        "    from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "    lr_decay = ReduceLROnPlateau(monitor='loss', factor=0.8, patience=3, verbose=1)\n",
        "    save_model = ModelCheckpoint('/content/drive/MyDrive/100+{epoch:02d}.h5',\n",
        "                                 monitor='val_accuracy',\n",
        "                                 period=5,\n",
        "                                 save_best_only=True)\n",
        "\n",
        "    # List of model names\n",
        "    model_names = ['100epoch+01.h5', '100epoch+06.h5', '100epoch+21.h5', '100epoch+01.h5', '100epoch+31.h5', '100epoch+51.h5', '100epoch+61.h5', '100epoch+66.h5', '100epoch+76.h5']\n",
        "\n",
        "    # List to store loaded models\n",
        "    loaded_models = []\n",
        "\n",
        "    custom_objects = {\n",
        "        'DefConvLayer_red': DefConvLayer_red  # Assuming 'DefConvLayer_red' is a custom layer\n",
        "    }\n",
        "\n",
        "    # Load models in a loop\n",
        "    for model_name in model_names:\n",
        "        # Construct the full path to the model file\n",
        "        model_path = '/content/drive/MyDrive/Epochs/100/' + model_name\n",
        "\n",
        "        # Load the model and append it to the list\n",
        "        model = load_model(model_path, custom_objects=custom_objects)\n",
        "        loaded_models.append(model)\n",
        "\n",
        "        # Evaluate the loaded model\n",
        "        results = model.evaluate(test_gen)\n",
        "        print('Test loss and accuracy for model', model_name, ':', results)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict(test_gen)\n",
        "        rounded_pred = np.argmax(predictions, axis=-1)\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(y_true=test_gen.classes, y_pred=rounded_pred)\n",
        "        cm_plot_labels = ['A', 'F', 'PT', 'TA', 'DC', 'LC', 'MC', 'PC']\n",
        "        plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix for model ' + model_name)\n",
        "\n",
        "        # Print classification report\n",
        "        print('Classification report for model', model_name, ':')\n",
        "        print(classification_report(y_true=test_gen.classes, y_pred=rounded_pred, target_names=cm_plot_labels))\n",
        "\n",
        "    # Return something meaningful, e.g., history\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "mDj2Er1H2Lmf",
        "trusted": true
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/leondgarse/keras_cv_attention_models"
      ],
      "metadata": {
        "id": "COIWAnOotVGZ",
        "outputId": "4bb5c7cb-93e2-4160-944e-9023abe9986d",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/leondgarse/keras_cv_attention_models\n",
            "  Cloning https://github.com/leondgarse/keras_cv_attention_models to /tmp/pip-req-build-issgkdqm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/leondgarse/keras_cv_attention_models /tmp/pip-req-build-issgkdqm\n",
            "  Resolved https://github.com/leondgarse/keras_cv_attention_models to commit 5bbbc792effde7d5d94c01b7c15625d9db109aa6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.4.2) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.4.2) (4.66.2)\n",
            "Collecting ftfy (from keras-cv-attention-models==1.4.2)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m847.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.4.2) (2023.12.25)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.4.2) (4.9.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-cv-attention-models==1.4.2) (2.15.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->keras-cv-attention-models==1.4.2) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-cv-attention-models==1.4.2) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv-attention-models==1.4.2) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-cv-attention-models==1.4.2) (0.43.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models==1.4.2) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models==1.4.2) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-cv-attention-models==1.4.2) (3.2.2)\n",
            "Building wheels for collected packages: keras-cv-attention-models\n",
            "  Building wheel for keras-cv-attention-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-cv-attention-models: filename=keras_cv_attention_models-1.4.2-py3-none-any.whl size=796745 sha256=ae72c0ebd0a90200b04cc7234fd0a2ff34d1f5d21c0bfd3b0f321c2e37406a90\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fzqhkz34/wheels/16/98/1e/847241ad48bd552bd921ea08001f22a1ecab471036f6b09bb4\n",
            "Successfully built keras-cv-attention-models\n",
            "Installing collected packages: ftfy, keras-cv-attention-models\n",
            "Successfully installed ftfy-6.2.0 keras-cv-attention-models-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_cv_attention_models import maxvit\n",
        "mm = maxvit.MaxViT_Tiny(input_shape=(224, 224, 3), pretrained=\"imagenet\")\n",
        "# mm.summary()"
      ],
      "metadata": {
        "id": "fLq_Narsto20",
        "outputId": "7f6f89c0-6a87-4975-9081-4a17acc3a23e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/maxvit/maxvit_tiny_224_imagenet.h5\n",
            "126113280/126113280 [==============================] - 1s 0us/step\n",
            ">>>> Load pretrained from: /root/.keras/models/maxvit_tiny_224_imagenet.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "def SSA(inputs,fltr):\n",
        "    shape=K.int_shape(inputs)\n",
        "    li=q=k=v=Conv2D(fltr,1,padding='same',activation='relu')(inputs)\n",
        "    print(\"Shape of Input of SSA\", inputs)\n",
        "    Qshape=K.int_shape(q)\n",
        "    Kshape= K.int_shape(k)\n",
        "    Vshape= K.int_shape(v)\n",
        "    a=Qshape[1]*Qshape[2]\n",
        "    q=Reshape((a,Qshape[3]))(q)\n",
        "    k=Reshape((a,Kshape[3]))(k)\n",
        "    k=Permute((2,1))(k)\n",
        "    qk=tf.matmul(q,k)\n",
        "    qk=Activation('softmax')(qk)\n",
        "    v=Reshape((a,Vshape[3]))(v)\n",
        "    qkv=tf.matmul(qk,v)\n",
        "    print(qkv.shape)\n",
        "    qkv=Reshape((Vshape[1],Vshape[2],Vshape[3]))(qkv)\n",
        "    qkv = Conv2D(shape[3], 1, strides=1, padding='same', activation='relu')(qkv)\n",
        "    print(\"Shape of Output of SSA\", qkv)\n",
        "    return qkv"
      ],
      "metadata": {
        "id": "gfdo3wQDgQWF",
        "trusted": true
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "def CDSA(input,fltr,nh):\n",
        "    attn = []\n",
        "    print(\"Shape of CDSA Input\", input.shape)\n",
        "    feature_split = tf.split(input, num_or_size_splits= num_splits, axis=3)\n",
        "    print(feature_split[0].shape)\n",
        "    shape=K.int_shape(feature_split[0])\n",
        "    x = SSA(feature_split[0],fltr)\n",
        "    attn.append(x)\n",
        "    for i in range(1,nh):\n",
        "        x = Add()([feature_split[i],x])\n",
        "        x = SSA(x,fltr)\n",
        "        attn.append(x)\n",
        "    mh_lka_attn = Add()(attn)\n",
        "    mh_lka_attn = Conv2D(fltr,1, strides=1, padding='same', activation='relu')(mh_lka_attn)\n",
        "    print(\"Shape of CDSA Output\", mh_lka_attn.shape)\n",
        "    return mh_lka_attn\n"
      ],
      "metadata": {
        "id": "NVDzywSVfVX_",
        "trusted": true
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CAL(input,fltr,nh):\n",
        "    print(\"Shape of CAL Input\", input.shape)\n",
        "    x = DefConv_full(input, fltr, kernel_size=3)\n",
        "    rs1 = x = Add()([x,input])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = CDSA(x,fltr,nh)\n",
        "    rs2 = x = Add()([rs1,x])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Conv2D(fltr, 1, padding='same', activation='relu')(x)\n",
        "    x = Add()([rs2,x])\n",
        "    print(\"Shape of CAL Output\", x.shape)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "PKnsju-la_cg",
        "trusted": true
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun= 'categorical_crossentropy'\n",
        "gpu_num=2\n",
        "k=5\n",
        "lr1=0.005\n",
        "lr2=0.0001\n",
        "image_size=224\n",
        "classes=8\n",
        "ratio=8\n",
        "fltr=256\n",
        "nh=2  # number of splits\n",
        "mag='40'"
      ],
      "metadata": {
        "id": "uQqoWozCTBhK",
        "trusted": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "input_image = Input(shape=(224, 224, 3))\n",
        "mn_input = input_image\n",
        "\n",
        "# Load the model\n",
        "base_model = maxvit.MaxViT_Tiny(input_shape=(224, 224, 3), pretrained=\"imagenet\")\n",
        "new_base_model = Model(inputs=base_model.input, outputs=base_model.get_layer('stack_3_block_5/grid_ffn_output').output)\n",
        "mn_output = new_base_model(mn_input)\n",
        "print(mn_output.shape)\n",
        "\n",
        "mn_output = Conv2D(fltr, 1, padding='same', activation='relu')(mn_output)\n",
        "print(mn_output.shape)\n",
        "mn_output = BatchNormalization()(mn_output)  # Add Batch Normalization\n",
        "mn_output = Dropout(0.5)(mn_output)\n",
        "num_splits = 2\n",
        "CAL_out = CAL(mn_output,fltr,nh)\n",
        "print(CAL_out.shape)\n",
        "CAL_out = GlobalAveragePooling2D()(CAL_out)\n",
        "out=Dense(classes,activation='softmax')(CAL_out)\n",
        "if gpu_num<1:\n",
        "    model=Model(inputs=input_image, outputs=out)\n",
        "    #model.summary()\n",
        "    parallel_model = multi_gpu_model(model, gpus=gpu_num)\n",
        "    parallel_model.summary()\n",
        "else:\n",
        "    parallel_model=Model(inputs=input_image, outputs=out)\n",
        "    parallel_model.summary()"
      ],
      "metadata": {
        "id": "i--5bFDwR9fx",
        "outputId": "6fa26a83-a1ad-496d-9d64-777c3ae48803",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Load pretrained from: /root/.keras/models/maxvit_tiny_224_imagenet.h5\n",
            "(None, 14, 14, 256)\n",
            "(None, 14, 14, 256)\n",
            "Shape of CAL Input (None, 14, 14, 256)\n",
            "Shape of CDSA Input (None, 14, 14, 256)\n",
            "(None, 14, 14, 128)\n",
            "Shape of Input of SSA KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 128), dtype=tf.float32, name=None), name='tf.split_44/split:0', description=\"created by layer 'tf.split_44'\")\n",
            "(None, 196, 256)\n",
            "Shape of Output of SSA KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 128), dtype=tf.float32, name=None), name='conv2d_3/Relu:0', description=\"created by layer 'conv2d_3'\")\n",
            "Shape of Input of SSA KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 128), dtype=tf.float32, name=None), name='add_1/add:0', description=\"created by layer 'add_1'\")\n",
            "(None, 196, 256)\n",
            "Shape of Output of SSA KerasTensor(type_spec=TensorSpec(shape=(None, 14, 14, 128), dtype=tf.float32, name=None), name='conv2d_5/Relu:0', description=\"created by layer 'conv2d_5'\")\n",
            "Shape of CDSA Output (None, 14, 14, 256)\n",
            "Shape of CAL Output (None, 14, 14, 256)\n",
            "(None, 14, 14, 256)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 14, 14, 256)          1263885   ['input_2[0][0]']             \n",
            "                                                          6                                       \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 14, 14, 256)          65792     ['model[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 14, 14, 256)          1024      ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 14, 14, 256)          0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 18)           41490     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " def_conv_layer_red (DefCon  (None, 14, 14, 256)          589824    ['dropout[0][0]',             \n",
            " vLayer_red)                                                         'conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 14, 14, 256)          0         ['def_conv_layer_red[0][0]',  \n",
            "                                                                     'dropout[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 14, 14, 256)          512       ['add[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.split_44 (TFOpLambda)    [(None, 14, 14, 128),        0         ['layer_normalization[0][0]'] \n",
            "                              (None, 14, 14, 128)]                                                \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 256)          33024     ['tf.split_44[0][0]']         \n",
            "                                                                                                  \n",
            " reshape_133 (Reshape)       (None, 196, 256)             0         ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_132 (Reshape)       (None, 196, 256)             0         ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " permute (Permute)           (None, 256, 196)             0         ['reshape_133[0][0]']         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_88 (TFOpL  (None, 196, 196)             0         ['reshape_132[0][0]',         \n",
            " ambda)                                                              'permute[0][0]']             \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 196, 196)             0         ['tf.linalg.matmul_88[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_134 (Reshape)       (None, 196, 256)             0         ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_89 (TFOpL  (None, 196, 256)             0         ['activation[0][0]',          \n",
            " ambda)                                                              'reshape_134[0][0]']         \n",
            "                                                                                                  \n",
            " reshape_135 (Reshape)       (None, 14, 14, 256)          0         ['tf.linalg.matmul_89[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 128)          32896     ['reshape_135[0][0]']         \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 14, 14, 128)          0         ['tf.split_44[0][1]',         \n",
            "                                                                     'conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 256)          33024     ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_137 (Reshape)       (None, 196, 256)             0         ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_136 (Reshape)       (None, 196, 256)             0         ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " permute_1 (Permute)         (None, 256, 196)             0         ['reshape_137[0][0]']         \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_90 (TFOpL  (None, 196, 196)             0         ['reshape_136[0][0]',         \n",
            " ambda)                                                              'permute_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 196, 196)             0         ['tf.linalg.matmul_90[0][0]'] \n",
            "                                                                                                  \n",
            " reshape_138 (Reshape)       (None, 196, 256)             0         ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_91 (TFOpL  (None, 196, 256)             0         ['activation_1[0][0]',        \n",
            " ambda)                                                              'reshape_138[0][0]']         \n",
            "                                                                                                  \n",
            " reshape_139 (Reshape)       (None, 14, 14, 256)          0         ['tf.linalg.matmul_91[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 128)          32896     ['reshape_139[0][0]']         \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 14, 14, 128)          0         ['conv2d_3[0][0]',            \n",
            "                                                                     'conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 14, 14, 256)          33024     ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 14, 14, 256)          0         ['add[0][0]',                 \n",
            "                                                                     'conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 14, 14, 256)          512       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 256)          65792     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 14, 14, 256)          0         ['add_3[0][0]',               \n",
            "                                                                     'conv2d_7[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 256)                  0         ['add_4[0][0]']               \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 8)                    2056      ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 13570722 (51.77 MB)\n",
            "Trainable params: 13540514 (51.65 MB)\n",
            "Non-trainable params: 30208 (118.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_model(parallel_model,train_gen,val_gen,test_gen,mag,image_size,'maxvit',lr1,lr2,4,100)"
      ],
      "metadata": {
        "id": "_6LvLQM-S6Vp",
        "outputId": "02b04b68-057e-4ce4-abd1-64173315d15e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 318s 2s/step - loss: 1.3412 - acc: 0.5216\n",
            "Test loss and accuracy for model 100epoch+01.h5 : [1.3412436246871948, 0.5216000080108643]\n",
            "157/157 [==============================] - 71s 413ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[  9   2  22   0   0   0   0   0]\n",
            " [  0 199  83   2   2   1   2   0]\n",
            " [  0   1  67   1   0   0   0   0]\n",
            " [  2  15  22  21   0   0   2   0]\n",
            " [  1  18  17   1   7   7   1   1]\n",
            " [  0   9  12   0   0  15   2   0]\n",
            " [  0   0  31   0   0   0   4   0]\n",
            " [  1   1  34   0   0   1   5   4]]\n",
            "Classification report for model 100epoch+01.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.69      0.27      0.39        33\n",
            "           F       0.81      0.69      0.75       289\n",
            "          PT       0.23      0.97      0.38        69\n",
            "          TA       0.84      0.34      0.48        62\n",
            "          DC       0.78      0.13      0.23        53\n",
            "          LC       0.62      0.39      0.48        38\n",
            "          MC       0.25      0.11      0.16        35\n",
            "          PC       0.80      0.09      0.16        46\n",
            "\n",
            "    accuracy                           0.52       625\n",
            "   macro avg       0.63      0.37      0.38       625\n",
            "weighted avg       0.70      0.52      0.52       625\n",
            "\n",
            "157/157 [==============================] - 72s 418ms/step - loss: 0.6546 - acc: 0.7632\n",
            "Test loss and accuracy for model 100epoch+06.h5 : [0.6546313166618347, 0.7631999850273132]\n",
            "157/157 [==============================] - 70s 411ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 33   0   0   0   0   0   0   0]\n",
            " [  2 223   8   5   2  47   1   1]\n",
            " [  0   0  67   0   0   2   0   0]\n",
            " [  7   8   4  34   0   7   0   2]\n",
            " [  4  16   0   0  29   4   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  3   0  21   0   0   1  10   0]\n",
            " [  0   0   2   0   1   0   0  43]]\n",
            "Classification report for model 100epoch+06.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.67      1.00      0.80        33\n",
            "           F       0.90      0.77      0.83       289\n",
            "          PT       0.66      0.97      0.78        69\n",
            "          TA       0.87      0.55      0.67        62\n",
            "          DC       0.91      0.55      0.68        53\n",
            "          LC       0.38      1.00      0.55        38\n",
            "          MC       0.91      0.29      0.43        35\n",
            "          PC       0.93      0.93      0.93        46\n",
            "\n",
            "    accuracy                           0.76       625\n",
            "   macro avg       0.78      0.76      0.71       625\n",
            "weighted avg       0.83      0.76      0.77       625\n",
            "\n",
            "157/157 [==============================] - 73s 428ms/step - loss: 0.2673 - acc: 0.9088\n",
            "Test loss and accuracy for model 100epoch+21.h5 : [0.2672657072544098, 0.9088000059127808]\n",
            "157/157 [==============================] - 70s 413ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 32   0   1   0   0   0   0   0]\n",
            " [  0 264   3  10   0  12   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  1   6   1  52   1   1   0   0]\n",
            " [  0   1   0   0  52   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0  18   0   0   1  16   0]\n",
            " [  0   0   1   0   0   0   0  45]]\n",
            "Classification report for model 100epoch+21.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.97      0.97      0.97        33\n",
            "           F       0.97      0.91      0.94       289\n",
            "          PT       0.74      1.00      0.85        69\n",
            "          TA       0.84      0.84      0.84        62\n",
            "          DC       0.98      0.98      0.98        53\n",
            "          LC       0.73      1.00      0.84        38\n",
            "          MC       1.00      0.46      0.63        35\n",
            "          PC       1.00      0.98      0.99        46\n",
            "\n",
            "    accuracy                           0.91       625\n",
            "   macro avg       0.90      0.89      0.88       625\n",
            "weighted avg       0.92      0.91      0.91       625\n",
            "\n",
            "157/157 [==============================] - 72s 421ms/step - loss: 1.3412 - acc: 0.5216\n",
            "Test loss and accuracy for model 100epoch+01.h5 : [1.3412436246871948, 0.5216000080108643]\n",
            "157/157 [==============================] - 70s 410ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[  9   2  22   0   0   0   0   0]\n",
            " [  0 199  83   2   2   1   2   0]\n",
            " [  0   1  67   1   0   0   0   0]\n",
            " [  2  15  22  21   0   0   2   0]\n",
            " [  1  18  17   1   7   7   1   1]\n",
            " [  0   9  12   0   0  15   2   0]\n",
            " [  0   0  31   0   0   0   4   0]\n",
            " [  1   1  34   0   0   1   5   4]]\n",
            "Classification report for model 100epoch+01.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.69      0.27      0.39        33\n",
            "           F       0.81      0.69      0.75       289\n",
            "          PT       0.23      0.97      0.38        69\n",
            "          TA       0.84      0.34      0.48        62\n",
            "          DC       0.78      0.13      0.23        53\n",
            "          LC       0.62      0.39      0.48        38\n",
            "          MC       0.25      0.11      0.16        35\n",
            "          PC       0.80      0.09      0.16        46\n",
            "\n",
            "    accuracy                           0.52       625\n",
            "   macro avg       0.63      0.37      0.38       625\n",
            "weighted avg       0.70      0.52      0.52       625\n",
            "\n",
            "157/157 [==============================] - 73s 426ms/step - loss: 0.1098 - acc: 0.9648\n",
            "Test loss and accuracy for model 100epoch+31.h5 : [0.10983714461326599, 0.9648000001907349]\n",
            "157/157 [==============================] - 71s 412ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 32   0   1   0   0   0   0   0]\n",
            " [  0 276   0   9   0   4   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  0   5   0  57   0   0   0   0]\n",
            " [  0   0   0   1  52   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0   0   0   0   0  35   0]\n",
            " [  0   0   2   0   0   0   0  44]]\n",
            "Classification report for model 100epoch+31.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      0.97      0.98        33\n",
            "           F       0.98      0.96      0.97       289\n",
            "          PT       0.96      1.00      0.98        69\n",
            "          TA       0.85      0.92      0.88        62\n",
            "          DC       1.00      0.98      0.99        53\n",
            "          LC       0.90      1.00      0.95        38\n",
            "          MC       1.00      1.00      1.00        35\n",
            "          PC       1.00      0.96      0.98        46\n",
            "\n",
            "    accuracy                           0.96       625\n",
            "   macro avg       0.96      0.97      0.97       625\n",
            "weighted avg       0.97      0.96      0.97       625\n",
            "\n",
            "157/157 [==============================] - 73s 424ms/step - loss: 0.1266 - acc: 0.9712\n",
            "Test loss and accuracy for model 100epoch+51.h5 : [0.12657158076763153, 0.9711999893188477]\n",
            "157/157 [==============================] - 70s 405ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 33   0   0   0   0   0   0   0]\n",
            " [  0 286   0   3   0   0   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  0  11   0  51   0   0   0   0]\n",
            " [  0   0   0   0  53   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0   3   0   0   0  32   0]\n",
            " [  0   0   1   0   0   0   0  45]]\n",
            "Classification report for model 100epoch+51.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      1.00      1.00        33\n",
            "           F       0.96      0.99      0.98       289\n",
            "          PT       0.95      1.00      0.97        69\n",
            "          TA       0.94      0.82      0.88        62\n",
            "          DC       1.00      1.00      1.00        53\n",
            "          LC       1.00      1.00      1.00        38\n",
            "          MC       1.00      0.91      0.96        35\n",
            "          PC       1.00      0.98      0.99        46\n",
            "\n",
            "    accuracy                           0.97       625\n",
            "   macro avg       0.98      0.96      0.97       625\n",
            "weighted avg       0.97      0.97      0.97       625\n",
            "\n",
            "157/157 [==============================] - 71s 416ms/step - loss: 0.0988 - acc: 0.9648\n",
            "Test loss and accuracy for model 100epoch+61.h5 : [0.09880608320236206, 0.9648000001907349]\n",
            "157/157 [==============================] - 71s 406ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 33   0   0   0   0   0   0   0]\n",
            " [  0 279   1   8   1   0   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  0  11   0  51   0   0   0   0]\n",
            " [  0   0   0   0  53   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0   0   0   0   0  35   0]\n",
            " [  0   0   1   0   0   0   0  45]]\n",
            "Classification report for model 100epoch+61.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      1.00      1.00        33\n",
            "           F       0.96      0.97      0.96       289\n",
            "          PT       0.97      1.00      0.99        69\n",
            "          TA       0.86      0.82      0.84        62\n",
            "          DC       0.98      1.00      0.99        53\n",
            "          LC       1.00      1.00      1.00        38\n",
            "          MC       1.00      1.00      1.00        35\n",
            "          PC       1.00      0.98      0.99        46\n",
            "\n",
            "    accuracy                           0.96       625\n",
            "   macro avg       0.97      0.97      0.97       625\n",
            "weighted avg       0.96      0.96      0.96       625\n",
            "\n",
            "157/157 [==============================] - 72s 418ms/step - loss: 0.0822 - acc: 0.9776\n",
            "Test loss and accuracy for model 100epoch+66.h5 : [0.08221160620450974, 0.9775999784469604]\n",
            "157/157 [==============================] - 69s 407ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 33   0   0   0   0   0   0   0]\n",
            " [  0 287   0   2   0   0   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  0  11   0  51   0   0   0   0]\n",
            " [  0   0   0   0  53   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0   0   0   0   0  35   0]\n",
            " [  0   0   1   0   0   0   0  45]]\n",
            "Classification report for model 100epoch+66.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       1.00      1.00      1.00        33\n",
            "           F       0.96      0.99      0.98       289\n",
            "          PT       0.99      1.00      0.99        69\n",
            "          TA       0.96      0.82      0.89        62\n",
            "          DC       1.00      1.00      1.00        53\n",
            "          LC       1.00      1.00      1.00        38\n",
            "          MC       1.00      1.00      1.00        35\n",
            "          PC       1.00      0.98      0.99        46\n",
            "\n",
            "    accuracy                           0.98       625\n",
            "   macro avg       0.99      0.97      0.98       625\n",
            "weighted avg       0.98      0.98      0.98       625\n",
            "\n",
            "157/157 [==============================] - 71s 414ms/step - loss: 0.0844 - acc: 0.9776\n",
            "Test loss and accuracy for model 100epoch+76.h5 : [0.08437773585319519, 0.9775999784469604]\n",
            "157/157 [==============================] - 70s 403ms/step\n",
            "Confusion matrix, without normalization\n",
            "[[ 33   0   0   0   0   0   0   0]\n",
            " [  0 289   0   0   0   0   0   0]\n",
            " [  0   0  69   0   0   0   0   0]\n",
            " [  1  12   0  49   0   0   0   0]\n",
            " [  0   0   0   0  53   0   0   0]\n",
            " [  0   0   0   0   0  38   0   0]\n",
            " [  0   0   0   0   0   0  35   0]\n",
            " [  0   0   1   0   0   0   0  45]]\n",
            "Classification report for model 100epoch+76.h5 :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.97      1.00      0.99        33\n",
            "           F       0.96      1.00      0.98       289\n",
            "          PT       0.99      1.00      0.99        69\n",
            "          TA       1.00      0.79      0.88        62\n",
            "          DC       1.00      1.00      1.00        53\n",
            "          LC       1.00      1.00      1.00        38\n",
            "          MC       1.00      1.00      1.00        35\n",
            "          PC       1.00      0.98      0.99        46\n",
            "\n",
            "    accuracy                           0.98       625\n",
            "   macro avg       0.99      0.97      0.98       625\n",
            "weighted avg       0.98      0.98      0.98       625\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3edca79327a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maxvit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-6485239c1f4d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_gen, val_gen, test_gen, mag, image_size, save_name, lr1, lr2, Epochs1, Epochs2)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Return something meaningful, e.g., history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}