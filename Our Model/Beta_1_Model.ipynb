{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd1acf392d404646ba918c4f24593cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af7c4d14786840a89d3ab1a3d205e25e",
              "IPY_MODEL_421005183050440bba2025146433297f",
              "IPY_MODEL_600df311d67f49b2ab2cfcc4e3efce5c"
            ],
            "layout": "IPY_MODEL_85983b4e2a2447b38f80100c78d43b26"
          }
        },
        "af7c4d14786840a89d3ab1a3d205e25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b11d1d2dbee34dc68ca87751bc91c14d",
            "placeholder": "​",
            "style": "IPY_MODEL_0590f6b586464eba9616ddb6107ded61",
            "value": "config.json: 100%"
          }
        },
        "421005183050440bba2025146433297f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12692f703a584321835c4ba828b0b8a3",
            "max": 597,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bfbaa51c4734e5c88c8a7ff32525b95",
            "value": 597
          }
        },
        "600df311d67f49b2ab2cfcc4e3efce5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9ccc4687024bf18fa0e955af7dbb96",
            "placeholder": "​",
            "style": "IPY_MODEL_6f1af3671b544ceea946c2fc2ec2d337",
            "value": " 597/597 [00:00&lt;00:00, 33.6kB/s]"
          }
        },
        "85983b4e2a2447b38f80100c78d43b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b11d1d2dbee34dc68ca87751bc91c14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0590f6b586464eba9616ddb6107ded61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12692f703a584321835c4ba828b0b8a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bfbaa51c4734e5c88c8a7ff32525b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9ccc4687024bf18fa0e955af7dbb96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f1af3671b544ceea946c2fc2ec2d337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "696cf7aeadcb412d9332e66205fddabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_765d345cf9a3423ea1309ee5c53d2dbc",
              "IPY_MODEL_66bb441e878c4abd98fc948f80ab5ce0",
              "IPY_MODEL_d028e0d90cfc4a38971afdd954d84d7a"
            ],
            "layout": "IPY_MODEL_640fd69c105746f6ad80a520e18f0ffc"
          }
        },
        "765d345cf9a3423ea1309ee5c53d2dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bdb5bc398794850a70408cc6473e3de",
            "placeholder": "​",
            "style": "IPY_MODEL_a0591a34bd4044b4baee66a671327329",
            "value": "model.safetensors: 100%"
          }
        },
        "66bb441e878c4abd98fc948f80ab5ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e2c04f31264945a7c9bf30febfdb67",
            "max": 123917994,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a3e000792ae466382df4dab7ae68773",
            "value": 123917994
          }
        },
        "d028e0d90cfc4a38971afdd954d84d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fc1c236fc644fbb8e31f174cc0e596d",
            "placeholder": "​",
            "style": "IPY_MODEL_fbb24d1d2e2b4d49a9dcc24728e53103",
            "value": " 124M/124M [00:02&lt;00:00, 51.7MB/s]"
          }
        },
        "640fd69c105746f6ad80a520e18f0ffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bdb5bc398794850a70408cc6473e3de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0591a34bd4044b4baee66a671327329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8e2c04f31264945a7c9bf30febfdb67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3e000792ae466382df4dab7ae68773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fc1c236fc644fbb8e31f174cc0e596d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbb24d1d2e2b4d49a9dcc24728e53103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fd1acf392d404646ba918c4f24593cab",
            "af7c4d14786840a89d3ab1a3d205e25e",
            "421005183050440bba2025146433297f",
            "600df311d67f49b2ab2cfcc4e3efce5c",
            "85983b4e2a2447b38f80100c78d43b26",
            "b11d1d2dbee34dc68ca87751bc91c14d",
            "0590f6b586464eba9616ddb6107ded61",
            "12692f703a584321835c4ba828b0b8a3",
            "4bfbaa51c4734e5c88c8a7ff32525b95",
            "9e9ccc4687024bf18fa0e955af7dbb96",
            "6f1af3671b544ceea946c2fc2ec2d337",
            "696cf7aeadcb412d9332e66205fddabe",
            "765d345cf9a3423ea1309ee5c53d2dbc",
            "66bb441e878c4abd98fc948f80ab5ce0",
            "d028e0d90cfc4a38971afdd954d84d7a",
            "640fd69c105746f6ad80a520e18f0ffc",
            "0bdb5bc398794850a70408cc6473e3de",
            "a0591a34bd4044b4baee66a671327329",
            "d8e2c04f31264945a7c9bf30febfdb67",
            "5a3e000792ae466382df4dab7ae68773",
            "9fc1c236fc644fbb8e31f174cc0e596d",
            "fbb24d1d2e2b4d49a9dcc24728e53103"
          ]
        },
        "id": "HXr-RmhxHsGQ",
        "outputId": "27605310-ead7-4178-93d9-3d6770129575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 timm-0.9.16\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/597 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd1acf392d404646ba918c4f24593cab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/124M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "696cf7aeadcb412d9332e66205fddabe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
        "import timm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "import timm\n",
        "\n",
        "model = timm.create_model(\"hf_hub:timm/maxvit_tiny_tf_224.in1k\", pretrained=True)\n",
        "\n",
        "model.to(device)  # Move model to GPU\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXulvZy7Hvzn",
        "outputId": "2fb0ad15-adf5-42f3-da1a-f8da42634ab6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Assuming the model variable contains your model\n",
        "summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrqSuM1JGeI",
        "outputId": "04ba9157-230c-40df-8f20-e5f82bd0db6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "        Conv2dSame-1         [-1, 64, 112, 112]           1,792\n",
            "          Identity-2         [-1, 64, 112, 112]               0\n",
            "          GELUTanh-3         [-1, 64, 112, 112]               0\n",
            "    BatchNormAct2d-4         [-1, 64, 112, 112]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,928\n",
            "              Stem-6         [-1, 64, 112, 112]               0\n",
            "     AvgPool2dSame-7           [-1, 64, 56, 56]               0\n",
            "          Identity-8           [-1, 64, 56, 56]               0\n",
            "      Downsample2d-9           [-1, 64, 56, 56]               0\n",
            "         Identity-10         [-1, 64, 112, 112]               0\n",
            "         Identity-11         [-1, 64, 112, 112]               0\n",
            "   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n",
            "         Identity-13         [-1, 64, 112, 112]               0\n",
            "           Conv2d-14        [-1, 256, 112, 112]          16,384\n",
            "         Identity-15        [-1, 256, 112, 112]               0\n",
            "         GELUTanh-16        [-1, 256, 112, 112]               0\n",
            "   BatchNormAct2d-17        [-1, 256, 112, 112]             512\n",
            "       Conv2dSame-18          [-1, 256, 56, 56]           2,304\n",
            "         Identity-19          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-20          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22             [-1, 16, 1, 1]           4,112\n",
            "         Identity-23             [-1, 16, 1, 1]               0\n",
            "             SiLU-24             [-1, 16, 1, 1]               0\n",
            "           Conv2d-25            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-26            [-1, 256, 1, 1]               0\n",
            "         SEModule-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          16,448\n",
            "         Identity-29           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-30           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-31           [-1, 56, 56, 64]             128\n",
            "           Linear-32            [-1, 7, 7, 192]          12,480\n",
            "           Linear-33             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-34             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-35             [-1, 7, 7, 64]               0\n",
            "         Identity-36           [-1, 56, 56, 64]               0\n",
            "         Identity-37           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-38           [-1, 56, 56, 64]             128\n",
            "           Linear-39          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-40          [-1, 56, 56, 256]               0\n",
            "          Dropout-41          [-1, 56, 56, 256]               0\n",
            "         Identity-42          [-1, 56, 56, 256]               0\n",
            "           Linear-43           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-44           [-1, 56, 56, 64]               0\n",
            "              Mlp-45           [-1, 56, 56, 64]               0\n",
            "         Identity-46           [-1, 56, 56, 64]               0\n",
            "         Identity-47           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-48           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 64]             128\n",
            "           Linear-50            [-1, 7, 7, 192]          12,480\n",
            "           Linear-51             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-52             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-53             [-1, 7, 7, 64]               0\n",
            "         Identity-54           [-1, 56, 56, 64]               0\n",
            "         Identity-55           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
            "           Linear-57          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-58          [-1, 56, 56, 256]               0\n",
            "          Dropout-59          [-1, 56, 56, 256]               0\n",
            "         Identity-60          [-1, 56, 56, 256]               0\n",
            "           Linear-61           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-62           [-1, 56, 56, 64]               0\n",
            "              Mlp-63           [-1, 56, 56, 64]               0\n",
            "         Identity-64           [-1, 56, 56, 64]               0\n",
            "         Identity-65           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-66           [-1, 56, 56, 64]               0\n",
            "     MaxxVitBlock-67           [-1, 64, 56, 56]               0\n",
            "         Identity-68           [-1, 64, 56, 56]               0\n",
            "         Identity-69           [-1, 64, 56, 56]               0\n",
            "         Identity-70           [-1, 64, 56, 56]               0\n",
            "   BatchNormAct2d-71           [-1, 64, 56, 56]             128\n",
            "         Identity-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73          [-1, 256, 56, 56]          16,384\n",
            "         Identity-74          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-75          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-76          [-1, 256, 56, 56]             512\n",
            "           Conv2d-77          [-1, 256, 56, 56]           2,304\n",
            "         Identity-78          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-79          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-80          [-1, 256, 56, 56]             512\n",
            "           Conv2d-81             [-1, 16, 1, 1]           4,112\n",
            "         Identity-82             [-1, 16, 1, 1]               0\n",
            "             SiLU-83             [-1, 16, 1, 1]               0\n",
            "           Conv2d-84            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-85            [-1, 256, 1, 1]               0\n",
            "         SEModule-86          [-1, 256, 56, 56]               0\n",
            "           Conv2d-87           [-1, 64, 56, 56]          16,448\n",
            "         Identity-88           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-89           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-90           [-1, 56, 56, 64]             128\n",
            "           Linear-91            [-1, 7, 7, 192]          12,480\n",
            "           Linear-92             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-93             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-94             [-1, 7, 7, 64]               0\n",
            "         Identity-95           [-1, 56, 56, 64]               0\n",
            "         Identity-96           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-97           [-1, 56, 56, 64]             128\n",
            "           Linear-98          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-99          [-1, 56, 56, 256]               0\n",
            "         Dropout-100          [-1, 56, 56, 256]               0\n",
            "        Identity-101          [-1, 56, 56, 256]               0\n",
            "          Linear-102           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-103           [-1, 56, 56, 64]               0\n",
            "             Mlp-104           [-1, 56, 56, 64]               0\n",
            "        Identity-105           [-1, 56, 56, 64]               0\n",
            "        Identity-106           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-107           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-108           [-1, 56, 56, 64]             128\n",
            "          Linear-109            [-1, 7, 7, 192]          12,480\n",
            "          Linear-110             [-1, 7, 7, 64]           4,160\n",
            "         Dropout-111             [-1, 7, 7, 64]               0\n",
            "     AttentionCl-112             [-1, 7, 7, 64]               0\n",
            "        Identity-113           [-1, 56, 56, 64]               0\n",
            "        Identity-114           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-115           [-1, 56, 56, 64]             128\n",
            "          Linear-116          [-1, 56, 56, 256]          16,640\n",
            "        GELUTanh-117          [-1, 56, 56, 256]               0\n",
            "         Dropout-118          [-1, 56, 56, 256]               0\n",
            "        Identity-119          [-1, 56, 56, 256]               0\n",
            "          Linear-120           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-121           [-1, 56, 56, 64]               0\n",
            "             Mlp-122           [-1, 56, 56, 64]               0\n",
            "        Identity-123           [-1, 56, 56, 64]               0\n",
            "        Identity-124           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-125           [-1, 56, 56, 64]               0\n",
            "    MaxxVitBlock-126           [-1, 64, 56, 56]               0\n",
            "    MaxxVitStage-127           [-1, 64, 56, 56]               0\n",
            "   AvgPool2dSame-128           [-1, 64, 28, 28]               0\n",
            "          Conv2d-129          [-1, 128, 28, 28]           8,320\n",
            "    Downsample2d-130          [-1, 128, 28, 28]               0\n",
            "        Identity-131           [-1, 64, 56, 56]               0\n",
            "        Identity-132           [-1, 64, 56, 56]               0\n",
            "  BatchNormAct2d-133           [-1, 64, 56, 56]             128\n",
            "        Identity-134           [-1, 64, 56, 56]               0\n",
            "          Conv2d-135          [-1, 512, 56, 56]          32,768\n",
            "        Identity-136          [-1, 512, 56, 56]               0\n",
            "        GELUTanh-137          [-1, 512, 56, 56]               0\n",
            "  BatchNormAct2d-138          [-1, 512, 56, 56]           1,024\n",
            "      Conv2dSame-139          [-1, 512, 28, 28]           4,608\n",
            "        Identity-140          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-141          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-142          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-143             [-1, 32, 1, 1]          16,416\n",
            "        Identity-144             [-1, 32, 1, 1]               0\n",
            "            SiLU-145             [-1, 32, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-147            [-1, 512, 1, 1]               0\n",
            "        SEModule-148          [-1, 512, 28, 28]               0\n",
            "          Conv2d-149          [-1, 128, 28, 28]          65,664\n",
            "        Identity-150          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-151          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-152          [-1, 28, 28, 128]             256\n",
            "          Linear-153            [-1, 7, 7, 384]          49,536\n",
            "          Linear-154            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-155            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-156            [-1, 7, 7, 128]               0\n",
            "        Identity-157          [-1, 28, 28, 128]               0\n",
            "        Identity-158          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-159          [-1, 28, 28, 128]             256\n",
            "          Linear-160          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-161          [-1, 28, 28, 512]               0\n",
            "         Dropout-162          [-1, 28, 28, 512]               0\n",
            "        Identity-163          [-1, 28, 28, 512]               0\n",
            "          Linear-164          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-165          [-1, 28, 28, 128]               0\n",
            "             Mlp-166          [-1, 28, 28, 128]               0\n",
            "        Identity-167          [-1, 28, 28, 128]               0\n",
            "        Identity-168          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-169          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-170          [-1, 28, 28, 128]             256\n",
            "          Linear-171            [-1, 7, 7, 384]          49,536\n",
            "          Linear-172            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-173            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-174            [-1, 7, 7, 128]               0\n",
            "        Identity-175          [-1, 28, 28, 128]               0\n",
            "        Identity-176          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-177          [-1, 28, 28, 128]             256\n",
            "          Linear-178          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-179          [-1, 28, 28, 512]               0\n",
            "         Dropout-180          [-1, 28, 28, 512]               0\n",
            "        Identity-181          [-1, 28, 28, 512]               0\n",
            "          Linear-182          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-183          [-1, 28, 28, 128]               0\n",
            "             Mlp-184          [-1, 28, 28, 128]               0\n",
            "        Identity-185          [-1, 28, 28, 128]               0\n",
            "        Identity-186          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-187          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-188          [-1, 128, 28, 28]               0\n",
            "        Identity-189          [-1, 128, 28, 28]               0\n",
            "        Identity-190          [-1, 128, 28, 28]               0\n",
            "        Identity-191          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-192          [-1, 128, 28, 28]             256\n",
            "        Identity-193          [-1, 128, 28, 28]               0\n",
            "          Conv2d-194          [-1, 512, 28, 28]          65,536\n",
            "        Identity-195          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-196          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-197          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-198          [-1, 512, 28, 28]           4,608\n",
            "        Identity-199          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-200          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-201          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-202             [-1, 32, 1, 1]          16,416\n",
            "        Identity-203             [-1, 32, 1, 1]               0\n",
            "            SiLU-204             [-1, 32, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-206            [-1, 512, 1, 1]               0\n",
            "        SEModule-207          [-1, 512, 28, 28]               0\n",
            "          Conv2d-208          [-1, 128, 28, 28]          65,664\n",
            "        Identity-209          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-210          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-211          [-1, 28, 28, 128]             256\n",
            "          Linear-212            [-1, 7, 7, 384]          49,536\n",
            "          Linear-213            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-214            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-215            [-1, 7, 7, 128]               0\n",
            "        Identity-216          [-1, 28, 28, 128]               0\n",
            "        Identity-217          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-218          [-1, 28, 28, 128]             256\n",
            "          Linear-219          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-220          [-1, 28, 28, 512]               0\n",
            "         Dropout-221          [-1, 28, 28, 512]               0\n",
            "        Identity-222          [-1, 28, 28, 512]               0\n",
            "          Linear-223          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-224          [-1, 28, 28, 128]               0\n",
            "             Mlp-225          [-1, 28, 28, 128]               0\n",
            "        Identity-226          [-1, 28, 28, 128]               0\n",
            "        Identity-227          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-228          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-229          [-1, 28, 28, 128]             256\n",
            "          Linear-230            [-1, 7, 7, 384]          49,536\n",
            "          Linear-231            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-232            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-233            [-1, 7, 7, 128]               0\n",
            "        Identity-234          [-1, 28, 28, 128]               0\n",
            "        Identity-235          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-236          [-1, 28, 28, 128]             256\n",
            "          Linear-237          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-238          [-1, 28, 28, 512]               0\n",
            "         Dropout-239          [-1, 28, 28, 512]               0\n",
            "        Identity-240          [-1, 28, 28, 512]               0\n",
            "          Linear-241          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-242          [-1, 28, 28, 128]               0\n",
            "             Mlp-243          [-1, 28, 28, 128]               0\n",
            "        Identity-244          [-1, 28, 28, 128]               0\n",
            "        Identity-245          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-246          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-247          [-1, 128, 28, 28]               0\n",
            "    MaxxVitStage-248          [-1, 128, 28, 28]               0\n",
            "   AvgPool2dSame-249          [-1, 128, 14, 14]               0\n",
            "          Conv2d-250          [-1, 256, 14, 14]          33,024\n",
            "    Downsample2d-251          [-1, 256, 14, 14]               0\n",
            "        Identity-252          [-1, 128, 28, 28]               0\n",
            "        Identity-253          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-254          [-1, 128, 28, 28]             256\n",
            "        Identity-255          [-1, 128, 28, 28]               0\n",
            "          Conv2d-256         [-1, 1024, 28, 28]         131,072\n",
            "        Identity-257         [-1, 1024, 28, 28]               0\n",
            "        GELUTanh-258         [-1, 1024, 28, 28]               0\n",
            "  BatchNormAct2d-259         [-1, 1024, 28, 28]           2,048\n",
            "      Conv2dSame-260         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-261         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-262         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-263         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-264             [-1, 64, 1, 1]          65,600\n",
            "        Identity-265             [-1, 64, 1, 1]               0\n",
            "            SiLU-266             [-1, 64, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-268           [-1, 1024, 1, 1]               0\n",
            "        SEModule-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "        Identity-271          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-272          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-273          [-1, 14, 14, 256]             512\n",
            "          Linear-274            [-1, 7, 7, 768]         197,376\n",
            "          Linear-275            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-276            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-277            [-1, 7, 7, 256]               0\n",
            "        Identity-278          [-1, 14, 14, 256]               0\n",
            "        Identity-279          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-280          [-1, 14, 14, 256]             512\n",
            "          Linear-281         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-282         [-1, 14, 14, 1024]               0\n",
            "         Dropout-283         [-1, 14, 14, 1024]               0\n",
            "        Identity-284         [-1, 14, 14, 1024]               0\n",
            "          Linear-285          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-286          [-1, 14, 14, 256]               0\n",
            "             Mlp-287          [-1, 14, 14, 256]               0\n",
            "        Identity-288          [-1, 14, 14, 256]               0\n",
            "        Identity-289          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-290          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-291          [-1, 14, 14, 256]             512\n",
            "          Linear-292            [-1, 7, 7, 768]         197,376\n",
            "          Linear-293            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-294            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-295            [-1, 7, 7, 256]               0\n",
            "        Identity-296          [-1, 14, 14, 256]               0\n",
            "        Identity-297          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-298          [-1, 14, 14, 256]             512\n",
            "          Linear-299         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-300         [-1, 14, 14, 1024]               0\n",
            "         Dropout-301         [-1, 14, 14, 1024]               0\n",
            "        Identity-302         [-1, 14, 14, 1024]               0\n",
            "          Linear-303          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-304          [-1, 14, 14, 256]               0\n",
            "             Mlp-305          [-1, 14, 14, 256]               0\n",
            "        Identity-306          [-1, 14, 14, 256]               0\n",
            "        Identity-307          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-308          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-309          [-1, 256, 14, 14]               0\n",
            "        Identity-310          [-1, 256, 14, 14]               0\n",
            "        Identity-311          [-1, 256, 14, 14]               0\n",
            "        Identity-312          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-313          [-1, 256, 14, 14]             512\n",
            "        Identity-314          [-1, 256, 14, 14]               0\n",
            "          Conv2d-315         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-316         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-317         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-318         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-319         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-320         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-321         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-322         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-323             [-1, 64, 1, 1]          65,600\n",
            "        Identity-324             [-1, 64, 1, 1]               0\n",
            "            SiLU-325             [-1, 64, 1, 1]               0\n",
            "          Conv2d-326           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-327           [-1, 1024, 1, 1]               0\n",
            "        SEModule-328         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-329          [-1, 256, 14, 14]         262,400\n",
            "        Identity-330          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-331          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-332          [-1, 14, 14, 256]             512\n",
            "          Linear-333            [-1, 7, 7, 768]         197,376\n",
            "          Linear-334            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-335            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-336            [-1, 7, 7, 256]               0\n",
            "        Identity-337          [-1, 14, 14, 256]               0\n",
            "        Identity-338          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-339          [-1, 14, 14, 256]             512\n",
            "          Linear-340         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-341         [-1, 14, 14, 1024]               0\n",
            "         Dropout-342         [-1, 14, 14, 1024]               0\n",
            "        Identity-343         [-1, 14, 14, 1024]               0\n",
            "          Linear-344          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-345          [-1, 14, 14, 256]               0\n",
            "             Mlp-346          [-1, 14, 14, 256]               0\n",
            "        Identity-347          [-1, 14, 14, 256]               0\n",
            "        Identity-348          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-349          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-350          [-1, 14, 14, 256]             512\n",
            "          Linear-351            [-1, 7, 7, 768]         197,376\n",
            "          Linear-352            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-353            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-354            [-1, 7, 7, 256]               0\n",
            "        Identity-355          [-1, 14, 14, 256]               0\n",
            "        Identity-356          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 256]             512\n",
            "          Linear-358         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-359         [-1, 14, 14, 1024]               0\n",
            "         Dropout-360         [-1, 14, 14, 1024]               0\n",
            "        Identity-361         [-1, 14, 14, 1024]               0\n",
            "          Linear-362          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-363          [-1, 14, 14, 256]               0\n",
            "             Mlp-364          [-1, 14, 14, 256]               0\n",
            "        Identity-365          [-1, 14, 14, 256]               0\n",
            "        Identity-366          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-367          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-368          [-1, 256, 14, 14]               0\n",
            "        Identity-369          [-1, 256, 14, 14]               0\n",
            "        Identity-370          [-1, 256, 14, 14]               0\n",
            "        Identity-371          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-372          [-1, 256, 14, 14]             512\n",
            "        Identity-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-375         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-376         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-377         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-378         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-379         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-380         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-381         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-382             [-1, 64, 1, 1]          65,600\n",
            "        Identity-383             [-1, 64, 1, 1]               0\n",
            "            SiLU-384             [-1, 64, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-386           [-1, 1024, 1, 1]               0\n",
            "        SEModule-387         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-388          [-1, 256, 14, 14]         262,400\n",
            "        Identity-389          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-390          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-391          [-1, 14, 14, 256]             512\n",
            "          Linear-392            [-1, 7, 7, 768]         197,376\n",
            "          Linear-393            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-394            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-395            [-1, 7, 7, 256]               0\n",
            "        Identity-396          [-1, 14, 14, 256]               0\n",
            "        Identity-397          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-398          [-1, 14, 14, 256]             512\n",
            "          Linear-399         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-400         [-1, 14, 14, 1024]               0\n",
            "         Dropout-401         [-1, 14, 14, 1024]               0\n",
            "        Identity-402         [-1, 14, 14, 1024]               0\n",
            "          Linear-403          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-404          [-1, 14, 14, 256]               0\n",
            "             Mlp-405          [-1, 14, 14, 256]               0\n",
            "        Identity-406          [-1, 14, 14, 256]               0\n",
            "        Identity-407          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-408          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-409          [-1, 14, 14, 256]             512\n",
            "          Linear-410            [-1, 7, 7, 768]         197,376\n",
            "          Linear-411            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-412            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-413            [-1, 7, 7, 256]               0\n",
            "        Identity-414          [-1, 14, 14, 256]               0\n",
            "        Identity-415          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-416          [-1, 14, 14, 256]             512\n",
            "          Linear-417         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-418         [-1, 14, 14, 1024]               0\n",
            "         Dropout-419         [-1, 14, 14, 1024]               0\n",
            "        Identity-420         [-1, 14, 14, 1024]               0\n",
            "          Linear-421          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-422          [-1, 14, 14, 256]               0\n",
            "             Mlp-423          [-1, 14, 14, 256]               0\n",
            "        Identity-424          [-1, 14, 14, 256]               0\n",
            "        Identity-425          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-426          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-427          [-1, 256, 14, 14]               0\n",
            "        Identity-428          [-1, 256, 14, 14]               0\n",
            "        Identity-429          [-1, 256, 14, 14]               0\n",
            "        Identity-430          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-431          [-1, 256, 14, 14]             512\n",
            "        Identity-432          [-1, 256, 14, 14]               0\n",
            "          Conv2d-433         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-434         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-435         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-436         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-437         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-438         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-439         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-440         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-441             [-1, 64, 1, 1]          65,600\n",
            "        Identity-442             [-1, 64, 1, 1]               0\n",
            "            SiLU-443             [-1, 64, 1, 1]               0\n",
            "          Conv2d-444           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-445           [-1, 1024, 1, 1]               0\n",
            "        SEModule-446         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-447          [-1, 256, 14, 14]         262,400\n",
            "        Identity-448          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-449          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-450          [-1, 14, 14, 256]             512\n",
            "          Linear-451            [-1, 7, 7, 768]         197,376\n",
            "          Linear-452            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-453            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-454            [-1, 7, 7, 256]               0\n",
            "        Identity-455          [-1, 14, 14, 256]               0\n",
            "        Identity-456          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-457          [-1, 14, 14, 256]             512\n",
            "          Linear-458         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-459         [-1, 14, 14, 1024]               0\n",
            "         Dropout-460         [-1, 14, 14, 1024]               0\n",
            "        Identity-461         [-1, 14, 14, 1024]               0\n",
            "          Linear-462          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-463          [-1, 14, 14, 256]               0\n",
            "             Mlp-464          [-1, 14, 14, 256]               0\n",
            "        Identity-465          [-1, 14, 14, 256]               0\n",
            "        Identity-466          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-467          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-468          [-1, 14, 14, 256]             512\n",
            "          Linear-469            [-1, 7, 7, 768]         197,376\n",
            "          Linear-470            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-471            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-472            [-1, 7, 7, 256]               0\n",
            "        Identity-473          [-1, 14, 14, 256]               0\n",
            "        Identity-474          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-475          [-1, 14, 14, 256]             512\n",
            "          Linear-476         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-477         [-1, 14, 14, 1024]               0\n",
            "         Dropout-478         [-1, 14, 14, 1024]               0\n",
            "        Identity-479         [-1, 14, 14, 1024]               0\n",
            "          Linear-480          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-481          [-1, 14, 14, 256]               0\n",
            "             Mlp-482          [-1, 14, 14, 256]               0\n",
            "        Identity-483          [-1, 14, 14, 256]               0\n",
            "        Identity-484          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-485          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-486          [-1, 256, 14, 14]               0\n",
            "        Identity-487          [-1, 256, 14, 14]               0\n",
            "        Identity-488          [-1, 256, 14, 14]               0\n",
            "        Identity-489          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-490          [-1, 256, 14, 14]             512\n",
            "        Identity-491          [-1, 256, 14, 14]               0\n",
            "          Conv2d-492         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-493         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-494         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-495         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-496         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-497         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-498         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-499         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-500             [-1, 64, 1, 1]          65,600\n",
            "        Identity-501             [-1, 64, 1, 1]               0\n",
            "            SiLU-502             [-1, 64, 1, 1]               0\n",
            "          Conv2d-503           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-504           [-1, 1024, 1, 1]               0\n",
            "        SEModule-505         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-506          [-1, 256, 14, 14]         262,400\n",
            "        Identity-507          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-508          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 256]             512\n",
            "          Linear-510            [-1, 7, 7, 768]         197,376\n",
            "          Linear-511            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-512            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-513            [-1, 7, 7, 256]               0\n",
            "        Identity-514          [-1, 14, 14, 256]               0\n",
            "        Identity-515          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-516          [-1, 14, 14, 256]             512\n",
            "          Linear-517         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-518         [-1, 14, 14, 1024]               0\n",
            "         Dropout-519         [-1, 14, 14, 1024]               0\n",
            "        Identity-520         [-1, 14, 14, 1024]               0\n",
            "          Linear-521          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-522          [-1, 14, 14, 256]               0\n",
            "             Mlp-523          [-1, 14, 14, 256]               0\n",
            "        Identity-524          [-1, 14, 14, 256]               0\n",
            "        Identity-525          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-526          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-527          [-1, 14, 14, 256]             512\n",
            "          Linear-528            [-1, 7, 7, 768]         197,376\n",
            "          Linear-529            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-530            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-531            [-1, 7, 7, 256]               0\n",
            "        Identity-532          [-1, 14, 14, 256]               0\n",
            "        Identity-533          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-534          [-1, 14, 14, 256]             512\n",
            "          Linear-535         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-536         [-1, 14, 14, 1024]               0\n",
            "         Dropout-537         [-1, 14, 14, 1024]               0\n",
            "        Identity-538         [-1, 14, 14, 1024]               0\n",
            "          Linear-539          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-540          [-1, 14, 14, 256]               0\n",
            "             Mlp-541          [-1, 14, 14, 256]               0\n",
            "        Identity-542          [-1, 14, 14, 256]               0\n",
            "        Identity-543          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-544          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-545          [-1, 256, 14, 14]               0\n",
            "    MaxxVitStage-546          [-1, 256, 14, 14]               0\n",
            "   AvgPool2dSame-547            [-1, 256, 7, 7]               0\n",
            "          Conv2d-548            [-1, 512, 7, 7]         131,584\n",
            "    Downsample2d-549            [-1, 512, 7, 7]               0\n",
            "        Identity-550          [-1, 256, 14, 14]               0\n",
            "        Identity-551          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-552          [-1, 256, 14, 14]             512\n",
            "        Identity-553          [-1, 256, 14, 14]               0\n",
            "          Conv2d-554         [-1, 2048, 14, 14]         524,288\n",
            "        Identity-555         [-1, 2048, 14, 14]               0\n",
            "        GELUTanh-556         [-1, 2048, 14, 14]               0\n",
            "  BatchNormAct2d-557         [-1, 2048, 14, 14]           4,096\n",
            "      Conv2dSame-558           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-559           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-560           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-561           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-562            [-1, 128, 1, 1]         262,272\n",
            "        Identity-563            [-1, 128, 1, 1]               0\n",
            "            SiLU-564            [-1, 128, 1, 1]               0\n",
            "          Conv2d-565           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-566           [-1, 2048, 1, 1]               0\n",
            "        SEModule-567           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-568            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-569            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-570            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-571            [-1, 7, 7, 512]           1,024\n",
            "          Linear-572           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-573            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-574            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-575            [-1, 7, 7, 512]               0\n",
            "        Identity-576            [-1, 7, 7, 512]               0\n",
            "        Identity-577            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-578            [-1, 7, 7, 512]           1,024\n",
            "          Linear-579           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-580           [-1, 7, 7, 2048]               0\n",
            "         Dropout-581           [-1, 7, 7, 2048]               0\n",
            "        Identity-582           [-1, 7, 7, 2048]               0\n",
            "          Linear-583            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-584            [-1, 7, 7, 512]               0\n",
            "             Mlp-585            [-1, 7, 7, 512]               0\n",
            "        Identity-586            [-1, 7, 7, 512]               0\n",
            "        Identity-587            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-588            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-589            [-1, 7, 7, 512]           1,024\n",
            "          Linear-590           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-591            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-592            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-593            [-1, 7, 7, 512]               0\n",
            "        Identity-594            [-1, 7, 7, 512]               0\n",
            "        Identity-595            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-596            [-1, 7, 7, 512]           1,024\n",
            "          Linear-597           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-598           [-1, 7, 7, 2048]               0\n",
            "         Dropout-599           [-1, 7, 7, 2048]               0\n",
            "        Identity-600           [-1, 7, 7, 2048]               0\n",
            "          Linear-601            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-602            [-1, 7, 7, 512]               0\n",
            "             Mlp-603            [-1, 7, 7, 512]               0\n",
            "        Identity-604            [-1, 7, 7, 512]               0\n",
            "        Identity-605            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-606            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-607            [-1, 512, 7, 7]               0\n",
            "        Identity-608            [-1, 512, 7, 7]               0\n",
            "        Identity-609            [-1, 512, 7, 7]               0\n",
            "        Identity-610            [-1, 512, 7, 7]               0\n",
            "  BatchNormAct2d-611            [-1, 512, 7, 7]           1,024\n",
            "        Identity-612            [-1, 512, 7, 7]               0\n",
            "          Conv2d-613           [-1, 2048, 7, 7]       1,048,576\n",
            "        Identity-614           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-615           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-616           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-617           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-618           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-619           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-620           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-621            [-1, 128, 1, 1]         262,272\n",
            "        Identity-622            [-1, 128, 1, 1]               0\n",
            "            SiLU-623            [-1, 128, 1, 1]               0\n",
            "          Conv2d-624           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-625           [-1, 2048, 1, 1]               0\n",
            "        SEModule-626           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-627            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-628            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-629            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-630            [-1, 7, 7, 512]           1,024\n",
            "          Linear-631           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-632            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-633            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-634            [-1, 7, 7, 512]               0\n",
            "        Identity-635            [-1, 7, 7, 512]               0\n",
            "        Identity-636            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-637            [-1, 7, 7, 512]           1,024\n",
            "          Linear-638           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-639           [-1, 7, 7, 2048]               0\n",
            "         Dropout-640           [-1, 7, 7, 2048]               0\n",
            "        Identity-641           [-1, 7, 7, 2048]               0\n",
            "          Linear-642            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-643            [-1, 7, 7, 512]               0\n",
            "             Mlp-644            [-1, 7, 7, 512]               0\n",
            "        Identity-645            [-1, 7, 7, 512]               0\n",
            "        Identity-646            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-647            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-648            [-1, 7, 7, 512]           1,024\n",
            "          Linear-649           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-650            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-651            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-652            [-1, 7, 7, 512]               0\n",
            "        Identity-653            [-1, 7, 7, 512]               0\n",
            "        Identity-654            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-655            [-1, 7, 7, 512]           1,024\n",
            "          Linear-656           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-657           [-1, 7, 7, 2048]               0\n",
            "         Dropout-658           [-1, 7, 7, 2048]               0\n",
            "        Identity-659           [-1, 7, 7, 2048]               0\n",
            "          Linear-660            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-661            [-1, 7, 7, 512]               0\n",
            "             Mlp-662            [-1, 7, 7, 512]               0\n",
            "        Identity-663            [-1, 7, 7, 512]               0\n",
            "        Identity-664            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-665            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-666            [-1, 512, 7, 7]               0\n",
            "    MaxxVitStage-667            [-1, 512, 7, 7]               0\n",
            "        Identity-668            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-669            [-1, 512, 1, 1]               0\n",
            "        Identity-670            [-1, 512, 1, 1]               0\n",
            "SelectAdaptivePool2d-671            [-1, 512, 1, 1]               0\n",
            "     LayerNorm2d-672            [-1, 512, 1, 1]           1,024\n",
            "         Flatten-673                  [-1, 512]               0\n",
            "          Linear-674                  [-1, 512]         262,656\n",
            "            Tanh-675                  [-1, 512]               0\n",
            "         Dropout-676                  [-1, 512]               0\n",
            "          Linear-677                 [-1, 1000]         513,000\n",
            "NormMlpClassifierHead-678                 [-1, 1000]               0\n",
            "================================================================\n",
            "Total params: 30,888,136\n",
            "Trainable params: 30,888,136\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 887.11\n",
            "Params size (MB): 117.83\n",
            "Estimated Total Size (MB): 1005.51\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a counter for the number of layers\n",
        "num_layers = 0\n",
        "\n",
        "# Iterate through the model's children and count the number of layers\n",
        "for child in model.children():\n",
        "    num_layers += 1\n",
        "\n",
        "print(\"Number of layers in the model:\", num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjm5pBbjJh0j",
        "outputId": "e581b374-7315-4364-be83-096bec26e4ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the model: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Define a function to select the first three layers from the model\n",
        "def select_first_three_layers(model):\n",
        "    first_three_layers = []\n",
        "    num_layers = 0\n",
        "    for name, module in model.named_children():\n",
        "        first_three_layers.append(module)\n",
        "        num_layers += 1\n",
        "        if num_layers == 3:\n",
        "            break\n",
        "    return nn.Sequential(*first_three_layers)\n",
        "\n",
        "# Select the first three layers\n",
        "first_three_layers_model = select_first_three_layers(model)\n",
        "\n",
        "# Use summary to display information for the first three layers\n",
        "summary(first_three_layers_model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHMdn-ULJx_9",
        "outputId": "609fe853-ce50-49df-8989-bc13333d022d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "        Conv2dSame-1         [-1, 64, 112, 112]           1,792\n",
            "          Identity-2         [-1, 64, 112, 112]               0\n",
            "          GELUTanh-3         [-1, 64, 112, 112]               0\n",
            "    BatchNormAct2d-4         [-1, 64, 112, 112]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,928\n",
            "              Stem-6         [-1, 64, 112, 112]               0\n",
            "     AvgPool2dSame-7           [-1, 64, 56, 56]               0\n",
            "          Identity-8           [-1, 64, 56, 56]               0\n",
            "      Downsample2d-9           [-1, 64, 56, 56]               0\n",
            "         Identity-10         [-1, 64, 112, 112]               0\n",
            "         Identity-11         [-1, 64, 112, 112]               0\n",
            "   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n",
            "         Identity-13         [-1, 64, 112, 112]               0\n",
            "           Conv2d-14        [-1, 256, 112, 112]          16,384\n",
            "         Identity-15        [-1, 256, 112, 112]               0\n",
            "         GELUTanh-16        [-1, 256, 112, 112]               0\n",
            "   BatchNormAct2d-17        [-1, 256, 112, 112]             512\n",
            "       Conv2dSame-18          [-1, 256, 56, 56]           2,304\n",
            "         Identity-19          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-20          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22             [-1, 16, 1, 1]           4,112\n",
            "         Identity-23             [-1, 16, 1, 1]               0\n",
            "             SiLU-24             [-1, 16, 1, 1]               0\n",
            "           Conv2d-25            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-26            [-1, 256, 1, 1]               0\n",
            "         SEModule-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          16,448\n",
            "         Identity-29           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-30           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-31           [-1, 56, 56, 64]             128\n",
            "           Linear-32            [-1, 7, 7, 192]          12,480\n",
            "           Linear-33             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-34             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-35             [-1, 7, 7, 64]               0\n",
            "         Identity-36           [-1, 56, 56, 64]               0\n",
            "         Identity-37           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-38           [-1, 56, 56, 64]             128\n",
            "           Linear-39          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-40          [-1, 56, 56, 256]               0\n",
            "          Dropout-41          [-1, 56, 56, 256]               0\n",
            "         Identity-42          [-1, 56, 56, 256]               0\n",
            "           Linear-43           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-44           [-1, 56, 56, 64]               0\n",
            "              Mlp-45           [-1, 56, 56, 64]               0\n",
            "         Identity-46           [-1, 56, 56, 64]               0\n",
            "         Identity-47           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-48           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 64]             128\n",
            "           Linear-50            [-1, 7, 7, 192]          12,480\n",
            "           Linear-51             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-52             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-53             [-1, 7, 7, 64]               0\n",
            "         Identity-54           [-1, 56, 56, 64]               0\n",
            "         Identity-55           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
            "           Linear-57          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-58          [-1, 56, 56, 256]               0\n",
            "          Dropout-59          [-1, 56, 56, 256]               0\n",
            "         Identity-60          [-1, 56, 56, 256]               0\n",
            "           Linear-61           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-62           [-1, 56, 56, 64]               0\n",
            "              Mlp-63           [-1, 56, 56, 64]               0\n",
            "         Identity-64           [-1, 56, 56, 64]               0\n",
            "         Identity-65           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-66           [-1, 56, 56, 64]               0\n",
            "     MaxxVitBlock-67           [-1, 64, 56, 56]               0\n",
            "         Identity-68           [-1, 64, 56, 56]               0\n",
            "         Identity-69           [-1, 64, 56, 56]               0\n",
            "         Identity-70           [-1, 64, 56, 56]               0\n",
            "   BatchNormAct2d-71           [-1, 64, 56, 56]             128\n",
            "         Identity-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73          [-1, 256, 56, 56]          16,384\n",
            "         Identity-74          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-75          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-76          [-1, 256, 56, 56]             512\n",
            "           Conv2d-77          [-1, 256, 56, 56]           2,304\n",
            "         Identity-78          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-79          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-80          [-1, 256, 56, 56]             512\n",
            "           Conv2d-81             [-1, 16, 1, 1]           4,112\n",
            "         Identity-82             [-1, 16, 1, 1]               0\n",
            "             SiLU-83             [-1, 16, 1, 1]               0\n",
            "           Conv2d-84            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-85            [-1, 256, 1, 1]               0\n",
            "         SEModule-86          [-1, 256, 56, 56]               0\n",
            "           Conv2d-87           [-1, 64, 56, 56]          16,448\n",
            "         Identity-88           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-89           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-90           [-1, 56, 56, 64]             128\n",
            "           Linear-91            [-1, 7, 7, 192]          12,480\n",
            "           Linear-92             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-93             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-94             [-1, 7, 7, 64]               0\n",
            "         Identity-95           [-1, 56, 56, 64]               0\n",
            "         Identity-96           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-97           [-1, 56, 56, 64]             128\n",
            "           Linear-98          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-99          [-1, 56, 56, 256]               0\n",
            "         Dropout-100          [-1, 56, 56, 256]               0\n",
            "        Identity-101          [-1, 56, 56, 256]               0\n",
            "          Linear-102           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-103           [-1, 56, 56, 64]               0\n",
            "             Mlp-104           [-1, 56, 56, 64]               0\n",
            "        Identity-105           [-1, 56, 56, 64]               0\n",
            "        Identity-106           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-107           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-108           [-1, 56, 56, 64]             128\n",
            "          Linear-109            [-1, 7, 7, 192]          12,480\n",
            "          Linear-110             [-1, 7, 7, 64]           4,160\n",
            "         Dropout-111             [-1, 7, 7, 64]               0\n",
            "     AttentionCl-112             [-1, 7, 7, 64]               0\n",
            "        Identity-113           [-1, 56, 56, 64]               0\n",
            "        Identity-114           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-115           [-1, 56, 56, 64]             128\n",
            "          Linear-116          [-1, 56, 56, 256]          16,640\n",
            "        GELUTanh-117          [-1, 56, 56, 256]               0\n",
            "         Dropout-118          [-1, 56, 56, 256]               0\n",
            "        Identity-119          [-1, 56, 56, 256]               0\n",
            "          Linear-120           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-121           [-1, 56, 56, 64]               0\n",
            "             Mlp-122           [-1, 56, 56, 64]               0\n",
            "        Identity-123           [-1, 56, 56, 64]               0\n",
            "        Identity-124           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-125           [-1, 56, 56, 64]               0\n",
            "    MaxxVitBlock-126           [-1, 64, 56, 56]               0\n",
            "    MaxxVitStage-127           [-1, 64, 56, 56]               0\n",
            "   AvgPool2dSame-128           [-1, 64, 28, 28]               0\n",
            "          Conv2d-129          [-1, 128, 28, 28]           8,320\n",
            "    Downsample2d-130          [-1, 128, 28, 28]               0\n",
            "        Identity-131           [-1, 64, 56, 56]               0\n",
            "        Identity-132           [-1, 64, 56, 56]               0\n",
            "  BatchNormAct2d-133           [-1, 64, 56, 56]             128\n",
            "        Identity-134           [-1, 64, 56, 56]               0\n",
            "          Conv2d-135          [-1, 512, 56, 56]          32,768\n",
            "        Identity-136          [-1, 512, 56, 56]               0\n",
            "        GELUTanh-137          [-1, 512, 56, 56]               0\n",
            "  BatchNormAct2d-138          [-1, 512, 56, 56]           1,024\n",
            "      Conv2dSame-139          [-1, 512, 28, 28]           4,608\n",
            "        Identity-140          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-141          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-142          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-143             [-1, 32, 1, 1]          16,416\n",
            "        Identity-144             [-1, 32, 1, 1]               0\n",
            "            SiLU-145             [-1, 32, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-147            [-1, 512, 1, 1]               0\n",
            "        SEModule-148          [-1, 512, 28, 28]               0\n",
            "          Conv2d-149          [-1, 128, 28, 28]          65,664\n",
            "        Identity-150          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-151          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-152          [-1, 28, 28, 128]             256\n",
            "          Linear-153            [-1, 7, 7, 384]          49,536\n",
            "          Linear-154            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-155            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-156            [-1, 7, 7, 128]               0\n",
            "        Identity-157          [-1, 28, 28, 128]               0\n",
            "        Identity-158          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-159          [-1, 28, 28, 128]             256\n",
            "          Linear-160          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-161          [-1, 28, 28, 512]               0\n",
            "         Dropout-162          [-1, 28, 28, 512]               0\n",
            "        Identity-163          [-1, 28, 28, 512]               0\n",
            "          Linear-164          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-165          [-1, 28, 28, 128]               0\n",
            "             Mlp-166          [-1, 28, 28, 128]               0\n",
            "        Identity-167          [-1, 28, 28, 128]               0\n",
            "        Identity-168          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-169          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-170          [-1, 28, 28, 128]             256\n",
            "          Linear-171            [-1, 7, 7, 384]          49,536\n",
            "          Linear-172            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-173            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-174            [-1, 7, 7, 128]               0\n",
            "        Identity-175          [-1, 28, 28, 128]               0\n",
            "        Identity-176          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-177          [-1, 28, 28, 128]             256\n",
            "          Linear-178          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-179          [-1, 28, 28, 512]               0\n",
            "         Dropout-180          [-1, 28, 28, 512]               0\n",
            "        Identity-181          [-1, 28, 28, 512]               0\n",
            "          Linear-182          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-183          [-1, 28, 28, 128]               0\n",
            "             Mlp-184          [-1, 28, 28, 128]               0\n",
            "        Identity-185          [-1, 28, 28, 128]               0\n",
            "        Identity-186          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-187          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-188          [-1, 128, 28, 28]               0\n",
            "        Identity-189          [-1, 128, 28, 28]               0\n",
            "        Identity-190          [-1, 128, 28, 28]               0\n",
            "        Identity-191          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-192          [-1, 128, 28, 28]             256\n",
            "        Identity-193          [-1, 128, 28, 28]               0\n",
            "          Conv2d-194          [-1, 512, 28, 28]          65,536\n",
            "        Identity-195          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-196          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-197          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-198          [-1, 512, 28, 28]           4,608\n",
            "        Identity-199          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-200          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-201          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-202             [-1, 32, 1, 1]          16,416\n",
            "        Identity-203             [-1, 32, 1, 1]               0\n",
            "            SiLU-204             [-1, 32, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-206            [-1, 512, 1, 1]               0\n",
            "        SEModule-207          [-1, 512, 28, 28]               0\n",
            "          Conv2d-208          [-1, 128, 28, 28]          65,664\n",
            "        Identity-209          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-210          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-211          [-1, 28, 28, 128]             256\n",
            "          Linear-212            [-1, 7, 7, 384]          49,536\n",
            "          Linear-213            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-214            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-215            [-1, 7, 7, 128]               0\n",
            "        Identity-216          [-1, 28, 28, 128]               0\n",
            "        Identity-217          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-218          [-1, 28, 28, 128]             256\n",
            "          Linear-219          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-220          [-1, 28, 28, 512]               0\n",
            "         Dropout-221          [-1, 28, 28, 512]               0\n",
            "        Identity-222          [-1, 28, 28, 512]               0\n",
            "          Linear-223          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-224          [-1, 28, 28, 128]               0\n",
            "             Mlp-225          [-1, 28, 28, 128]               0\n",
            "        Identity-226          [-1, 28, 28, 128]               0\n",
            "        Identity-227          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-228          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-229          [-1, 28, 28, 128]             256\n",
            "          Linear-230            [-1, 7, 7, 384]          49,536\n",
            "          Linear-231            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-232            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-233            [-1, 7, 7, 128]               0\n",
            "        Identity-234          [-1, 28, 28, 128]               0\n",
            "        Identity-235          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-236          [-1, 28, 28, 128]             256\n",
            "          Linear-237          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-238          [-1, 28, 28, 512]               0\n",
            "         Dropout-239          [-1, 28, 28, 512]               0\n",
            "        Identity-240          [-1, 28, 28, 512]               0\n",
            "          Linear-241          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-242          [-1, 28, 28, 128]               0\n",
            "             Mlp-243          [-1, 28, 28, 128]               0\n",
            "        Identity-244          [-1, 28, 28, 128]               0\n",
            "        Identity-245          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-246          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-247          [-1, 128, 28, 28]               0\n",
            "    MaxxVitStage-248          [-1, 128, 28, 28]               0\n",
            "   AvgPool2dSame-249          [-1, 128, 14, 14]               0\n",
            "          Conv2d-250          [-1, 256, 14, 14]          33,024\n",
            "    Downsample2d-251          [-1, 256, 14, 14]               0\n",
            "        Identity-252          [-1, 128, 28, 28]               0\n",
            "        Identity-253          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-254          [-1, 128, 28, 28]             256\n",
            "        Identity-255          [-1, 128, 28, 28]               0\n",
            "          Conv2d-256         [-1, 1024, 28, 28]         131,072\n",
            "        Identity-257         [-1, 1024, 28, 28]               0\n",
            "        GELUTanh-258         [-1, 1024, 28, 28]               0\n",
            "  BatchNormAct2d-259         [-1, 1024, 28, 28]           2,048\n",
            "      Conv2dSame-260         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-261         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-262         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-263         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-264             [-1, 64, 1, 1]          65,600\n",
            "        Identity-265             [-1, 64, 1, 1]               0\n",
            "            SiLU-266             [-1, 64, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-268           [-1, 1024, 1, 1]               0\n",
            "        SEModule-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "        Identity-271          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-272          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-273          [-1, 14, 14, 256]             512\n",
            "          Linear-274            [-1, 7, 7, 768]         197,376\n",
            "          Linear-275            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-276            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-277            [-1, 7, 7, 256]               0\n",
            "        Identity-278          [-1, 14, 14, 256]               0\n",
            "        Identity-279          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-280          [-1, 14, 14, 256]             512\n",
            "          Linear-281         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-282         [-1, 14, 14, 1024]               0\n",
            "         Dropout-283         [-1, 14, 14, 1024]               0\n",
            "        Identity-284         [-1, 14, 14, 1024]               0\n",
            "          Linear-285          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-286          [-1, 14, 14, 256]               0\n",
            "             Mlp-287          [-1, 14, 14, 256]               0\n",
            "        Identity-288          [-1, 14, 14, 256]               0\n",
            "        Identity-289          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-290          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-291          [-1, 14, 14, 256]             512\n",
            "          Linear-292            [-1, 7, 7, 768]         197,376\n",
            "          Linear-293            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-294            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-295            [-1, 7, 7, 256]               0\n",
            "        Identity-296          [-1, 14, 14, 256]               0\n",
            "        Identity-297          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-298          [-1, 14, 14, 256]             512\n",
            "          Linear-299         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-300         [-1, 14, 14, 1024]               0\n",
            "         Dropout-301         [-1, 14, 14, 1024]               0\n",
            "        Identity-302         [-1, 14, 14, 1024]               0\n",
            "          Linear-303          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-304          [-1, 14, 14, 256]               0\n",
            "             Mlp-305          [-1, 14, 14, 256]               0\n",
            "        Identity-306          [-1, 14, 14, 256]               0\n",
            "        Identity-307          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-308          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-309          [-1, 256, 14, 14]               0\n",
            "        Identity-310          [-1, 256, 14, 14]               0\n",
            "        Identity-311          [-1, 256, 14, 14]               0\n",
            "        Identity-312          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-313          [-1, 256, 14, 14]             512\n",
            "        Identity-314          [-1, 256, 14, 14]               0\n",
            "          Conv2d-315         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-316         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-317         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-318         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-319         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-320         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-321         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-322         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-323             [-1, 64, 1, 1]          65,600\n",
            "        Identity-324             [-1, 64, 1, 1]               0\n",
            "            SiLU-325             [-1, 64, 1, 1]               0\n",
            "          Conv2d-326           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-327           [-1, 1024, 1, 1]               0\n",
            "        SEModule-328         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-329          [-1, 256, 14, 14]         262,400\n",
            "        Identity-330          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-331          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-332          [-1, 14, 14, 256]             512\n",
            "          Linear-333            [-1, 7, 7, 768]         197,376\n",
            "          Linear-334            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-335            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-336            [-1, 7, 7, 256]               0\n",
            "        Identity-337          [-1, 14, 14, 256]               0\n",
            "        Identity-338          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-339          [-1, 14, 14, 256]             512\n",
            "          Linear-340         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-341         [-1, 14, 14, 1024]               0\n",
            "         Dropout-342         [-1, 14, 14, 1024]               0\n",
            "        Identity-343         [-1, 14, 14, 1024]               0\n",
            "          Linear-344          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-345          [-1, 14, 14, 256]               0\n",
            "             Mlp-346          [-1, 14, 14, 256]               0\n",
            "        Identity-347          [-1, 14, 14, 256]               0\n",
            "        Identity-348          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-349          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-350          [-1, 14, 14, 256]             512\n",
            "          Linear-351            [-1, 7, 7, 768]         197,376\n",
            "          Linear-352            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-353            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-354            [-1, 7, 7, 256]               0\n",
            "        Identity-355          [-1, 14, 14, 256]               0\n",
            "        Identity-356          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 256]             512\n",
            "          Linear-358         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-359         [-1, 14, 14, 1024]               0\n",
            "         Dropout-360         [-1, 14, 14, 1024]               0\n",
            "        Identity-361         [-1, 14, 14, 1024]               0\n",
            "          Linear-362          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-363          [-1, 14, 14, 256]               0\n",
            "             Mlp-364          [-1, 14, 14, 256]               0\n",
            "        Identity-365          [-1, 14, 14, 256]               0\n",
            "        Identity-366          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-367          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-368          [-1, 256, 14, 14]               0\n",
            "        Identity-369          [-1, 256, 14, 14]               0\n",
            "        Identity-370          [-1, 256, 14, 14]               0\n",
            "        Identity-371          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-372          [-1, 256, 14, 14]             512\n",
            "        Identity-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-375         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-376         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-377         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-378         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-379         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-380         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-381         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-382             [-1, 64, 1, 1]          65,600\n",
            "        Identity-383             [-1, 64, 1, 1]               0\n",
            "            SiLU-384             [-1, 64, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-386           [-1, 1024, 1, 1]               0\n",
            "        SEModule-387         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-388          [-1, 256, 14, 14]         262,400\n",
            "        Identity-389          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-390          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-391          [-1, 14, 14, 256]             512\n",
            "          Linear-392            [-1, 7, 7, 768]         197,376\n",
            "          Linear-393            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-394            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-395            [-1, 7, 7, 256]               0\n",
            "        Identity-396          [-1, 14, 14, 256]               0\n",
            "        Identity-397          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-398          [-1, 14, 14, 256]             512\n",
            "          Linear-399         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-400         [-1, 14, 14, 1024]               0\n",
            "         Dropout-401         [-1, 14, 14, 1024]               0\n",
            "        Identity-402         [-1, 14, 14, 1024]               0\n",
            "          Linear-403          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-404          [-1, 14, 14, 256]               0\n",
            "             Mlp-405          [-1, 14, 14, 256]               0\n",
            "        Identity-406          [-1, 14, 14, 256]               0\n",
            "        Identity-407          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-408          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-409          [-1, 14, 14, 256]             512\n",
            "          Linear-410            [-1, 7, 7, 768]         197,376\n",
            "          Linear-411            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-412            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-413            [-1, 7, 7, 256]               0\n",
            "        Identity-414          [-1, 14, 14, 256]               0\n",
            "        Identity-415          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-416          [-1, 14, 14, 256]             512\n",
            "          Linear-417         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-418         [-1, 14, 14, 1024]               0\n",
            "         Dropout-419         [-1, 14, 14, 1024]               0\n",
            "        Identity-420         [-1, 14, 14, 1024]               0\n",
            "          Linear-421          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-422          [-1, 14, 14, 256]               0\n",
            "             Mlp-423          [-1, 14, 14, 256]               0\n",
            "        Identity-424          [-1, 14, 14, 256]               0\n",
            "        Identity-425          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-426          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-427          [-1, 256, 14, 14]               0\n",
            "        Identity-428          [-1, 256, 14, 14]               0\n",
            "        Identity-429          [-1, 256, 14, 14]               0\n",
            "        Identity-430          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-431          [-1, 256, 14, 14]             512\n",
            "        Identity-432          [-1, 256, 14, 14]               0\n",
            "          Conv2d-433         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-434         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-435         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-436         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-437         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-438         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-439         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-440         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-441             [-1, 64, 1, 1]          65,600\n",
            "        Identity-442             [-1, 64, 1, 1]               0\n",
            "            SiLU-443             [-1, 64, 1, 1]               0\n",
            "          Conv2d-444           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-445           [-1, 1024, 1, 1]               0\n",
            "        SEModule-446         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-447          [-1, 256, 14, 14]         262,400\n",
            "        Identity-448          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-449          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-450          [-1, 14, 14, 256]             512\n",
            "          Linear-451            [-1, 7, 7, 768]         197,376\n",
            "          Linear-452            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-453            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-454            [-1, 7, 7, 256]               0\n",
            "        Identity-455          [-1, 14, 14, 256]               0\n",
            "        Identity-456          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-457          [-1, 14, 14, 256]             512\n",
            "          Linear-458         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-459         [-1, 14, 14, 1024]               0\n",
            "         Dropout-460         [-1, 14, 14, 1024]               0\n",
            "        Identity-461         [-1, 14, 14, 1024]               0\n",
            "          Linear-462          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-463          [-1, 14, 14, 256]               0\n",
            "             Mlp-464          [-1, 14, 14, 256]               0\n",
            "        Identity-465          [-1, 14, 14, 256]               0\n",
            "        Identity-466          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-467          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-468          [-1, 14, 14, 256]             512\n",
            "          Linear-469            [-1, 7, 7, 768]         197,376\n",
            "          Linear-470            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-471            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-472            [-1, 7, 7, 256]               0\n",
            "        Identity-473          [-1, 14, 14, 256]               0\n",
            "        Identity-474          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-475          [-1, 14, 14, 256]             512\n",
            "          Linear-476         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-477         [-1, 14, 14, 1024]               0\n",
            "         Dropout-478         [-1, 14, 14, 1024]               0\n",
            "        Identity-479         [-1, 14, 14, 1024]               0\n",
            "          Linear-480          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-481          [-1, 14, 14, 256]               0\n",
            "             Mlp-482          [-1, 14, 14, 256]               0\n",
            "        Identity-483          [-1, 14, 14, 256]               0\n",
            "        Identity-484          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-485          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-486          [-1, 256, 14, 14]               0\n",
            "        Identity-487          [-1, 256, 14, 14]               0\n",
            "        Identity-488          [-1, 256, 14, 14]               0\n",
            "        Identity-489          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-490          [-1, 256, 14, 14]             512\n",
            "        Identity-491          [-1, 256, 14, 14]               0\n",
            "          Conv2d-492         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-493         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-494         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-495         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-496         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-497         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-498         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-499         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-500             [-1, 64, 1, 1]          65,600\n",
            "        Identity-501             [-1, 64, 1, 1]               0\n",
            "            SiLU-502             [-1, 64, 1, 1]               0\n",
            "          Conv2d-503           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-504           [-1, 1024, 1, 1]               0\n",
            "        SEModule-505         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-506          [-1, 256, 14, 14]         262,400\n",
            "        Identity-507          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-508          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 256]             512\n",
            "          Linear-510            [-1, 7, 7, 768]         197,376\n",
            "          Linear-511            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-512            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-513            [-1, 7, 7, 256]               0\n",
            "        Identity-514          [-1, 14, 14, 256]               0\n",
            "        Identity-515          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-516          [-1, 14, 14, 256]             512\n",
            "          Linear-517         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-518         [-1, 14, 14, 1024]               0\n",
            "         Dropout-519         [-1, 14, 14, 1024]               0\n",
            "        Identity-520         [-1, 14, 14, 1024]               0\n",
            "          Linear-521          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-522          [-1, 14, 14, 256]               0\n",
            "             Mlp-523          [-1, 14, 14, 256]               0\n",
            "        Identity-524          [-1, 14, 14, 256]               0\n",
            "        Identity-525          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-526          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-527          [-1, 14, 14, 256]             512\n",
            "          Linear-528            [-1, 7, 7, 768]         197,376\n",
            "          Linear-529            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-530            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-531            [-1, 7, 7, 256]               0\n",
            "        Identity-532          [-1, 14, 14, 256]               0\n",
            "        Identity-533          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-534          [-1, 14, 14, 256]             512\n",
            "          Linear-535         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-536         [-1, 14, 14, 1024]               0\n",
            "         Dropout-537         [-1, 14, 14, 1024]               0\n",
            "        Identity-538         [-1, 14, 14, 1024]               0\n",
            "          Linear-539          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-540          [-1, 14, 14, 256]               0\n",
            "             Mlp-541          [-1, 14, 14, 256]               0\n",
            "        Identity-542          [-1, 14, 14, 256]               0\n",
            "        Identity-543          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-544          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-545          [-1, 256, 14, 14]               0\n",
            "    MaxxVitStage-546          [-1, 256, 14, 14]               0\n",
            "   AvgPool2dSame-547            [-1, 256, 7, 7]               0\n",
            "          Conv2d-548            [-1, 512, 7, 7]         131,584\n",
            "    Downsample2d-549            [-1, 512, 7, 7]               0\n",
            "        Identity-550          [-1, 256, 14, 14]               0\n",
            "        Identity-551          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-552          [-1, 256, 14, 14]             512\n",
            "        Identity-553          [-1, 256, 14, 14]               0\n",
            "          Conv2d-554         [-1, 2048, 14, 14]         524,288\n",
            "        Identity-555         [-1, 2048, 14, 14]               0\n",
            "        GELUTanh-556         [-1, 2048, 14, 14]               0\n",
            "  BatchNormAct2d-557         [-1, 2048, 14, 14]           4,096\n",
            "      Conv2dSame-558           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-559           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-560           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-561           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-562            [-1, 128, 1, 1]         262,272\n",
            "        Identity-563            [-1, 128, 1, 1]               0\n",
            "            SiLU-564            [-1, 128, 1, 1]               0\n",
            "          Conv2d-565           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-566           [-1, 2048, 1, 1]               0\n",
            "        SEModule-567           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-568            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-569            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-570            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-571            [-1, 7, 7, 512]           1,024\n",
            "          Linear-572           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-573            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-574            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-575            [-1, 7, 7, 512]               0\n",
            "        Identity-576            [-1, 7, 7, 512]               0\n",
            "        Identity-577            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-578            [-1, 7, 7, 512]           1,024\n",
            "          Linear-579           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-580           [-1, 7, 7, 2048]               0\n",
            "         Dropout-581           [-1, 7, 7, 2048]               0\n",
            "        Identity-582           [-1, 7, 7, 2048]               0\n",
            "          Linear-583            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-584            [-1, 7, 7, 512]               0\n",
            "             Mlp-585            [-1, 7, 7, 512]               0\n",
            "        Identity-586            [-1, 7, 7, 512]               0\n",
            "        Identity-587            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-588            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-589            [-1, 7, 7, 512]           1,024\n",
            "          Linear-590           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-591            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-592            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-593            [-1, 7, 7, 512]               0\n",
            "        Identity-594            [-1, 7, 7, 512]               0\n",
            "        Identity-595            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-596            [-1, 7, 7, 512]           1,024\n",
            "          Linear-597           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-598           [-1, 7, 7, 2048]               0\n",
            "         Dropout-599           [-1, 7, 7, 2048]               0\n",
            "        Identity-600           [-1, 7, 7, 2048]               0\n",
            "          Linear-601            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-602            [-1, 7, 7, 512]               0\n",
            "             Mlp-603            [-1, 7, 7, 512]               0\n",
            "        Identity-604            [-1, 7, 7, 512]               0\n",
            "        Identity-605            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-606            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-607            [-1, 512, 7, 7]               0\n",
            "        Identity-608            [-1, 512, 7, 7]               0\n",
            "        Identity-609            [-1, 512, 7, 7]               0\n",
            "        Identity-610            [-1, 512, 7, 7]               0\n",
            "  BatchNormAct2d-611            [-1, 512, 7, 7]           1,024\n",
            "        Identity-612            [-1, 512, 7, 7]               0\n",
            "          Conv2d-613           [-1, 2048, 7, 7]       1,048,576\n",
            "        Identity-614           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-615           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-616           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-617           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-618           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-619           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-620           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-621            [-1, 128, 1, 1]         262,272\n",
            "        Identity-622            [-1, 128, 1, 1]               0\n",
            "            SiLU-623            [-1, 128, 1, 1]               0\n",
            "          Conv2d-624           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-625           [-1, 2048, 1, 1]               0\n",
            "        SEModule-626           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-627            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-628            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-629            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-630            [-1, 7, 7, 512]           1,024\n",
            "          Linear-631           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-632            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-633            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-634            [-1, 7, 7, 512]               0\n",
            "        Identity-635            [-1, 7, 7, 512]               0\n",
            "        Identity-636            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-637            [-1, 7, 7, 512]           1,024\n",
            "          Linear-638           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-639           [-1, 7, 7, 2048]               0\n",
            "         Dropout-640           [-1, 7, 7, 2048]               0\n",
            "        Identity-641           [-1, 7, 7, 2048]               0\n",
            "          Linear-642            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-643            [-1, 7, 7, 512]               0\n",
            "             Mlp-644            [-1, 7, 7, 512]               0\n",
            "        Identity-645            [-1, 7, 7, 512]               0\n",
            "        Identity-646            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-647            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-648            [-1, 7, 7, 512]           1,024\n",
            "          Linear-649           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-650            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-651            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-652            [-1, 7, 7, 512]               0\n",
            "        Identity-653            [-1, 7, 7, 512]               0\n",
            "        Identity-654            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-655            [-1, 7, 7, 512]           1,024\n",
            "          Linear-656           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-657           [-1, 7, 7, 2048]               0\n",
            "         Dropout-658           [-1, 7, 7, 2048]               0\n",
            "        Identity-659           [-1, 7, 7, 2048]               0\n",
            "          Linear-660            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-661            [-1, 7, 7, 512]               0\n",
            "             Mlp-662            [-1, 7, 7, 512]               0\n",
            "        Identity-663            [-1, 7, 7, 512]               0\n",
            "        Identity-664            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-665            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-666            [-1, 512, 7, 7]               0\n",
            "    MaxxVitStage-667            [-1, 512, 7, 7]               0\n",
            "        Identity-668            [-1, 512, 7, 7]               0\n",
            "================================================================\n",
            "Total params: 30,111,456\n",
            "Trainable params: 30,111,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 887.06\n",
            "Params size (MB): 114.87\n",
            "Estimated Total Size (MB): 1002.50\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a counter for the number of layers\n",
        "num_layers = 0\n",
        "\n",
        "# Iterate through the model's children and count the number of layers\n",
        "for child in first_three_layers_model.children():\n",
        "    num_layers += 1\n",
        "\n",
        "print(\"Number of layers in the model:\", num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK5l6RmwJ6yu",
        "outputId": "693806cb-c5eb-4f49-aa57-fb4c44dddb84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the model: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store parameters of the first three layers\n",
        "first_three_layers_params = []\n",
        "\n",
        "# Define a counter to track the number of layers\n",
        "num_layers = 0\n",
        "\n",
        "# Iterate through the named parameters of the model\n",
        "for name, param in model.named_parameters():\n",
        "    # Check if the parameter belongs to one of the first three layers\n",
        "    if num_layers < 3:\n",
        "        first_three_layers_params.append((name, param))\n",
        "\n",
        "    # Increment the layer counter\n",
        "    num_layers += 1\n",
        "\n",
        "# Print the parameters of the first three layers\n",
        "for name, param in first_three_layers_params:\n",
        "    print(name, param.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhU09bvZL-ax",
        "outputId": "329d9eb7-f3f8-42b5-fa70-6696923051cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem.conv1.weight torch.Size([64, 3, 3, 3])\n",
            "stem.conv1.bias torch.Size([64])\n",
            "stem.norm1.weight torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialSelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, height, width = x.size()\n",
        "        query = self.query_conv(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch_size, -1, height * width)\n",
        "        value = self.value_conv(x).view(batch_size, -1, height * width)\n",
        "        attention_scores = torch.matmul(query, key)\n",
        "        attention_scores = self.softmax(attention_scores)\n",
        "        out = torch.matmul(value, attention_scores.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, height, width)\n",
        "        out = x + out\n",
        "        return out\n",
        "\n",
        "\n",
        "class SpatialSelfAttentionProcessor(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialSelfAttentionProcessor, self).__init__()\n",
        "        self.ssa = SpatialSelfAttention(in_channels=in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        split_tensors = x.chunk(4, dim=2)\n",
        "        split_tensors = [t.chunk(4, dim=3) for t in split_tensors]\n",
        "        split_tensors = [item for sublist in split_tensors for item in sublist]\n",
        "\n",
        "        output_tensors = []\n",
        "        prev_output = None\n",
        "        for i, split_input in enumerate(split_tensors):\n",
        "            if i > 0:\n",
        "                split_input = split_input + prev_output\n",
        "            split_output = self.ssa(split_input)\n",
        "            output_tensors.append(split_output)\n",
        "            prev_output = split_output\n",
        "\n",
        "        output_tensor = torch.cat(output_tensors, dim=3)\n",
        "        output_tensor = torch.cat(output_tensor.chunk(4, dim=2), dim=1)\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Freeze layers up to the third layer to prevent updating their weights during training\n",
        "for param in first_three_layers_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create an instance of the Spatial Self-Attention Processor\n",
        "spatial_self_attention_processor = SpatialSelfAttentionProcessor(in_channels=512)\n",
        "\n",
        "# Combine the first three layers model with the Spatial Self-Attention Processor\n",
        "combined_model = nn.Sequential(\n",
        "    first_three_layers_model,  # Pre-trained layers up to the third layer\n",
        "    spatial_self_attention_processor  # Spatial Self-Attention Processor to be attached after the third layer\n",
        ")\n",
        "\n",
        "# Move the combined model to the appropriate device for computation, e.g., GPU\n",
        "combined_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ03zi-oZhUl",
        "outputId": "9176ba9b-7de3-4299-cce3-4ee6bc8ad7b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Stem(\n",
              "      (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "      (norm1): BatchNormAct2d(\n",
              "        64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "        (drop): Identity()\n",
              "        (act): GELUTanh()\n",
              "      )\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): MaxxVitStage(\n",
              "        (blocks): Sequential(\n",
              "          (0): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Downsample2d(\n",
              "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "                (expand): Identity()\n",
              "              )\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): MaxxVitStage(\n",
              "        (blocks): Sequential(\n",
              "          (0): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Downsample2d(\n",
              "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "                (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): MaxxVitStage(\n",
              "        (blocks): Sequential(\n",
              "          (0): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Downsample2d(\n",
              "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "                (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (2): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (3): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (4): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): MaxxVitStage(\n",
              "        (blocks): Sequential(\n",
              "          (0): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Downsample2d(\n",
              "                (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "                (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              )\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): MaxxVitBlock(\n",
              "            (conv): MbConvBlock(\n",
              "              (shortcut): Identity()\n",
              "              (pre_norm): BatchNormAct2d(\n",
              "                512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): Identity()\n",
              "              )\n",
              "              (down): Identity()\n",
              "              (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (norm1): BatchNormAct2d(\n",
              "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
              "              (norm2): BatchNormAct2d(\n",
              "                2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "                (drop): Identity()\n",
              "                (act): GELUTanh()\n",
              "              )\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): SiLU(inplace=True)\n",
              "                (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop_path): Identity()\n",
              "            )\n",
              "            (attn_block): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "            (attn_grid): PartitionAttentionCl(\n",
              "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn): AttentionCl(\n",
              "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "                (rel_pos): RelPosBiasTf()\n",
              "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls1): Identity()\n",
              "              (drop_path1): Identity()\n",
              "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): Mlp(\n",
              "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                (act): GELUTanh()\n",
              "                (drop1): Dropout(p=0.0, inplace=False)\n",
              "                (norm): Identity()\n",
              "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                (drop2): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (ls2): Identity()\n",
              "              (drop_path2): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Identity()\n",
              "  )\n",
              "  (1): SpatialSelfAttentionProcessor(\n",
              "    (ssa): SpatialSelfAttention(\n",
              "      (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (softmax): Softmax(dim=-1)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Print summary of the combined model\n",
        "summary(combined_model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNYhnMlvVxKV",
        "outputId": "be4695e2-da04-440b-f253-6a57a857f107"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "        Conv2dSame-1         [-1, 64, 112, 112]           1,792\n",
            "          Identity-2         [-1, 64, 112, 112]               0\n",
            "          GELUTanh-3         [-1, 64, 112, 112]               0\n",
            "    BatchNormAct2d-4         [-1, 64, 112, 112]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,928\n",
            "              Stem-6         [-1, 64, 112, 112]               0\n",
            "     AvgPool2dSame-7           [-1, 64, 56, 56]               0\n",
            "          Identity-8           [-1, 64, 56, 56]               0\n",
            "      Downsample2d-9           [-1, 64, 56, 56]               0\n",
            "         Identity-10         [-1, 64, 112, 112]               0\n",
            "         Identity-11         [-1, 64, 112, 112]               0\n",
            "   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n",
            "         Identity-13         [-1, 64, 112, 112]               0\n",
            "           Conv2d-14        [-1, 256, 112, 112]          16,384\n",
            "         Identity-15        [-1, 256, 112, 112]               0\n",
            "         GELUTanh-16        [-1, 256, 112, 112]               0\n",
            "   BatchNormAct2d-17        [-1, 256, 112, 112]             512\n",
            "       Conv2dSame-18          [-1, 256, 56, 56]           2,304\n",
            "         Identity-19          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-20          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22             [-1, 16, 1, 1]           4,112\n",
            "         Identity-23             [-1, 16, 1, 1]               0\n",
            "             SiLU-24             [-1, 16, 1, 1]               0\n",
            "           Conv2d-25            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-26            [-1, 256, 1, 1]               0\n",
            "         SEModule-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          16,448\n",
            "         Identity-29           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-30           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-31           [-1, 56, 56, 64]             128\n",
            "           Linear-32            [-1, 7, 7, 192]          12,480\n",
            "           Linear-33             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-34             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-35             [-1, 7, 7, 64]               0\n",
            "         Identity-36           [-1, 56, 56, 64]               0\n",
            "         Identity-37           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-38           [-1, 56, 56, 64]             128\n",
            "           Linear-39          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-40          [-1, 56, 56, 256]               0\n",
            "          Dropout-41          [-1, 56, 56, 256]               0\n",
            "         Identity-42          [-1, 56, 56, 256]               0\n",
            "           Linear-43           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-44           [-1, 56, 56, 64]               0\n",
            "              Mlp-45           [-1, 56, 56, 64]               0\n",
            "         Identity-46           [-1, 56, 56, 64]               0\n",
            "         Identity-47           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-48           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 64]             128\n",
            "           Linear-50            [-1, 7, 7, 192]          12,480\n",
            "           Linear-51             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-52             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-53             [-1, 7, 7, 64]               0\n",
            "         Identity-54           [-1, 56, 56, 64]               0\n",
            "         Identity-55           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
            "           Linear-57          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-58          [-1, 56, 56, 256]               0\n",
            "          Dropout-59          [-1, 56, 56, 256]               0\n",
            "         Identity-60          [-1, 56, 56, 256]               0\n",
            "           Linear-61           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-62           [-1, 56, 56, 64]               0\n",
            "              Mlp-63           [-1, 56, 56, 64]               0\n",
            "         Identity-64           [-1, 56, 56, 64]               0\n",
            "         Identity-65           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-66           [-1, 56, 56, 64]               0\n",
            "     MaxxVitBlock-67           [-1, 64, 56, 56]               0\n",
            "         Identity-68           [-1, 64, 56, 56]               0\n",
            "         Identity-69           [-1, 64, 56, 56]               0\n",
            "         Identity-70           [-1, 64, 56, 56]               0\n",
            "   BatchNormAct2d-71           [-1, 64, 56, 56]             128\n",
            "         Identity-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73          [-1, 256, 56, 56]          16,384\n",
            "         Identity-74          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-75          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-76          [-1, 256, 56, 56]             512\n",
            "           Conv2d-77          [-1, 256, 56, 56]           2,304\n",
            "         Identity-78          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-79          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-80          [-1, 256, 56, 56]             512\n",
            "           Conv2d-81             [-1, 16, 1, 1]           4,112\n",
            "         Identity-82             [-1, 16, 1, 1]               0\n",
            "             SiLU-83             [-1, 16, 1, 1]               0\n",
            "           Conv2d-84            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-85            [-1, 256, 1, 1]               0\n",
            "         SEModule-86          [-1, 256, 56, 56]               0\n",
            "           Conv2d-87           [-1, 64, 56, 56]          16,448\n",
            "         Identity-88           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-89           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-90           [-1, 56, 56, 64]             128\n",
            "           Linear-91            [-1, 7, 7, 192]          12,480\n",
            "           Linear-92             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-93             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-94             [-1, 7, 7, 64]               0\n",
            "         Identity-95           [-1, 56, 56, 64]               0\n",
            "         Identity-96           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-97           [-1, 56, 56, 64]             128\n",
            "           Linear-98          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-99          [-1, 56, 56, 256]               0\n",
            "         Dropout-100          [-1, 56, 56, 256]               0\n",
            "        Identity-101          [-1, 56, 56, 256]               0\n",
            "          Linear-102           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-103           [-1, 56, 56, 64]               0\n",
            "             Mlp-104           [-1, 56, 56, 64]               0\n",
            "        Identity-105           [-1, 56, 56, 64]               0\n",
            "        Identity-106           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-107           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-108           [-1, 56, 56, 64]             128\n",
            "          Linear-109            [-1, 7, 7, 192]          12,480\n",
            "          Linear-110             [-1, 7, 7, 64]           4,160\n",
            "         Dropout-111             [-1, 7, 7, 64]               0\n",
            "     AttentionCl-112             [-1, 7, 7, 64]               0\n",
            "        Identity-113           [-1, 56, 56, 64]               0\n",
            "        Identity-114           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-115           [-1, 56, 56, 64]             128\n",
            "          Linear-116          [-1, 56, 56, 256]          16,640\n",
            "        GELUTanh-117          [-1, 56, 56, 256]               0\n",
            "         Dropout-118          [-1, 56, 56, 256]               0\n",
            "        Identity-119          [-1, 56, 56, 256]               0\n",
            "          Linear-120           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-121           [-1, 56, 56, 64]               0\n",
            "             Mlp-122           [-1, 56, 56, 64]               0\n",
            "        Identity-123           [-1, 56, 56, 64]               0\n",
            "        Identity-124           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-125           [-1, 56, 56, 64]               0\n",
            "    MaxxVitBlock-126           [-1, 64, 56, 56]               0\n",
            "    MaxxVitStage-127           [-1, 64, 56, 56]               0\n",
            "   AvgPool2dSame-128           [-1, 64, 28, 28]               0\n",
            "          Conv2d-129          [-1, 128, 28, 28]           8,320\n",
            "    Downsample2d-130          [-1, 128, 28, 28]               0\n",
            "        Identity-131           [-1, 64, 56, 56]               0\n",
            "        Identity-132           [-1, 64, 56, 56]               0\n",
            "  BatchNormAct2d-133           [-1, 64, 56, 56]             128\n",
            "        Identity-134           [-1, 64, 56, 56]               0\n",
            "          Conv2d-135          [-1, 512, 56, 56]          32,768\n",
            "        Identity-136          [-1, 512, 56, 56]               0\n",
            "        GELUTanh-137          [-1, 512, 56, 56]               0\n",
            "  BatchNormAct2d-138          [-1, 512, 56, 56]           1,024\n",
            "      Conv2dSame-139          [-1, 512, 28, 28]           4,608\n",
            "        Identity-140          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-141          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-142          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-143             [-1, 32, 1, 1]          16,416\n",
            "        Identity-144             [-1, 32, 1, 1]               0\n",
            "            SiLU-145             [-1, 32, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-147            [-1, 512, 1, 1]               0\n",
            "        SEModule-148          [-1, 512, 28, 28]               0\n",
            "          Conv2d-149          [-1, 128, 28, 28]          65,664\n",
            "        Identity-150          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-151          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-152          [-1, 28, 28, 128]             256\n",
            "          Linear-153            [-1, 7, 7, 384]          49,536\n",
            "          Linear-154            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-155            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-156            [-1, 7, 7, 128]               0\n",
            "        Identity-157          [-1, 28, 28, 128]               0\n",
            "        Identity-158          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-159          [-1, 28, 28, 128]             256\n",
            "          Linear-160          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-161          [-1, 28, 28, 512]               0\n",
            "         Dropout-162          [-1, 28, 28, 512]               0\n",
            "        Identity-163          [-1, 28, 28, 512]               0\n",
            "          Linear-164          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-165          [-1, 28, 28, 128]               0\n",
            "             Mlp-166          [-1, 28, 28, 128]               0\n",
            "        Identity-167          [-1, 28, 28, 128]               0\n",
            "        Identity-168          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-169          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-170          [-1, 28, 28, 128]             256\n",
            "          Linear-171            [-1, 7, 7, 384]          49,536\n",
            "          Linear-172            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-173            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-174            [-1, 7, 7, 128]               0\n",
            "        Identity-175          [-1, 28, 28, 128]               0\n",
            "        Identity-176          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-177          [-1, 28, 28, 128]             256\n",
            "          Linear-178          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-179          [-1, 28, 28, 512]               0\n",
            "         Dropout-180          [-1, 28, 28, 512]               0\n",
            "        Identity-181          [-1, 28, 28, 512]               0\n",
            "          Linear-182          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-183          [-1, 28, 28, 128]               0\n",
            "             Mlp-184          [-1, 28, 28, 128]               0\n",
            "        Identity-185          [-1, 28, 28, 128]               0\n",
            "        Identity-186          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-187          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-188          [-1, 128, 28, 28]               0\n",
            "        Identity-189          [-1, 128, 28, 28]               0\n",
            "        Identity-190          [-1, 128, 28, 28]               0\n",
            "        Identity-191          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-192          [-1, 128, 28, 28]             256\n",
            "        Identity-193          [-1, 128, 28, 28]               0\n",
            "          Conv2d-194          [-1, 512, 28, 28]          65,536\n",
            "        Identity-195          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-196          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-197          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-198          [-1, 512, 28, 28]           4,608\n",
            "        Identity-199          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-200          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-201          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-202             [-1, 32, 1, 1]          16,416\n",
            "        Identity-203             [-1, 32, 1, 1]               0\n",
            "            SiLU-204             [-1, 32, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-206            [-1, 512, 1, 1]               0\n",
            "        SEModule-207          [-1, 512, 28, 28]               0\n",
            "          Conv2d-208          [-1, 128, 28, 28]          65,664\n",
            "        Identity-209          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-210          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-211          [-1, 28, 28, 128]             256\n",
            "          Linear-212            [-1, 7, 7, 384]          49,536\n",
            "          Linear-213            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-214            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-215            [-1, 7, 7, 128]               0\n",
            "        Identity-216          [-1, 28, 28, 128]               0\n",
            "        Identity-217          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-218          [-1, 28, 28, 128]             256\n",
            "          Linear-219          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-220          [-1, 28, 28, 512]               0\n",
            "         Dropout-221          [-1, 28, 28, 512]               0\n",
            "        Identity-222          [-1, 28, 28, 512]               0\n",
            "          Linear-223          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-224          [-1, 28, 28, 128]               0\n",
            "             Mlp-225          [-1, 28, 28, 128]               0\n",
            "        Identity-226          [-1, 28, 28, 128]               0\n",
            "        Identity-227          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-228          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-229          [-1, 28, 28, 128]             256\n",
            "          Linear-230            [-1, 7, 7, 384]          49,536\n",
            "          Linear-231            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-232            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-233            [-1, 7, 7, 128]               0\n",
            "        Identity-234          [-1, 28, 28, 128]               0\n",
            "        Identity-235          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-236          [-1, 28, 28, 128]             256\n",
            "          Linear-237          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-238          [-1, 28, 28, 512]               0\n",
            "         Dropout-239          [-1, 28, 28, 512]               0\n",
            "        Identity-240          [-1, 28, 28, 512]               0\n",
            "          Linear-241          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-242          [-1, 28, 28, 128]               0\n",
            "             Mlp-243          [-1, 28, 28, 128]               0\n",
            "        Identity-244          [-1, 28, 28, 128]               0\n",
            "        Identity-245          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-246          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-247          [-1, 128, 28, 28]               0\n",
            "    MaxxVitStage-248          [-1, 128, 28, 28]               0\n",
            "   AvgPool2dSame-249          [-1, 128, 14, 14]               0\n",
            "          Conv2d-250          [-1, 256, 14, 14]          33,024\n",
            "    Downsample2d-251          [-1, 256, 14, 14]               0\n",
            "        Identity-252          [-1, 128, 28, 28]               0\n",
            "        Identity-253          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-254          [-1, 128, 28, 28]             256\n",
            "        Identity-255          [-1, 128, 28, 28]               0\n",
            "          Conv2d-256         [-1, 1024, 28, 28]         131,072\n",
            "        Identity-257         [-1, 1024, 28, 28]               0\n",
            "        GELUTanh-258         [-1, 1024, 28, 28]               0\n",
            "  BatchNormAct2d-259         [-1, 1024, 28, 28]           2,048\n",
            "      Conv2dSame-260         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-261         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-262         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-263         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-264             [-1, 64, 1, 1]          65,600\n",
            "        Identity-265             [-1, 64, 1, 1]               0\n",
            "            SiLU-266             [-1, 64, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-268           [-1, 1024, 1, 1]               0\n",
            "        SEModule-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "        Identity-271          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-272          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-273          [-1, 14, 14, 256]             512\n",
            "          Linear-274            [-1, 7, 7, 768]         197,376\n",
            "          Linear-275            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-276            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-277            [-1, 7, 7, 256]               0\n",
            "        Identity-278          [-1, 14, 14, 256]               0\n",
            "        Identity-279          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-280          [-1, 14, 14, 256]             512\n",
            "          Linear-281         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-282         [-1, 14, 14, 1024]               0\n",
            "         Dropout-283         [-1, 14, 14, 1024]               0\n",
            "        Identity-284         [-1, 14, 14, 1024]               0\n",
            "          Linear-285          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-286          [-1, 14, 14, 256]               0\n",
            "             Mlp-287          [-1, 14, 14, 256]               0\n",
            "        Identity-288          [-1, 14, 14, 256]               0\n",
            "        Identity-289          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-290          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-291          [-1, 14, 14, 256]             512\n",
            "          Linear-292            [-1, 7, 7, 768]         197,376\n",
            "          Linear-293            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-294            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-295            [-1, 7, 7, 256]               0\n",
            "        Identity-296          [-1, 14, 14, 256]               0\n",
            "        Identity-297          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-298          [-1, 14, 14, 256]             512\n",
            "          Linear-299         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-300         [-1, 14, 14, 1024]               0\n",
            "         Dropout-301         [-1, 14, 14, 1024]               0\n",
            "        Identity-302         [-1, 14, 14, 1024]               0\n",
            "          Linear-303          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-304          [-1, 14, 14, 256]               0\n",
            "             Mlp-305          [-1, 14, 14, 256]               0\n",
            "        Identity-306          [-1, 14, 14, 256]               0\n",
            "        Identity-307          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-308          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-309          [-1, 256, 14, 14]               0\n",
            "        Identity-310          [-1, 256, 14, 14]               0\n",
            "        Identity-311          [-1, 256, 14, 14]               0\n",
            "        Identity-312          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-313          [-1, 256, 14, 14]             512\n",
            "        Identity-314          [-1, 256, 14, 14]               0\n",
            "          Conv2d-315         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-316         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-317         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-318         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-319         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-320         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-321         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-322         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-323             [-1, 64, 1, 1]          65,600\n",
            "        Identity-324             [-1, 64, 1, 1]               0\n",
            "            SiLU-325             [-1, 64, 1, 1]               0\n",
            "          Conv2d-326           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-327           [-1, 1024, 1, 1]               0\n",
            "        SEModule-328         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-329          [-1, 256, 14, 14]         262,400\n",
            "        Identity-330          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-331          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-332          [-1, 14, 14, 256]             512\n",
            "          Linear-333            [-1, 7, 7, 768]         197,376\n",
            "          Linear-334            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-335            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-336            [-1, 7, 7, 256]               0\n",
            "        Identity-337          [-1, 14, 14, 256]               0\n",
            "        Identity-338          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-339          [-1, 14, 14, 256]             512\n",
            "          Linear-340         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-341         [-1, 14, 14, 1024]               0\n",
            "         Dropout-342         [-1, 14, 14, 1024]               0\n",
            "        Identity-343         [-1, 14, 14, 1024]               0\n",
            "          Linear-344          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-345          [-1, 14, 14, 256]               0\n",
            "             Mlp-346          [-1, 14, 14, 256]               0\n",
            "        Identity-347          [-1, 14, 14, 256]               0\n",
            "        Identity-348          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-349          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-350          [-1, 14, 14, 256]             512\n",
            "          Linear-351            [-1, 7, 7, 768]         197,376\n",
            "          Linear-352            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-353            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-354            [-1, 7, 7, 256]               0\n",
            "        Identity-355          [-1, 14, 14, 256]               0\n",
            "        Identity-356          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 256]             512\n",
            "          Linear-358         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-359         [-1, 14, 14, 1024]               0\n",
            "         Dropout-360         [-1, 14, 14, 1024]               0\n",
            "        Identity-361         [-1, 14, 14, 1024]               0\n",
            "          Linear-362          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-363          [-1, 14, 14, 256]               0\n",
            "             Mlp-364          [-1, 14, 14, 256]               0\n",
            "        Identity-365          [-1, 14, 14, 256]               0\n",
            "        Identity-366          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-367          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-368          [-1, 256, 14, 14]               0\n",
            "        Identity-369          [-1, 256, 14, 14]               0\n",
            "        Identity-370          [-1, 256, 14, 14]               0\n",
            "        Identity-371          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-372          [-1, 256, 14, 14]             512\n",
            "        Identity-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-375         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-376         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-377         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-378         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-379         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-380         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-381         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-382             [-1, 64, 1, 1]          65,600\n",
            "        Identity-383             [-1, 64, 1, 1]               0\n",
            "            SiLU-384             [-1, 64, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-386           [-1, 1024, 1, 1]               0\n",
            "        SEModule-387         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-388          [-1, 256, 14, 14]         262,400\n",
            "        Identity-389          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-390          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-391          [-1, 14, 14, 256]             512\n",
            "          Linear-392            [-1, 7, 7, 768]         197,376\n",
            "          Linear-393            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-394            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-395            [-1, 7, 7, 256]               0\n",
            "        Identity-396          [-1, 14, 14, 256]               0\n",
            "        Identity-397          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-398          [-1, 14, 14, 256]             512\n",
            "          Linear-399         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-400         [-1, 14, 14, 1024]               0\n",
            "         Dropout-401         [-1, 14, 14, 1024]               0\n",
            "        Identity-402         [-1, 14, 14, 1024]               0\n",
            "          Linear-403          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-404          [-1, 14, 14, 256]               0\n",
            "             Mlp-405          [-1, 14, 14, 256]               0\n",
            "        Identity-406          [-1, 14, 14, 256]               0\n",
            "        Identity-407          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-408          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-409          [-1, 14, 14, 256]             512\n",
            "          Linear-410            [-1, 7, 7, 768]         197,376\n",
            "          Linear-411            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-412            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-413            [-1, 7, 7, 256]               0\n",
            "        Identity-414          [-1, 14, 14, 256]               0\n",
            "        Identity-415          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-416          [-1, 14, 14, 256]             512\n",
            "          Linear-417         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-418         [-1, 14, 14, 1024]               0\n",
            "         Dropout-419         [-1, 14, 14, 1024]               0\n",
            "        Identity-420         [-1, 14, 14, 1024]               0\n",
            "          Linear-421          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-422          [-1, 14, 14, 256]               0\n",
            "             Mlp-423          [-1, 14, 14, 256]               0\n",
            "        Identity-424          [-1, 14, 14, 256]               0\n",
            "        Identity-425          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-426          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-427          [-1, 256, 14, 14]               0\n",
            "        Identity-428          [-1, 256, 14, 14]               0\n",
            "        Identity-429          [-1, 256, 14, 14]               0\n",
            "        Identity-430          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-431          [-1, 256, 14, 14]             512\n",
            "        Identity-432          [-1, 256, 14, 14]               0\n",
            "          Conv2d-433         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-434         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-435         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-436         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-437         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-438         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-439         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-440         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-441             [-1, 64, 1, 1]          65,600\n",
            "        Identity-442             [-1, 64, 1, 1]               0\n",
            "            SiLU-443             [-1, 64, 1, 1]               0\n",
            "          Conv2d-444           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-445           [-1, 1024, 1, 1]               0\n",
            "        SEModule-446         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-447          [-1, 256, 14, 14]         262,400\n",
            "        Identity-448          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-449          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-450          [-1, 14, 14, 256]             512\n",
            "          Linear-451            [-1, 7, 7, 768]         197,376\n",
            "          Linear-452            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-453            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-454            [-1, 7, 7, 256]               0\n",
            "        Identity-455          [-1, 14, 14, 256]               0\n",
            "        Identity-456          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-457          [-1, 14, 14, 256]             512\n",
            "          Linear-458         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-459         [-1, 14, 14, 1024]               0\n",
            "         Dropout-460         [-1, 14, 14, 1024]               0\n",
            "        Identity-461         [-1, 14, 14, 1024]               0\n",
            "          Linear-462          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-463          [-1, 14, 14, 256]               0\n",
            "             Mlp-464          [-1, 14, 14, 256]               0\n",
            "        Identity-465          [-1, 14, 14, 256]               0\n",
            "        Identity-466          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-467          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-468          [-1, 14, 14, 256]             512\n",
            "          Linear-469            [-1, 7, 7, 768]         197,376\n",
            "          Linear-470            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-471            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-472            [-1, 7, 7, 256]               0\n",
            "        Identity-473          [-1, 14, 14, 256]               0\n",
            "        Identity-474          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-475          [-1, 14, 14, 256]             512\n",
            "          Linear-476         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-477         [-1, 14, 14, 1024]               0\n",
            "         Dropout-478         [-1, 14, 14, 1024]               0\n",
            "        Identity-479         [-1, 14, 14, 1024]               0\n",
            "          Linear-480          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-481          [-1, 14, 14, 256]               0\n",
            "             Mlp-482          [-1, 14, 14, 256]               0\n",
            "        Identity-483          [-1, 14, 14, 256]               0\n",
            "        Identity-484          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-485          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-486          [-1, 256, 14, 14]               0\n",
            "        Identity-487          [-1, 256, 14, 14]               0\n",
            "        Identity-488          [-1, 256, 14, 14]               0\n",
            "        Identity-489          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-490          [-1, 256, 14, 14]             512\n",
            "        Identity-491          [-1, 256, 14, 14]               0\n",
            "          Conv2d-492         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-493         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-494         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-495         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-496         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-497         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-498         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-499         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-500             [-1, 64, 1, 1]          65,600\n",
            "        Identity-501             [-1, 64, 1, 1]               0\n",
            "            SiLU-502             [-1, 64, 1, 1]               0\n",
            "          Conv2d-503           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-504           [-1, 1024, 1, 1]               0\n",
            "        SEModule-505         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-506          [-1, 256, 14, 14]         262,400\n",
            "        Identity-507          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-508          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 256]             512\n",
            "          Linear-510            [-1, 7, 7, 768]         197,376\n",
            "          Linear-511            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-512            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-513            [-1, 7, 7, 256]               0\n",
            "        Identity-514          [-1, 14, 14, 256]               0\n",
            "        Identity-515          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-516          [-1, 14, 14, 256]             512\n",
            "          Linear-517         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-518         [-1, 14, 14, 1024]               0\n",
            "         Dropout-519         [-1, 14, 14, 1024]               0\n",
            "        Identity-520         [-1, 14, 14, 1024]               0\n",
            "          Linear-521          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-522          [-1, 14, 14, 256]               0\n",
            "             Mlp-523          [-1, 14, 14, 256]               0\n",
            "        Identity-524          [-1, 14, 14, 256]               0\n",
            "        Identity-525          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-526          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-527          [-1, 14, 14, 256]             512\n",
            "          Linear-528            [-1, 7, 7, 768]         197,376\n",
            "          Linear-529            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-530            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-531            [-1, 7, 7, 256]               0\n",
            "        Identity-532          [-1, 14, 14, 256]               0\n",
            "        Identity-533          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-534          [-1, 14, 14, 256]             512\n",
            "          Linear-535         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-536         [-1, 14, 14, 1024]               0\n",
            "         Dropout-537         [-1, 14, 14, 1024]               0\n",
            "        Identity-538         [-1, 14, 14, 1024]               0\n",
            "          Linear-539          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-540          [-1, 14, 14, 256]               0\n",
            "             Mlp-541          [-1, 14, 14, 256]               0\n",
            "        Identity-542          [-1, 14, 14, 256]               0\n",
            "        Identity-543          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-544          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-545          [-1, 256, 14, 14]               0\n",
            "    MaxxVitStage-546          [-1, 256, 14, 14]               0\n",
            "   AvgPool2dSame-547            [-1, 256, 7, 7]               0\n",
            "          Conv2d-548            [-1, 512, 7, 7]         131,584\n",
            "    Downsample2d-549            [-1, 512, 7, 7]               0\n",
            "        Identity-550          [-1, 256, 14, 14]               0\n",
            "        Identity-551          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-552          [-1, 256, 14, 14]             512\n",
            "        Identity-553          [-1, 256, 14, 14]               0\n",
            "          Conv2d-554         [-1, 2048, 14, 14]         524,288\n",
            "        Identity-555         [-1, 2048, 14, 14]               0\n",
            "        GELUTanh-556         [-1, 2048, 14, 14]               0\n",
            "  BatchNormAct2d-557         [-1, 2048, 14, 14]           4,096\n",
            "      Conv2dSame-558           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-559           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-560           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-561           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-562            [-1, 128, 1, 1]         262,272\n",
            "        Identity-563            [-1, 128, 1, 1]               0\n",
            "            SiLU-564            [-1, 128, 1, 1]               0\n",
            "          Conv2d-565           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-566           [-1, 2048, 1, 1]               0\n",
            "        SEModule-567           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-568            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-569            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-570            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-571            [-1, 7, 7, 512]           1,024\n",
            "          Linear-572           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-573            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-574            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-575            [-1, 7, 7, 512]               0\n",
            "        Identity-576            [-1, 7, 7, 512]               0\n",
            "        Identity-577            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-578            [-1, 7, 7, 512]           1,024\n",
            "          Linear-579           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-580           [-1, 7, 7, 2048]               0\n",
            "         Dropout-581           [-1, 7, 7, 2048]               0\n",
            "        Identity-582           [-1, 7, 7, 2048]               0\n",
            "          Linear-583            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-584            [-1, 7, 7, 512]               0\n",
            "             Mlp-585            [-1, 7, 7, 512]               0\n",
            "        Identity-586            [-1, 7, 7, 512]               0\n",
            "        Identity-587            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-588            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-589            [-1, 7, 7, 512]           1,024\n",
            "          Linear-590           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-591            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-592            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-593            [-1, 7, 7, 512]               0\n",
            "        Identity-594            [-1, 7, 7, 512]               0\n",
            "        Identity-595            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-596            [-1, 7, 7, 512]           1,024\n",
            "          Linear-597           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-598           [-1, 7, 7, 2048]               0\n",
            "         Dropout-599           [-1, 7, 7, 2048]               0\n",
            "        Identity-600           [-1, 7, 7, 2048]               0\n",
            "          Linear-601            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-602            [-1, 7, 7, 512]               0\n",
            "             Mlp-603            [-1, 7, 7, 512]               0\n",
            "        Identity-604            [-1, 7, 7, 512]               0\n",
            "        Identity-605            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-606            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-607            [-1, 512, 7, 7]               0\n",
            "        Identity-608            [-1, 512, 7, 7]               0\n",
            "        Identity-609            [-1, 512, 7, 7]               0\n",
            "        Identity-610            [-1, 512, 7, 7]               0\n",
            "  BatchNormAct2d-611            [-1, 512, 7, 7]           1,024\n",
            "        Identity-612            [-1, 512, 7, 7]               0\n",
            "          Conv2d-613           [-1, 2048, 7, 7]       1,048,576\n",
            "        Identity-614           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-615           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-616           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-617           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-618           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-619           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-620           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-621            [-1, 128, 1, 1]         262,272\n",
            "        Identity-622            [-1, 128, 1, 1]               0\n",
            "            SiLU-623            [-1, 128, 1, 1]               0\n",
            "          Conv2d-624           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-625           [-1, 2048, 1, 1]               0\n",
            "        SEModule-626           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-627            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-628            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-629            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-630            [-1, 7, 7, 512]           1,024\n",
            "          Linear-631           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-632            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-633            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-634            [-1, 7, 7, 512]               0\n",
            "        Identity-635            [-1, 7, 7, 512]               0\n",
            "        Identity-636            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-637            [-1, 7, 7, 512]           1,024\n",
            "          Linear-638           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-639           [-1, 7, 7, 2048]               0\n",
            "         Dropout-640           [-1, 7, 7, 2048]               0\n",
            "        Identity-641           [-1, 7, 7, 2048]               0\n",
            "          Linear-642            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-643            [-1, 7, 7, 512]               0\n",
            "             Mlp-644            [-1, 7, 7, 512]               0\n",
            "        Identity-645            [-1, 7, 7, 512]               0\n",
            "        Identity-646            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-647            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-648            [-1, 7, 7, 512]           1,024\n",
            "          Linear-649           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-650            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-651            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-652            [-1, 7, 7, 512]               0\n",
            "        Identity-653            [-1, 7, 7, 512]               0\n",
            "        Identity-654            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-655            [-1, 7, 7, 512]           1,024\n",
            "          Linear-656           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-657           [-1, 7, 7, 2048]               0\n",
            "         Dropout-658           [-1, 7, 7, 2048]               0\n",
            "        Identity-659           [-1, 7, 7, 2048]               0\n",
            "          Linear-660            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-661            [-1, 7, 7, 512]               0\n",
            "             Mlp-662            [-1, 7, 7, 512]               0\n",
            "        Identity-663            [-1, 7, 7, 512]               0\n",
            "        Identity-664            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-665            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-666            [-1, 512, 7, 7]               0\n",
            "    MaxxVitStage-667            [-1, 512, 7, 7]               0\n",
            "        Identity-668            [-1, 512, 7, 7]               0\n",
            "          Conv2d-669             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-670             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-671            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-672                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-673            [-1, 512, 2, 2]               0\n",
            "          Conv2d-674             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-675             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-676            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-677                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-678            [-1, 512, 2, 2]               0\n",
            "          Conv2d-679             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-680             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-681            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-682                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-683            [-1, 512, 2, 2]               0\n",
            "          Conv2d-684             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-685             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-686            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-687                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-688            [-1, 512, 2, 2]               0\n",
            "          Conv2d-689             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-690             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-691            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-692                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-693            [-1, 512, 2, 2]               0\n",
            "          Conv2d-694             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-695             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-696            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-697                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-698            [-1, 512, 2, 2]               0\n",
            "          Conv2d-699             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-700             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-701            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-702                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-703            [-1, 512, 2, 2]               0\n",
            "          Conv2d-704             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-705             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-706            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-707                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-708            [-1, 512, 2, 2]               0\n",
            "          Conv2d-709             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-710             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-711            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-712                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-713            [-1, 512, 2, 2]               0\n",
            "          Conv2d-714             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-715             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-716            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-717                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-718            [-1, 512, 2, 2]               0\n",
            "          Conv2d-719             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-720             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-721            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-722                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-723            [-1, 512, 2, 2]               0\n",
            "          Conv2d-724             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-725             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-726            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-727                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-728            [-1, 512, 2, 2]               0\n",
            "          Conv2d-729             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-730             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-731            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-732                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-733            [-1, 512, 2, 2]               0\n",
            "          Conv2d-734             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-735             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-736            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-737                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-738            [-1, 512, 2, 2]               0\n",
            "          Conv2d-739             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-740             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-741            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-742                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-743            [-1, 512, 2, 2]               0\n",
            "          Conv2d-744             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-745             [-1, 64, 2, 2]          32,832\n",
            "          Conv2d-746            [-1, 512, 2, 2]         262,656\n",
            "         Softmax-747                 [-1, 4, 4]               0\n",
            "SpatialSelfAttention-748            [-1, 512, 2, 2]               0\n",
            "SpatialSelfAttentionProcessor-749          [-1, 1024, 1, 32]               0\n",
            "================================================================\n",
            "Total params: 35,364,576\n",
            "Trainable params: 5,253,120\n",
            "Non-trainable params: 30,111,456\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 887.87\n",
            "Params size (MB): 134.91\n",
            "Estimated Total Size (MB): 1023.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqdNsSl_WJQX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJv44I7eWLVa"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}