{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def print_model_layers(model, prefix=''):\n",
        "    \"\"\"\n",
        "    Prints all layers and sub-modules of a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to inspect.\n",
        "    - prefix (str): A prefix for layer naming, used for nested models.\n",
        "    \"\"\"\n",
        "    for name, module in model.named_children():\n",
        "        if len(list(module.children())) > 0:  # If the module has children, recursively print its layers\n",
        "            print_model_layers(module, prefix + name + '.')\n",
        "        else:\n",
        "            print(f'{prefix + name}: {module}')\n",
        "\n",
        "def get_submodel(model, layer_name):\n",
        "    \"\"\"\n",
        "    Extracts a submodel from the original model up to a specified layer.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The original PyTorch model.\n",
        "    - layer_name (str): The name of the layer up to which the submodel should be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - torch.nn.Module: A submodel that ends at the specified layer.\n",
        "    \"\"\"\n",
        "    submodel = nn.Sequential()\n",
        "    for name, module in model.named_children():\n",
        "        submodel.add_module(name, module)\n",
        "        if name == layer_name:\n",
        "            break\n",
        "        # If the current module has children, recurse into it to find the layer\n",
        "        elif len(list(module.children())) > 0:\n",
        "            submodel[-1] = get_submodel(module, layer_name)\n",
        "            if submodel[-1] is not None:\n",
        "                break\n",
        "    return submodel\n",
        "\n",
        "# Load the model\n",
        "model_name = \"hf_hub:timm/maxvit_tiny_tf_224.in1k\"\n",
        "model = timm.create_model(model_name, pretrained=False)\n",
        "\n",
        "# Print all layers and sub-modules of the model\n",
        "print(\"Printing all layers and sub-modules of the model:\")\n",
        "print_model_layers(model)\n",
        "\n",
        "# Input for layer name\n",
        "layer_name = input(\"Enter the layer name up to which you want the submodel: \")\n",
        "\n",
        "# Extract and print the submodel\n",
        "submodel = get_submodel(model, layer_name)\n",
        "if submodel is not None:\n",
        "    print(f\"\\nSubmodel up to layer '{layer_name}':\")\n",
        "    print(submodel)\n",
        "else:\n",
        "    print(f\"Layer '{layer_name}' not found in the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABQ1iTNEn1gh",
        "outputId": "44275072-9cf2-43ff-b2d1-aeb37b94d867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing all layers and sub-modules of the model:\n",
            "stem.conv1: Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "stem.norm1.drop: Identity()\n",
            "stem.norm1.act: GELUTanh()\n",
            "stem.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "stages.0.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.0.blocks.0.conv.shortcut.expand: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.0.conv.down: Identity()\n",
            "stages.0.blocks.0.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.conv2_kxk: Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
            "stages.0.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.bn: Identity()\n",
            "stages.0.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.0.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.0.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.drop_path: Identity()\n",
            "stages.0.blocks.0.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls1: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls2: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.0.blocks.1.conv.shortcut: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.1.conv.down: Identity()\n",
            "stages.0.blocks.1.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.conv2_kxk: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "stages.0.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.bn: Identity()\n",
            "stages.0.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.1.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.1.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.drop_path: Identity()\n",
            "stages.0.blocks.1.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls1: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls2: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.1.blocks.0.conv.shortcut.expand: Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.0.conv.down: Identity()\n",
            "stages.1.blocks.0.conv.conv1_1x1: Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.conv2_kxk: Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
            "stages.1.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.bn: Identity()\n",
            "stages.1.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.0.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.0.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.drop_path: Identity()\n",
            "stages.1.blocks.0.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls1: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls2: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.1.conv.shortcut: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.1.conv.down: Identity()\n",
            "stages.1.blocks.1.conv.conv1_1x1: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.conv2_kxk: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "stages.1.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.bn: Identity()\n",
            "stages.1.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.1.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.1.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.drop_path: Identity()\n",
            "stages.1.blocks.1.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls1: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls2: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.2.blocks.0.conv.shortcut.expand: Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.0.conv.down: Identity()\n",
            "stages.2.blocks.0.conv.conv1_1x1: Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.conv2_kxk: Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
            "stages.2.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.bn: Identity()\n",
            "stages.2.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.0.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.0.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.drop_path: Identity()\n",
            "stages.2.blocks.0.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls1: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls2: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.1.conv.shortcut: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.1.conv.down: Identity()\n",
            "stages.2.blocks.1.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.bn: Identity()\n",
            "stages.2.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.1.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.1.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.drop_path: Identity()\n",
            "stages.2.blocks.1.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls1: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls2: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.2.conv.shortcut: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.2.conv.down: Identity()\n",
            "stages.2.blocks.2.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.2.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.2.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.bn: Identity()\n",
            "stages.2.blocks.2.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.2.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.2.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.drop_path: Identity()\n",
            "stages.2.blocks.2.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls1: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls2: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.3.conv.shortcut: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.3.conv.down: Identity()\n",
            "stages.2.blocks.3.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.3.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.3.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.bn: Identity()\n",
            "stages.2.blocks.3.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.3.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.3.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.drop_path: Identity()\n",
            "stages.2.blocks.3.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls1: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls2: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.4.conv.shortcut: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.4.conv.down: Identity()\n",
            "stages.2.blocks.4.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.4.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.4.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.bn: Identity()\n",
            "stages.2.blocks.4.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.4.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.4.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.drop_path: Identity()\n",
            "stages.2.blocks.4.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls1: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls2: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.3.blocks.0.conv.shortcut.expand: Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.0.conv.down: Identity()\n",
            "stages.3.blocks.0.conv.conv1_1x1: Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.conv2_kxk: Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
            "stages.3.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.bn: Identity()\n",
            "stages.3.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.0.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.0.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.drop_path: Identity()\n",
            "stages.3.blocks.0.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls1: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls2: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.1.conv.shortcut: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.1.conv.down: Identity()\n",
            "stages.3.blocks.1.conv.conv1_1x1: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.conv2_kxk: Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
            "stages.3.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.bn: Identity()\n",
            "stages.3.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.1.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.1.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.drop_path: Identity()\n",
            "stages.3.blocks.1.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls1: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls2: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path2: Identity()\n",
            "norm: Identity()\n",
            "head.global_pool.pool: AdaptiveAvgPool2d(output_size=1)\n",
            "head.global_pool.flatten: Identity()\n",
            "head.norm: LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
            "head.flatten: Flatten(start_dim=1, end_dim=-1)\n",
            "head.pre_logits.fc: Linear(in_features=512, out_features=512, bias=True)\n",
            "head.pre_logits.act: Tanh()\n",
            "head.drop: Dropout(p=0.0, inplace=False)\n",
            "head.fc: Linear(in_features=512, out_features=1000, bias=True)\n",
            "Enter the layer name up to which you want the submodel: stages.2.blocks.4.attn_grid.drop_path2\n",
            "\n",
            "Submodel up to layer 'stages.2.blocks.4.attn_grid.drop_path2':\n",
            "Sequential(\n",
            "  (stem): Sequential(\n",
            "    (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (norm1): Sequential(\n",
            "      (drop): Identity()\n",
            "      (act): GELUTanh()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import timm\n",
        "from torchsummary import summary\n",
        "\n",
        "# Install timm if not already installed\n",
        "try:\n",
        "    import timm\n",
        "except ImportError:\n",
        "    !pip install timm\n",
        "\n",
        "# Function to get the submodel up to a specified layer\n",
        "def get_submodel(model, layer_name):\n",
        "    submodel_layers = []\n",
        "    found = False\n",
        "    for name, module in model.named_children():\n",
        "        if name == layer_name:\n",
        "            found = True\n",
        "            break\n",
        "        submodel_layers.append(module)\n",
        "    if found:\n",
        "        return nn.Sequential(*submodel_layers)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load the model\n",
        "model = timm.create_model(\"hf_hub:timm/maxvit_tiny_tf_224.in1k\", pretrained=False)\n",
        "\n",
        "# Input for layer name\n",
        "layer_name = input(\"Enter the layer name up to which you want the submodel: \")\n",
        "\n",
        "# Extract and print the submodel\n",
        "submodel = get_submodel(model, layer_name)\n",
        "if submodel is not None:\n",
        "    print(f\"\\nSubmodel up to layer '{layer_name}':\")\n",
        "    print(submodel)\n",
        "\n",
        "    # Print summary of the submodel\n",
        "    summary(submodel, input_size=(3, 224, 224))\n",
        "\n",
        "    # Print output shape of the submodel\n",
        "    input_tensor = torch.rand(1, 3, 224, 224)  # Assuming batch size of 1\n",
        "    output_tensor = submodel(input_tensor)\n",
        "    print(f\"\\nOutput shape of the submodel: {output_tensor.shape}\")\n",
        "else:\n",
        "    print(f\"Layer '{layer_name}' not found in the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8w3D5RMY1NQ",
        "outputId": "1b264254-eafa-420b-b630-42337ccee28d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the layer name up to which you want the submodel: stages.2.blocks.4.attn_grid.drop_path2\n",
            "Layer 'stages.2.blocks.4.attn_grid.drop_path2' not found in the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def print_model_layers(model, prefix=''):\n",
        "\n",
        "    for name, module in model.named_children():\n",
        "        if len(list(module.children())) > 0:  # If the module has children, recursively print its layers\n",
        "            print_model_layers(module, prefix + name + '.')\n",
        "        else:\n",
        "            print(f'{prefix + name}: {module}')\n",
        "\n",
        "def get_submodel(model, layer_name):\n",
        "    \"\"\"\n",
        "    Extracts a submodel from the original model up to a specified layer.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The original PyTorch model.\n",
        "    - layer_name (str): The name of the layer up to which the submodel should be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - torch.nn.Module: A submodel that ends at the specified layer.\n",
        "    \"\"\"\n",
        "    submodel = nn.Sequential()\n",
        "    for name, module in model.named_children():\n",
        "        submodel.add_module(name, module)\n",
        "        if name == layer_name:\n",
        "            break\n",
        "        # If the current module has children, recurse into it to find the layer\n",
        "        elif len(list(module.children())) > 0:\n",
        "            submodel[-1] = get_submodel(module, layer_name)\n",
        "            if submodel[-1] is not None:\n",
        "                break\n",
        "    return submodel\n",
        "\n",
        "# Load the model\n",
        "model_name = \"hf_hub:timm/maxvit_tiny_tf_224.in1k\"\n",
        "model = timm.create_model(model_name, pretrained=False)\n",
        "\n",
        "# Print all layers and sub-modules of the model\n",
        "print(\"Printing all layers and sub-modules of the model:\")\n",
        "print_model_layers(model)\n",
        "\n",
        "# Input for layer name\n",
        "layer_name = input(\"Enter the layer name up to which you want the submodel: \")\n",
        "\n",
        "# Extract and print the submodel\n",
        "submodel = get_submodel(model, layer_name)\n",
        "if submodel is not None:\n",
        "    print(f\"\\nSubmodel up to layer '{layer_name}':\")\n",
        "    print(submodel)\n",
        "else:\n",
        "    print(f\"Layer '{layer_name}' not found in the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtQwToX6YLtr",
        "outputId": "246a935b-9d76-43a1-9339-c016eba09994"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing all layers and sub-modules of the model:\n",
            "stem.conv1: Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "stem.norm1.drop: Identity()\n",
            "stem.norm1.act: GELUTanh()\n",
            "stem.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "stages.0.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.0.blocks.0.conv.shortcut.expand: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.0.conv.down: Identity()\n",
            "stages.0.blocks.0.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.conv2_kxk: Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
            "stages.0.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.bn: Identity()\n",
            "stages.0.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.0.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.0.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.drop_path: Identity()\n",
            "stages.0.blocks.0.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls1: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls2: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.0.blocks.1.conv.shortcut: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.1.conv.down: Identity()\n",
            "stages.0.blocks.1.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.conv2_kxk: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "stages.0.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.bn: Identity()\n",
            "stages.0.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.1.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.1.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.drop_path: Identity()\n",
            "stages.0.blocks.1.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls1: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls2: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.1.blocks.0.conv.shortcut.expand: Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.0.conv.down: Identity()\n",
            "stages.1.blocks.0.conv.conv1_1x1: Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.conv2_kxk: Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
            "stages.1.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.bn: Identity()\n",
            "stages.1.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.0.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.0.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.drop_path: Identity()\n",
            "stages.1.blocks.0.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls1: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls2: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.1.conv.shortcut: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.1.conv.down: Identity()\n",
            "stages.1.blocks.1.conv.conv1_1x1: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.conv2_kxk: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "stages.1.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.bn: Identity()\n",
            "stages.1.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.1.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.1.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.drop_path: Identity()\n",
            "stages.1.blocks.1.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls1: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls2: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.2.blocks.0.conv.shortcut.expand: Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.0.conv.down: Identity()\n",
            "stages.2.blocks.0.conv.conv1_1x1: Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.conv2_kxk: Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
            "stages.2.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.bn: Identity()\n",
            "stages.2.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.0.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.0.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.drop_path: Identity()\n",
            "stages.2.blocks.0.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls1: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls2: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.1.conv.shortcut: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.1.conv.down: Identity()\n",
            "stages.2.blocks.1.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.bn: Identity()\n",
            "stages.2.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.1.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.1.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.drop_path: Identity()\n",
            "stages.2.blocks.1.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls1: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls2: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.2.conv.shortcut: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.2.conv.down: Identity()\n",
            "stages.2.blocks.2.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.2.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.2.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.bn: Identity()\n",
            "stages.2.blocks.2.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.2.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.2.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.drop_path: Identity()\n",
            "stages.2.blocks.2.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls1: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls2: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.3.conv.shortcut: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.3.conv.down: Identity()\n",
            "stages.2.blocks.3.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.3.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.3.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.bn: Identity()\n",
            "stages.2.blocks.3.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.3.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.3.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.drop_path: Identity()\n",
            "stages.2.blocks.3.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls1: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls2: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.4.conv.shortcut: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.4.conv.down: Identity()\n",
            "stages.2.blocks.4.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.4.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.4.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.bn: Identity()\n",
            "stages.2.blocks.4.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.4.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.4.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.drop_path: Identity()\n",
            "stages.2.blocks.4.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls1: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls2: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.3.blocks.0.conv.shortcut.expand: Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.0.conv.down: Identity()\n",
            "stages.3.blocks.0.conv.conv1_1x1: Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.conv2_kxk: Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
            "stages.3.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.bn: Identity()\n",
            "stages.3.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.0.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.0.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.drop_path: Identity()\n",
            "stages.3.blocks.0.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls1: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls2: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.1.conv.shortcut: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.1.conv.down: Identity()\n",
            "stages.3.blocks.1.conv.conv1_1x1: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.conv2_kxk: Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
            "stages.3.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.bn: Identity()\n",
            "stages.3.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.1.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.1.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.drop_path: Identity()\n",
            "stages.3.blocks.1.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls1: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls2: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path2: Identity()\n",
            "norm: Identity()\n",
            "head.global_pool.pool: AdaptiveAvgPool2d(output_size=1)\n",
            "head.global_pool.flatten: Identity()\n",
            "head.norm: LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
            "head.flatten: Flatten(start_dim=1, end_dim=-1)\n",
            "head.pre_logits.fc: Linear(in_features=512, out_features=512, bias=True)\n",
            "head.pre_logits.act: Tanh()\n",
            "head.drop: Dropout(p=0.0, inplace=False)\n",
            "head.fc: Linear(in_features=512, out_features=1000, bias=True)\n",
            "Enter the layer name up to which you want the submodel: stages.2.blocks.4.attn_grid.drop_path2\n",
            "\n",
            "Submodel up to layer 'stages.2.blocks.4.attn_grid.drop_path2':\n",
            "Sequential(\n",
            "  (stem): Sequential(\n",
            "    (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (norm1): Sequential(\n",
            "      (drop): Identity()\n",
            "      (act): GELUTanh()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "def print_model_layers(model, prefix=''):\n",
        "    \"\"\"\n",
        "    Prints all layers and sub-modules of a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to inspect.\n",
        "    - prefix (str): A prefix for layer naming, used for nested models.\n",
        "    \"\"\"\n",
        "    for name, module in model.named_children():\n",
        "        if len(list(module.children())) > 0:  # If the module has children, recursively print its layers\n",
        "            print_model_layers(module, prefix + name + '.')\n",
        "        else:\n",
        "            print(f'{prefix + name}: {module}')\n",
        "\n",
        "def get_submodel(model, layer_name):\n",
        "    \"\"\"\n",
        "    Extracts a submodel from the original model up to a specified layer.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The original PyTorch model.\n",
        "    - layer_name (str): The name of the layer up to which the submodel should be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - torch.nn.Module: A submodel that starts from the input of the model and ends at the specified layer.\n",
        "    \"\"\"\n",
        "    submodel = nn.Sequential()\n",
        "    found_layer = False\n",
        "    for name, module in model.named_children():\n",
        "        if found_layer:\n",
        "            break\n",
        "        submodel.add_module(name, module)\n",
        "        if name == layer_name:\n",
        "            found_layer = True\n",
        "        elif len(list(module.children())) > 0:\n",
        "            sub_submodel = get_submodel(module, layer_name)\n",
        "            if sub_submodel is not None:\n",
        "                submodel.add_module(name, sub_submodel)\n",
        "                found_layer = True\n",
        "    if found_layer:\n",
        "        return submodel\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load the model\n",
        "model_name = \"hf_hub:timm/maxvit_tiny_tf_224.in1k\"\n",
        "model = timm.create_model(model_name, pretrained=False)\n",
        "\n",
        "# Print all layers and sub-modules of the model\n",
        "print(\"Printing all layers and sub-modules of the model:\")\n",
        "print_model_layers(model)\n",
        "\n",
        "# Input for layer name\n",
        "layer_name = input(\"Enter the layer name up to which you want the submodel: \")\n",
        "\n",
        "# Extract the submodel\n",
        "submodel = get_submodel(model, layer_name)\n",
        "if submodel is not None:\n",
        "    print(f\"\\nSubmodel up to layer '{layer_name}':\")\n",
        "    print(submodel)\n",
        "\n",
        "    # Print summary of the submodel\n",
        "    summary(submodel, input_size=(3, 224, 224))\n",
        "else:\n",
        "    print(f\"Layer '{layer_name}' not found in the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqBK7Qm5aVeH",
        "outputId": "b52549a8-d60c-40d0-cb96-2e361c1b008e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing all layers and sub-modules of the model:\n",
            "stem.conv1: Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "stem.norm1.drop: Identity()\n",
            "stem.norm1.act: GELUTanh()\n",
            "stem.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "stages.0.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.0.blocks.0.conv.shortcut.expand: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.0.conv.down: Identity()\n",
            "stages.0.blocks.0.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.conv2_kxk: Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
            "stages.0.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.0.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.bn: Identity()\n",
            "stages.0.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.0.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.0.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.0.conv.drop_path: Identity()\n",
            "stages.0.blocks.0.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls1: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_block.ls2: Identity()\n",
            "stages.0.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.0.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.0.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.0.blocks.1.conv.shortcut: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.0.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.0.blocks.1.conv.down: Identity()\n",
            "stages.0.blocks.1.conv.conv1_1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.0.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.conv2_kxk: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "stages.0.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.0.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.0.blocks.1.conv.se.fc1: Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.bn: Identity()\n",
            "stages.0.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.0.blocks.1.conv.se.fc2: Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.0.blocks.1.conv.conv3_1x1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.0.blocks.1.conv.drop_path: Identity()\n",
            "stages.0.blocks.1.attn_block.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls1: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_block.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_block.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_block.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_block.ls2: Identity()\n",
            "stages.0.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm1: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.attn.qkv: Linear(in_features=64, out_features=192, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.0.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.attn.proj: Linear(in_features=64, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.0.blocks.1.attn_grid.norm2: LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.fc1: Linear(in_features=64, out_features=256, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.0.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.0.blocks.1.attn_grid.mlp.fc2: Linear(in_features=256, out_features=64, bias=True)\n",
            "stages.0.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.0.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.0.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.1.blocks.0.conv.shortcut.expand: Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.0.conv.down: Identity()\n",
            "stages.1.blocks.0.conv.conv1_1x1: Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.conv2_kxk: Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
            "stages.1.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.0.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.bn: Identity()\n",
            "stages.1.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.0.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.0.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.0.conv.drop_path: Identity()\n",
            "stages.1.blocks.0.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls1: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_block.ls2: Identity()\n",
            "stages.1.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.0.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.0.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.1.blocks.1.conv.shortcut: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.1.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.1.blocks.1.conv.down: Identity()\n",
            "stages.1.blocks.1.conv.conv1_1x1: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.1.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.conv2_kxk: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "stages.1.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.1.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.1.blocks.1.conv.se.fc1: Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.bn: Identity()\n",
            "stages.1.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.1.blocks.1.conv.se.fc2: Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.1.blocks.1.conv.conv3_1x1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.1.blocks.1.conv.drop_path: Identity()\n",
            "stages.1.blocks.1.attn_block.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls1: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_block.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_block.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_block.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_block.ls2: Identity()\n",
            "stages.1.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.1.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.1.blocks.1.attn_grid.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.1.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.1.blocks.1.attn_grid.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
            "stages.1.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.1.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.1.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.2.blocks.0.conv.shortcut.expand: Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.0.conv.down: Identity()\n",
            "stages.2.blocks.0.conv.conv1_1x1: Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.conv2_kxk: Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
            "stages.2.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.0.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.bn: Identity()\n",
            "stages.2.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.0.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.0.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.0.conv.drop_path: Identity()\n",
            "stages.2.blocks.0.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls1: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_block.ls2: Identity()\n",
            "stages.2.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.0.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.0.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.1.conv.shortcut: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.1.conv.down: Identity()\n",
            "stages.2.blocks.1.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.1.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.bn: Identity()\n",
            "stages.2.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.1.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.1.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.1.conv.drop_path: Identity()\n",
            "stages.2.blocks.1.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls1: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_block.ls2: Identity()\n",
            "stages.2.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.1.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.1.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.1.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.2.conv.shortcut: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.2.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.2.conv.down: Identity()\n",
            "stages.2.blocks.2.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.2.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.2.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.2.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.2.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.bn: Identity()\n",
            "stages.2.blocks.2.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.2.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.2.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.2.conv.drop_path: Identity()\n",
            "stages.2.blocks.2.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls1: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_block.ls2: Identity()\n",
            "stages.2.blocks.2.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.2.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.2.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.2.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.2.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.2.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.2.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.2.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.3.conv.shortcut: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.3.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.3.conv.down: Identity()\n",
            "stages.2.blocks.3.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.3.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.3.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.3.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.3.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.bn: Identity()\n",
            "stages.2.blocks.3.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.3.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.3.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.3.conv.drop_path: Identity()\n",
            "stages.2.blocks.3.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls1: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_block.ls2: Identity()\n",
            "stages.2.blocks.3.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.3.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.3.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.3.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.3.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.3.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.3.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.3.attn_grid.drop_path2: Identity()\n",
            "stages.2.blocks.4.conv.shortcut: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.drop: Identity()\n",
            "stages.2.blocks.4.conv.pre_norm.act: Identity()\n",
            "stages.2.blocks.4.conv.down: Identity()\n",
            "stages.2.blocks.4.conv.conv1_1x1: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.2.blocks.4.conv.norm1.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm1.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.conv2_kxk: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "stages.2.blocks.4.conv.norm2.drop: Identity()\n",
            "stages.2.blocks.4.conv.norm2.act: GELUTanh()\n",
            "stages.2.blocks.4.conv.se.fc1: Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.bn: Identity()\n",
            "stages.2.blocks.4.conv.se.act: SiLU(inplace=True)\n",
            "stages.2.blocks.4.conv.se.fc2: Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.se.gate: Sigmoid()\n",
            "stages.2.blocks.4.conv.conv3_1x1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.2.blocks.4.conv.drop_path: Identity()\n",
            "stages.2.blocks.4.attn_block.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls1: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_block.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_block.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_block.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_block.ls2: Identity()\n",
            "stages.2.blocks.4.attn_block.drop_path2: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.2.blocks.4.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls1: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path1: Identity()\n",
            "stages.2.blocks.4.attn_grid.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.act: GELUTanh()\n",
            "stages.2.blocks.4.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.mlp.norm: Identity()\n",
            "stages.2.blocks.4.attn_grid.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
            "stages.2.blocks.4.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.2.blocks.4.attn_grid.ls2: Identity()\n",
            "stages.2.blocks.4.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.0.conv.shortcut.pool: AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
            "stages.3.blocks.0.conv.shortcut.expand: Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.0.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.0.conv.down: Identity()\n",
            "stages.3.blocks.0.conv.conv1_1x1: Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.0.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.conv2_kxk: Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
            "stages.3.blocks.0.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.0.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.0.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.bn: Identity()\n",
            "stages.3.blocks.0.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.0.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.0.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.0.conv.drop_path: Identity()\n",
            "stages.3.blocks.0.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls1: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_block.ls2: Identity()\n",
            "stages.3.blocks.0.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.0.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.0.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.0.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.0.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.0.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.0.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.0.attn_grid.drop_path2: Identity()\n",
            "stages.3.blocks.1.conv.shortcut: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.drop: Identity()\n",
            "stages.3.blocks.1.conv.pre_norm.act: Identity()\n",
            "stages.3.blocks.1.conv.down: Identity()\n",
            "stages.3.blocks.1.conv.conv1_1x1: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "stages.3.blocks.1.conv.norm1.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm1.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.conv2_kxk: Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
            "stages.3.blocks.1.conv.norm2.drop: Identity()\n",
            "stages.3.blocks.1.conv.norm2.act: GELUTanh()\n",
            "stages.3.blocks.1.conv.se.fc1: Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.bn: Identity()\n",
            "stages.3.blocks.1.conv.se.act: SiLU(inplace=True)\n",
            "stages.3.blocks.1.conv.se.fc2: Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.se.gate: Sigmoid()\n",
            "stages.3.blocks.1.conv.conv3_1x1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "stages.3.blocks.1.conv.drop_path: Identity()\n",
            "stages.3.blocks.1.attn_block.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_block.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls1: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_block.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_block.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_block.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_block.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_block.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_block.ls2: Identity()\n",
            "stages.3.blocks.1.attn_block.drop_path2: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.rel_pos: RelPosBiasTf()\n",
            "stages.3.blocks.1.attn_grid.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls1: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path1: Identity()\n",
            "stages.3.blocks.1.attn_grid.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.act: GELUTanh()\n",
            "stages.3.blocks.1.attn_grid.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.mlp.norm: Identity()\n",
            "stages.3.blocks.1.attn_grid.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
            "stages.3.blocks.1.attn_grid.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
            "stages.3.blocks.1.attn_grid.ls2: Identity()\n",
            "stages.3.blocks.1.attn_grid.drop_path2: Identity()\n",
            "norm: Identity()\n",
            "head.global_pool.pool: AdaptiveAvgPool2d(output_size=1)\n",
            "head.global_pool.flatten: Identity()\n",
            "head.norm: LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
            "head.flatten: Flatten(start_dim=1, end_dim=-1)\n",
            "head.pre_logits.fc: Linear(in_features=512, out_features=512, bias=True)\n",
            "head.pre_logits.act: Tanh()\n",
            "head.drop: Dropout(p=0.0, inplace=False)\n",
            "head.fc: Linear(in_features=512, out_features=1000, bias=True)\n",
            "Enter the layer name up to which you want the submodel: stages.2.blocks.4.attn_grid.drop_path2\n",
            "Layer 'stages.2.blocks.4.attn_grid.drop_path2' not found in the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_partial_model(model, start_layer_name, end_layer_name):\n",
        "    \"\"\"\n",
        "    Creates a new model containing layers from start_layer_name to end_layer_name.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The original model.\n",
        "    - start_layer_name (str): Name of the starting layer.\n",
        "    - end_layer_name (str): Name of the ending layer.\n",
        "\n",
        "    Returns:\n",
        "    - new_model (torch.nn.Module): The new model containing the desired layers.\n",
        "    \"\"\"\n",
        "    new_model = nn.Sequential()\n",
        "\n",
        "    found_start_layer = False\n",
        "    for name, module in model.named_children():\n",
        "        if not found_start_layer:\n",
        "            if name == start_layer_name.split('.')[0]:\n",
        "                found_start_layer = True\n",
        "                new_model.add_module(name, module)\n",
        "        else:\n",
        "            new_model.add_module(name, module)\n",
        "            if name == end_layer_name.split('.')[0]:\n",
        "                break\n",
        "\n",
        "    return new_model\n",
        "\n",
        "# Define the names of the start and end layers\n",
        "start_layer_name = 'stem.conv1'\n",
        "end_layer_name = 'stages.2.blocks.4.attn_grid.drop_path2'\n",
        "\n",
        "# Create the partial model\n",
        "partial_model = create_partial_model(model, start_layer_name, end_layer_name)\n"
      ],
      "metadata": {
        "id": "hEcWWRZjbMm-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the model variable contains your model\n",
        "summary(partial_model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2gssZLmcWDe",
        "outputId": "1373e15d-6818-4b88-f3ed-e16863254bae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "        Conv2dSame-1         [-1, 64, 112, 112]           1,792\n",
            "          Identity-2         [-1, 64, 112, 112]               0\n",
            "          GELUTanh-3         [-1, 64, 112, 112]               0\n",
            "    BatchNormAct2d-4         [-1, 64, 112, 112]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,928\n",
            "              Stem-6         [-1, 64, 112, 112]               0\n",
            "     AvgPool2dSame-7           [-1, 64, 56, 56]               0\n",
            "          Identity-8           [-1, 64, 56, 56]               0\n",
            "      Downsample2d-9           [-1, 64, 56, 56]               0\n",
            "         Identity-10         [-1, 64, 112, 112]               0\n",
            "         Identity-11         [-1, 64, 112, 112]               0\n",
            "   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n",
            "         Identity-13         [-1, 64, 112, 112]               0\n",
            "           Conv2d-14        [-1, 256, 112, 112]          16,384\n",
            "         Identity-15        [-1, 256, 112, 112]               0\n",
            "         GELUTanh-16        [-1, 256, 112, 112]               0\n",
            "   BatchNormAct2d-17        [-1, 256, 112, 112]             512\n",
            "       Conv2dSame-18          [-1, 256, 56, 56]           2,304\n",
            "         Identity-19          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-20          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22             [-1, 16, 1, 1]           4,112\n",
            "         Identity-23             [-1, 16, 1, 1]               0\n",
            "             SiLU-24             [-1, 16, 1, 1]               0\n",
            "           Conv2d-25            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-26            [-1, 256, 1, 1]               0\n",
            "         SEModule-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          16,448\n",
            "         Identity-29           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-30           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-31           [-1, 56, 56, 64]             128\n",
            "           Linear-32            [-1, 7, 7, 192]          12,480\n",
            "           Linear-33             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-34             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-35             [-1, 7, 7, 64]               0\n",
            "         Identity-36           [-1, 56, 56, 64]               0\n",
            "         Identity-37           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-38           [-1, 56, 56, 64]             128\n",
            "           Linear-39          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-40          [-1, 56, 56, 256]               0\n",
            "          Dropout-41          [-1, 56, 56, 256]               0\n",
            "         Identity-42          [-1, 56, 56, 256]               0\n",
            "           Linear-43           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-44           [-1, 56, 56, 64]               0\n",
            "              Mlp-45           [-1, 56, 56, 64]               0\n",
            "         Identity-46           [-1, 56, 56, 64]               0\n",
            "         Identity-47           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-48           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 64]             128\n",
            "           Linear-50            [-1, 7, 7, 192]          12,480\n",
            "           Linear-51             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-52             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-53             [-1, 7, 7, 64]               0\n",
            "         Identity-54           [-1, 56, 56, 64]               0\n",
            "         Identity-55           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
            "           Linear-57          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-58          [-1, 56, 56, 256]               0\n",
            "          Dropout-59          [-1, 56, 56, 256]               0\n",
            "         Identity-60          [-1, 56, 56, 256]               0\n",
            "           Linear-61           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-62           [-1, 56, 56, 64]               0\n",
            "              Mlp-63           [-1, 56, 56, 64]               0\n",
            "         Identity-64           [-1, 56, 56, 64]               0\n",
            "         Identity-65           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-66           [-1, 56, 56, 64]               0\n",
            "     MaxxVitBlock-67           [-1, 64, 56, 56]               0\n",
            "         Identity-68           [-1, 64, 56, 56]               0\n",
            "         Identity-69           [-1, 64, 56, 56]               0\n",
            "         Identity-70           [-1, 64, 56, 56]               0\n",
            "   BatchNormAct2d-71           [-1, 64, 56, 56]             128\n",
            "         Identity-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73          [-1, 256, 56, 56]          16,384\n",
            "         Identity-74          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-75          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-76          [-1, 256, 56, 56]             512\n",
            "           Conv2d-77          [-1, 256, 56, 56]           2,304\n",
            "         Identity-78          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-79          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-80          [-1, 256, 56, 56]             512\n",
            "           Conv2d-81             [-1, 16, 1, 1]           4,112\n",
            "         Identity-82             [-1, 16, 1, 1]               0\n",
            "             SiLU-83             [-1, 16, 1, 1]               0\n",
            "           Conv2d-84            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-85            [-1, 256, 1, 1]               0\n",
            "         SEModule-86          [-1, 256, 56, 56]               0\n",
            "           Conv2d-87           [-1, 64, 56, 56]          16,448\n",
            "         Identity-88           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-89           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-90           [-1, 56, 56, 64]             128\n",
            "           Linear-91            [-1, 7, 7, 192]          12,480\n",
            "           Linear-92             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-93             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-94             [-1, 7, 7, 64]               0\n",
            "         Identity-95           [-1, 56, 56, 64]               0\n",
            "         Identity-96           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-97           [-1, 56, 56, 64]             128\n",
            "           Linear-98          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-99          [-1, 56, 56, 256]               0\n",
            "         Dropout-100          [-1, 56, 56, 256]               0\n",
            "        Identity-101          [-1, 56, 56, 256]               0\n",
            "          Linear-102           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-103           [-1, 56, 56, 64]               0\n",
            "             Mlp-104           [-1, 56, 56, 64]               0\n",
            "        Identity-105           [-1, 56, 56, 64]               0\n",
            "        Identity-106           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-107           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-108           [-1, 56, 56, 64]             128\n",
            "          Linear-109            [-1, 7, 7, 192]          12,480\n",
            "          Linear-110             [-1, 7, 7, 64]           4,160\n",
            "         Dropout-111             [-1, 7, 7, 64]               0\n",
            "     AttentionCl-112             [-1, 7, 7, 64]               0\n",
            "        Identity-113           [-1, 56, 56, 64]               0\n",
            "        Identity-114           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-115           [-1, 56, 56, 64]             128\n",
            "          Linear-116          [-1, 56, 56, 256]          16,640\n",
            "        GELUTanh-117          [-1, 56, 56, 256]               0\n",
            "         Dropout-118          [-1, 56, 56, 256]               0\n",
            "        Identity-119          [-1, 56, 56, 256]               0\n",
            "          Linear-120           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-121           [-1, 56, 56, 64]               0\n",
            "             Mlp-122           [-1, 56, 56, 64]               0\n",
            "        Identity-123           [-1, 56, 56, 64]               0\n",
            "        Identity-124           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-125           [-1, 56, 56, 64]               0\n",
            "    MaxxVitBlock-126           [-1, 64, 56, 56]               0\n",
            "    MaxxVitStage-127           [-1, 64, 56, 56]               0\n",
            "   AvgPool2dSame-128           [-1, 64, 28, 28]               0\n",
            "          Conv2d-129          [-1, 128, 28, 28]           8,320\n",
            "    Downsample2d-130          [-1, 128, 28, 28]               0\n",
            "        Identity-131           [-1, 64, 56, 56]               0\n",
            "        Identity-132           [-1, 64, 56, 56]               0\n",
            "  BatchNormAct2d-133           [-1, 64, 56, 56]             128\n",
            "        Identity-134           [-1, 64, 56, 56]               0\n",
            "          Conv2d-135          [-1, 512, 56, 56]          32,768\n",
            "        Identity-136          [-1, 512, 56, 56]               0\n",
            "        GELUTanh-137          [-1, 512, 56, 56]               0\n",
            "  BatchNormAct2d-138          [-1, 512, 56, 56]           1,024\n",
            "      Conv2dSame-139          [-1, 512, 28, 28]           4,608\n",
            "        Identity-140          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-141          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-142          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-143             [-1, 32, 1, 1]          16,416\n",
            "        Identity-144             [-1, 32, 1, 1]               0\n",
            "            SiLU-145             [-1, 32, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-147            [-1, 512, 1, 1]               0\n",
            "        SEModule-148          [-1, 512, 28, 28]               0\n",
            "          Conv2d-149          [-1, 128, 28, 28]          65,664\n",
            "        Identity-150          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-151          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-152          [-1, 28, 28, 128]             256\n",
            "          Linear-153            [-1, 7, 7, 384]          49,536\n",
            "          Linear-154            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-155            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-156            [-1, 7, 7, 128]               0\n",
            "        Identity-157          [-1, 28, 28, 128]               0\n",
            "        Identity-158          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-159          [-1, 28, 28, 128]             256\n",
            "          Linear-160          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-161          [-1, 28, 28, 512]               0\n",
            "         Dropout-162          [-1, 28, 28, 512]               0\n",
            "        Identity-163          [-1, 28, 28, 512]               0\n",
            "          Linear-164          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-165          [-1, 28, 28, 128]               0\n",
            "             Mlp-166          [-1, 28, 28, 128]               0\n",
            "        Identity-167          [-1, 28, 28, 128]               0\n",
            "        Identity-168          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-169          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-170          [-1, 28, 28, 128]             256\n",
            "          Linear-171            [-1, 7, 7, 384]          49,536\n",
            "          Linear-172            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-173            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-174            [-1, 7, 7, 128]               0\n",
            "        Identity-175          [-1, 28, 28, 128]               0\n",
            "        Identity-176          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-177          [-1, 28, 28, 128]             256\n",
            "          Linear-178          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-179          [-1, 28, 28, 512]               0\n",
            "         Dropout-180          [-1, 28, 28, 512]               0\n",
            "        Identity-181          [-1, 28, 28, 512]               0\n",
            "          Linear-182          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-183          [-1, 28, 28, 128]               0\n",
            "             Mlp-184          [-1, 28, 28, 128]               0\n",
            "        Identity-185          [-1, 28, 28, 128]               0\n",
            "        Identity-186          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-187          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-188          [-1, 128, 28, 28]               0\n",
            "        Identity-189          [-1, 128, 28, 28]               0\n",
            "        Identity-190          [-1, 128, 28, 28]               0\n",
            "        Identity-191          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-192          [-1, 128, 28, 28]             256\n",
            "        Identity-193          [-1, 128, 28, 28]               0\n",
            "          Conv2d-194          [-1, 512, 28, 28]          65,536\n",
            "        Identity-195          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-196          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-197          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-198          [-1, 512, 28, 28]           4,608\n",
            "        Identity-199          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-200          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-201          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-202             [-1, 32, 1, 1]          16,416\n",
            "        Identity-203             [-1, 32, 1, 1]               0\n",
            "            SiLU-204             [-1, 32, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-206            [-1, 512, 1, 1]               0\n",
            "        SEModule-207          [-1, 512, 28, 28]               0\n",
            "          Conv2d-208          [-1, 128, 28, 28]          65,664\n",
            "        Identity-209          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-210          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-211          [-1, 28, 28, 128]             256\n",
            "          Linear-212            [-1, 7, 7, 384]          49,536\n",
            "          Linear-213            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-214            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-215            [-1, 7, 7, 128]               0\n",
            "        Identity-216          [-1, 28, 28, 128]               0\n",
            "        Identity-217          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-218          [-1, 28, 28, 128]             256\n",
            "          Linear-219          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-220          [-1, 28, 28, 512]               0\n",
            "         Dropout-221          [-1, 28, 28, 512]               0\n",
            "        Identity-222          [-1, 28, 28, 512]               0\n",
            "          Linear-223          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-224          [-1, 28, 28, 128]               0\n",
            "             Mlp-225          [-1, 28, 28, 128]               0\n",
            "        Identity-226          [-1, 28, 28, 128]               0\n",
            "        Identity-227          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-228          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-229          [-1, 28, 28, 128]             256\n",
            "          Linear-230            [-1, 7, 7, 384]          49,536\n",
            "          Linear-231            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-232            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-233            [-1, 7, 7, 128]               0\n",
            "        Identity-234          [-1, 28, 28, 128]               0\n",
            "        Identity-235          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-236          [-1, 28, 28, 128]             256\n",
            "          Linear-237          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-238          [-1, 28, 28, 512]               0\n",
            "         Dropout-239          [-1, 28, 28, 512]               0\n",
            "        Identity-240          [-1, 28, 28, 512]               0\n",
            "          Linear-241          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-242          [-1, 28, 28, 128]               0\n",
            "             Mlp-243          [-1, 28, 28, 128]               0\n",
            "        Identity-244          [-1, 28, 28, 128]               0\n",
            "        Identity-245          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-246          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-247          [-1, 128, 28, 28]               0\n",
            "    MaxxVitStage-248          [-1, 128, 28, 28]               0\n",
            "   AvgPool2dSame-249          [-1, 128, 14, 14]               0\n",
            "          Conv2d-250          [-1, 256, 14, 14]          33,024\n",
            "    Downsample2d-251          [-1, 256, 14, 14]               0\n",
            "        Identity-252          [-1, 128, 28, 28]               0\n",
            "        Identity-253          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-254          [-1, 128, 28, 28]             256\n",
            "        Identity-255          [-1, 128, 28, 28]               0\n",
            "          Conv2d-256         [-1, 1024, 28, 28]         131,072\n",
            "        Identity-257         [-1, 1024, 28, 28]               0\n",
            "        GELUTanh-258         [-1, 1024, 28, 28]               0\n",
            "  BatchNormAct2d-259         [-1, 1024, 28, 28]           2,048\n",
            "      Conv2dSame-260         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-261         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-262         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-263         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-264             [-1, 64, 1, 1]          65,600\n",
            "        Identity-265             [-1, 64, 1, 1]               0\n",
            "            SiLU-266             [-1, 64, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-268           [-1, 1024, 1, 1]               0\n",
            "        SEModule-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "        Identity-271          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-272          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-273          [-1, 14, 14, 256]             512\n",
            "          Linear-274            [-1, 7, 7, 768]         197,376\n",
            "          Linear-275            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-276            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-277            [-1, 7, 7, 256]               0\n",
            "        Identity-278          [-1, 14, 14, 256]               0\n",
            "        Identity-279          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-280          [-1, 14, 14, 256]             512\n",
            "          Linear-281         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-282         [-1, 14, 14, 1024]               0\n",
            "         Dropout-283         [-1, 14, 14, 1024]               0\n",
            "        Identity-284         [-1, 14, 14, 1024]               0\n",
            "          Linear-285          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-286          [-1, 14, 14, 256]               0\n",
            "             Mlp-287          [-1, 14, 14, 256]               0\n",
            "        Identity-288          [-1, 14, 14, 256]               0\n",
            "        Identity-289          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-290          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-291          [-1, 14, 14, 256]             512\n",
            "          Linear-292            [-1, 7, 7, 768]         197,376\n",
            "          Linear-293            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-294            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-295            [-1, 7, 7, 256]               0\n",
            "        Identity-296          [-1, 14, 14, 256]               0\n",
            "        Identity-297          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-298          [-1, 14, 14, 256]             512\n",
            "          Linear-299         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-300         [-1, 14, 14, 1024]               0\n",
            "         Dropout-301         [-1, 14, 14, 1024]               0\n",
            "        Identity-302         [-1, 14, 14, 1024]               0\n",
            "          Linear-303          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-304          [-1, 14, 14, 256]               0\n",
            "             Mlp-305          [-1, 14, 14, 256]               0\n",
            "        Identity-306          [-1, 14, 14, 256]               0\n",
            "        Identity-307          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-308          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-309          [-1, 256, 14, 14]               0\n",
            "        Identity-310          [-1, 256, 14, 14]               0\n",
            "        Identity-311          [-1, 256, 14, 14]               0\n",
            "        Identity-312          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-313          [-1, 256, 14, 14]             512\n",
            "        Identity-314          [-1, 256, 14, 14]               0\n",
            "          Conv2d-315         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-316         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-317         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-318         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-319         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-320         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-321         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-322         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-323             [-1, 64, 1, 1]          65,600\n",
            "        Identity-324             [-1, 64, 1, 1]               0\n",
            "            SiLU-325             [-1, 64, 1, 1]               0\n",
            "          Conv2d-326           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-327           [-1, 1024, 1, 1]               0\n",
            "        SEModule-328         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-329          [-1, 256, 14, 14]         262,400\n",
            "        Identity-330          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-331          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-332          [-1, 14, 14, 256]             512\n",
            "          Linear-333            [-1, 7, 7, 768]         197,376\n",
            "          Linear-334            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-335            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-336            [-1, 7, 7, 256]               0\n",
            "        Identity-337          [-1, 14, 14, 256]               0\n",
            "        Identity-338          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-339          [-1, 14, 14, 256]             512\n",
            "          Linear-340         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-341         [-1, 14, 14, 1024]               0\n",
            "         Dropout-342         [-1, 14, 14, 1024]               0\n",
            "        Identity-343         [-1, 14, 14, 1024]               0\n",
            "          Linear-344          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-345          [-1, 14, 14, 256]               0\n",
            "             Mlp-346          [-1, 14, 14, 256]               0\n",
            "        Identity-347          [-1, 14, 14, 256]               0\n",
            "        Identity-348          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-349          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-350          [-1, 14, 14, 256]             512\n",
            "          Linear-351            [-1, 7, 7, 768]         197,376\n",
            "          Linear-352            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-353            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-354            [-1, 7, 7, 256]               0\n",
            "        Identity-355          [-1, 14, 14, 256]               0\n",
            "        Identity-356          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 256]             512\n",
            "          Linear-358         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-359         [-1, 14, 14, 1024]               0\n",
            "         Dropout-360         [-1, 14, 14, 1024]               0\n",
            "        Identity-361         [-1, 14, 14, 1024]               0\n",
            "          Linear-362          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-363          [-1, 14, 14, 256]               0\n",
            "             Mlp-364          [-1, 14, 14, 256]               0\n",
            "        Identity-365          [-1, 14, 14, 256]               0\n",
            "        Identity-366          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-367          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-368          [-1, 256, 14, 14]               0\n",
            "        Identity-369          [-1, 256, 14, 14]               0\n",
            "        Identity-370          [-1, 256, 14, 14]               0\n",
            "        Identity-371          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-372          [-1, 256, 14, 14]             512\n",
            "        Identity-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-375         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-376         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-377         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-378         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-379         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-380         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-381         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-382             [-1, 64, 1, 1]          65,600\n",
            "        Identity-383             [-1, 64, 1, 1]               0\n",
            "            SiLU-384             [-1, 64, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-386           [-1, 1024, 1, 1]               0\n",
            "        SEModule-387         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-388          [-1, 256, 14, 14]         262,400\n",
            "        Identity-389          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-390          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-391          [-1, 14, 14, 256]             512\n",
            "          Linear-392            [-1, 7, 7, 768]         197,376\n",
            "          Linear-393            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-394            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-395            [-1, 7, 7, 256]               0\n",
            "        Identity-396          [-1, 14, 14, 256]               0\n",
            "        Identity-397          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-398          [-1, 14, 14, 256]             512\n",
            "          Linear-399         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-400         [-1, 14, 14, 1024]               0\n",
            "         Dropout-401         [-1, 14, 14, 1024]               0\n",
            "        Identity-402         [-1, 14, 14, 1024]               0\n",
            "          Linear-403          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-404          [-1, 14, 14, 256]               0\n",
            "             Mlp-405          [-1, 14, 14, 256]               0\n",
            "        Identity-406          [-1, 14, 14, 256]               0\n",
            "        Identity-407          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-408          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-409          [-1, 14, 14, 256]             512\n",
            "          Linear-410            [-1, 7, 7, 768]         197,376\n",
            "          Linear-411            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-412            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-413            [-1, 7, 7, 256]               0\n",
            "        Identity-414          [-1, 14, 14, 256]               0\n",
            "        Identity-415          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-416          [-1, 14, 14, 256]             512\n",
            "          Linear-417         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-418         [-1, 14, 14, 1024]               0\n",
            "         Dropout-419         [-1, 14, 14, 1024]               0\n",
            "        Identity-420         [-1, 14, 14, 1024]               0\n",
            "          Linear-421          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-422          [-1, 14, 14, 256]               0\n",
            "             Mlp-423          [-1, 14, 14, 256]               0\n",
            "        Identity-424          [-1, 14, 14, 256]               0\n",
            "        Identity-425          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-426          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-427          [-1, 256, 14, 14]               0\n",
            "        Identity-428          [-1, 256, 14, 14]               0\n",
            "        Identity-429          [-1, 256, 14, 14]               0\n",
            "        Identity-430          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-431          [-1, 256, 14, 14]             512\n",
            "        Identity-432          [-1, 256, 14, 14]               0\n",
            "          Conv2d-433         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-434         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-435         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-436         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-437         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-438         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-439         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-440         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-441             [-1, 64, 1, 1]          65,600\n",
            "        Identity-442             [-1, 64, 1, 1]               0\n",
            "            SiLU-443             [-1, 64, 1, 1]               0\n",
            "          Conv2d-444           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-445           [-1, 1024, 1, 1]               0\n",
            "        SEModule-446         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-447          [-1, 256, 14, 14]         262,400\n",
            "        Identity-448          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-449          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-450          [-1, 14, 14, 256]             512\n",
            "          Linear-451            [-1, 7, 7, 768]         197,376\n",
            "          Linear-452            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-453            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-454            [-1, 7, 7, 256]               0\n",
            "        Identity-455          [-1, 14, 14, 256]               0\n",
            "        Identity-456          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-457          [-1, 14, 14, 256]             512\n",
            "          Linear-458         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-459         [-1, 14, 14, 1024]               0\n",
            "         Dropout-460         [-1, 14, 14, 1024]               0\n",
            "        Identity-461         [-1, 14, 14, 1024]               0\n",
            "          Linear-462          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-463          [-1, 14, 14, 256]               0\n",
            "             Mlp-464          [-1, 14, 14, 256]               0\n",
            "        Identity-465          [-1, 14, 14, 256]               0\n",
            "        Identity-466          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-467          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-468          [-1, 14, 14, 256]             512\n",
            "          Linear-469            [-1, 7, 7, 768]         197,376\n",
            "          Linear-470            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-471            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-472            [-1, 7, 7, 256]               0\n",
            "        Identity-473          [-1, 14, 14, 256]               0\n",
            "        Identity-474          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-475          [-1, 14, 14, 256]             512\n",
            "          Linear-476         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-477         [-1, 14, 14, 1024]               0\n",
            "         Dropout-478         [-1, 14, 14, 1024]               0\n",
            "        Identity-479         [-1, 14, 14, 1024]               0\n",
            "          Linear-480          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-481          [-1, 14, 14, 256]               0\n",
            "             Mlp-482          [-1, 14, 14, 256]               0\n",
            "        Identity-483          [-1, 14, 14, 256]               0\n",
            "        Identity-484          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-485          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-486          [-1, 256, 14, 14]               0\n",
            "        Identity-487          [-1, 256, 14, 14]               0\n",
            "        Identity-488          [-1, 256, 14, 14]               0\n",
            "        Identity-489          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-490          [-1, 256, 14, 14]             512\n",
            "        Identity-491          [-1, 256, 14, 14]               0\n",
            "          Conv2d-492         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-493         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-494         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-495         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-496         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-497         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-498         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-499         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-500             [-1, 64, 1, 1]          65,600\n",
            "        Identity-501             [-1, 64, 1, 1]               0\n",
            "            SiLU-502             [-1, 64, 1, 1]               0\n",
            "          Conv2d-503           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-504           [-1, 1024, 1, 1]               0\n",
            "        SEModule-505         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-506          [-1, 256, 14, 14]         262,400\n",
            "        Identity-507          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-508          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 256]             512\n",
            "          Linear-510            [-1, 7, 7, 768]         197,376\n",
            "          Linear-511            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-512            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-513            [-1, 7, 7, 256]               0\n",
            "        Identity-514          [-1, 14, 14, 256]               0\n",
            "        Identity-515          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-516          [-1, 14, 14, 256]             512\n",
            "          Linear-517         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-518         [-1, 14, 14, 1024]               0\n",
            "         Dropout-519         [-1, 14, 14, 1024]               0\n",
            "        Identity-520         [-1, 14, 14, 1024]               0\n",
            "          Linear-521          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-522          [-1, 14, 14, 256]               0\n",
            "             Mlp-523          [-1, 14, 14, 256]               0\n",
            "        Identity-524          [-1, 14, 14, 256]               0\n",
            "        Identity-525          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-526          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-527          [-1, 14, 14, 256]             512\n",
            "          Linear-528            [-1, 7, 7, 768]         197,376\n",
            "          Linear-529            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-530            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-531            [-1, 7, 7, 256]               0\n",
            "        Identity-532          [-1, 14, 14, 256]               0\n",
            "        Identity-533          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-534          [-1, 14, 14, 256]             512\n",
            "          Linear-535         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-536         [-1, 14, 14, 1024]               0\n",
            "         Dropout-537         [-1, 14, 14, 1024]               0\n",
            "        Identity-538         [-1, 14, 14, 1024]               0\n",
            "          Linear-539          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-540          [-1, 14, 14, 256]               0\n",
            "             Mlp-541          [-1, 14, 14, 256]               0\n",
            "        Identity-542          [-1, 14, 14, 256]               0\n",
            "        Identity-543          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-544          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-545          [-1, 256, 14, 14]               0\n",
            "    MaxxVitStage-546          [-1, 256, 14, 14]               0\n",
            "   AvgPool2dSame-547            [-1, 256, 7, 7]               0\n",
            "          Conv2d-548            [-1, 512, 7, 7]         131,584\n",
            "    Downsample2d-549            [-1, 512, 7, 7]               0\n",
            "        Identity-550          [-1, 256, 14, 14]               0\n",
            "        Identity-551          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-552          [-1, 256, 14, 14]             512\n",
            "        Identity-553          [-1, 256, 14, 14]               0\n",
            "          Conv2d-554         [-1, 2048, 14, 14]         524,288\n",
            "        Identity-555         [-1, 2048, 14, 14]               0\n",
            "        GELUTanh-556         [-1, 2048, 14, 14]               0\n",
            "  BatchNormAct2d-557         [-1, 2048, 14, 14]           4,096\n",
            "      Conv2dSame-558           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-559           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-560           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-561           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-562            [-1, 128, 1, 1]         262,272\n",
            "        Identity-563            [-1, 128, 1, 1]               0\n",
            "            SiLU-564            [-1, 128, 1, 1]               0\n",
            "          Conv2d-565           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-566           [-1, 2048, 1, 1]               0\n",
            "        SEModule-567           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-568            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-569            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-570            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-571            [-1, 7, 7, 512]           1,024\n",
            "          Linear-572           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-573            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-574            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-575            [-1, 7, 7, 512]               0\n",
            "        Identity-576            [-1, 7, 7, 512]               0\n",
            "        Identity-577            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-578            [-1, 7, 7, 512]           1,024\n",
            "          Linear-579           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-580           [-1, 7, 7, 2048]               0\n",
            "         Dropout-581           [-1, 7, 7, 2048]               0\n",
            "        Identity-582           [-1, 7, 7, 2048]               0\n",
            "          Linear-583            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-584            [-1, 7, 7, 512]               0\n",
            "             Mlp-585            [-1, 7, 7, 512]               0\n",
            "        Identity-586            [-1, 7, 7, 512]               0\n",
            "        Identity-587            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-588            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-589            [-1, 7, 7, 512]           1,024\n",
            "          Linear-590           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-591            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-592            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-593            [-1, 7, 7, 512]               0\n",
            "        Identity-594            [-1, 7, 7, 512]               0\n",
            "        Identity-595            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-596            [-1, 7, 7, 512]           1,024\n",
            "          Linear-597           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-598           [-1, 7, 7, 2048]               0\n",
            "         Dropout-599           [-1, 7, 7, 2048]               0\n",
            "        Identity-600           [-1, 7, 7, 2048]               0\n",
            "          Linear-601            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-602            [-1, 7, 7, 512]               0\n",
            "             Mlp-603            [-1, 7, 7, 512]               0\n",
            "        Identity-604            [-1, 7, 7, 512]               0\n",
            "        Identity-605            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-606            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-607            [-1, 512, 7, 7]               0\n",
            "        Identity-608            [-1, 512, 7, 7]               0\n",
            "        Identity-609            [-1, 512, 7, 7]               0\n",
            "        Identity-610            [-1, 512, 7, 7]               0\n",
            "  BatchNormAct2d-611            [-1, 512, 7, 7]           1,024\n",
            "        Identity-612            [-1, 512, 7, 7]               0\n",
            "          Conv2d-613           [-1, 2048, 7, 7]       1,048,576\n",
            "        Identity-614           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-615           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-616           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-617           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-618           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-619           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-620           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-621            [-1, 128, 1, 1]         262,272\n",
            "        Identity-622            [-1, 128, 1, 1]               0\n",
            "            SiLU-623            [-1, 128, 1, 1]               0\n",
            "          Conv2d-624           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-625           [-1, 2048, 1, 1]               0\n",
            "        SEModule-626           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-627            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-628            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-629            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-630            [-1, 7, 7, 512]           1,024\n",
            "          Linear-631           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-632            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-633            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-634            [-1, 7, 7, 512]               0\n",
            "        Identity-635            [-1, 7, 7, 512]               0\n",
            "        Identity-636            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-637            [-1, 7, 7, 512]           1,024\n",
            "          Linear-638           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-639           [-1, 7, 7, 2048]               0\n",
            "         Dropout-640           [-1, 7, 7, 2048]               0\n",
            "        Identity-641           [-1, 7, 7, 2048]               0\n",
            "          Linear-642            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-643            [-1, 7, 7, 512]               0\n",
            "             Mlp-644            [-1, 7, 7, 512]               0\n",
            "        Identity-645            [-1, 7, 7, 512]               0\n",
            "        Identity-646            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-647            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-648            [-1, 7, 7, 512]           1,024\n",
            "          Linear-649           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-650            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-651            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-652            [-1, 7, 7, 512]               0\n",
            "        Identity-653            [-1, 7, 7, 512]               0\n",
            "        Identity-654            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-655            [-1, 7, 7, 512]           1,024\n",
            "          Linear-656           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-657           [-1, 7, 7, 2048]               0\n",
            "         Dropout-658           [-1, 7, 7, 2048]               0\n",
            "        Identity-659           [-1, 7, 7, 2048]               0\n",
            "          Linear-660            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-661            [-1, 7, 7, 512]               0\n",
            "             Mlp-662            [-1, 7, 7, 512]               0\n",
            "        Identity-663            [-1, 7, 7, 512]               0\n",
            "        Identity-664            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-665            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-666            [-1, 512, 7, 7]               0\n",
            "    MaxxVitStage-667            [-1, 512, 7, 7]               0\n",
            "================================================================\n",
            "Total params: 30,111,456\n",
            "Trainable params: 30,111,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 886.87\n",
            "Params size (MB): 114.87\n",
            "Estimated Total Size (MB): 1002.31\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_layer_names(model, prefix='', layer_names=[]):\n",
        "    for name, module in model.named_children():\n",
        "        if len(list(module.children())) > 0:\n",
        "            collect_layer_names(module, prefix + name + '.', layer_names)\n",
        "        else:\n",
        "            layer_names.append(prefix + name)\n",
        "    return layer_names\n",
        "\n",
        "layer_names = collect_layer_names(model)\n",
        "for name in layer_names:\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1onjKeehl1",
        "outputId": "d81f9492-29db-412d-a406-4c08484469ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem.conv1\n",
            "stem.norm1.drop\n",
            "stem.norm1.act\n",
            "stem.conv2\n",
            "stages.0.blocks.0.conv.shortcut.pool\n",
            "stages.0.blocks.0.conv.shortcut.expand\n",
            "stages.0.blocks.0.conv.pre_norm.drop\n",
            "stages.0.blocks.0.conv.pre_norm.act\n",
            "stages.0.blocks.0.conv.down\n",
            "stages.0.blocks.0.conv.conv1_1x1\n",
            "stages.0.blocks.0.conv.norm1.drop\n",
            "stages.0.blocks.0.conv.norm1.act\n",
            "stages.0.blocks.0.conv.conv2_kxk\n",
            "stages.0.blocks.0.conv.norm2.drop\n",
            "stages.0.blocks.0.conv.norm2.act\n",
            "stages.0.blocks.0.conv.se.fc1\n",
            "stages.0.blocks.0.conv.se.bn\n",
            "stages.0.blocks.0.conv.se.act\n",
            "stages.0.blocks.0.conv.se.fc2\n",
            "stages.0.blocks.0.conv.se.gate\n",
            "stages.0.blocks.0.conv.conv3_1x1\n",
            "stages.0.blocks.0.conv.drop_path\n",
            "stages.0.blocks.0.attn_block.norm1\n",
            "stages.0.blocks.0.attn_block.attn.qkv\n",
            "stages.0.blocks.0.attn_block.attn.rel_pos\n",
            "stages.0.blocks.0.attn_block.attn.attn_drop\n",
            "stages.0.blocks.0.attn_block.attn.proj\n",
            "stages.0.blocks.0.attn_block.attn.proj_drop\n",
            "stages.0.blocks.0.attn_block.ls1\n",
            "stages.0.blocks.0.attn_block.drop_path1\n",
            "stages.0.blocks.0.attn_block.norm2\n",
            "stages.0.blocks.0.attn_block.mlp.fc1\n",
            "stages.0.blocks.0.attn_block.mlp.act\n",
            "stages.0.blocks.0.attn_block.mlp.drop1\n",
            "stages.0.blocks.0.attn_block.mlp.norm\n",
            "stages.0.blocks.0.attn_block.mlp.fc2\n",
            "stages.0.blocks.0.attn_block.mlp.drop2\n",
            "stages.0.blocks.0.attn_block.ls2\n",
            "stages.0.blocks.0.attn_block.drop_path2\n",
            "stages.0.blocks.0.attn_grid.norm1\n",
            "stages.0.blocks.0.attn_grid.attn.qkv\n",
            "stages.0.blocks.0.attn_grid.attn.rel_pos\n",
            "stages.0.blocks.0.attn_grid.attn.attn_drop\n",
            "stages.0.blocks.0.attn_grid.attn.proj\n",
            "stages.0.blocks.0.attn_grid.attn.proj_drop\n",
            "stages.0.blocks.0.attn_grid.ls1\n",
            "stages.0.blocks.0.attn_grid.drop_path1\n",
            "stages.0.blocks.0.attn_grid.norm2\n",
            "stages.0.blocks.0.attn_grid.mlp.fc1\n",
            "stages.0.blocks.0.attn_grid.mlp.act\n",
            "stages.0.blocks.0.attn_grid.mlp.drop1\n",
            "stages.0.blocks.0.attn_grid.mlp.norm\n",
            "stages.0.blocks.0.attn_grid.mlp.fc2\n",
            "stages.0.blocks.0.attn_grid.mlp.drop2\n",
            "stages.0.blocks.0.attn_grid.ls2\n",
            "stages.0.blocks.0.attn_grid.drop_path2\n",
            "stages.0.blocks.1.conv.shortcut\n",
            "stages.0.blocks.1.conv.pre_norm.drop\n",
            "stages.0.blocks.1.conv.pre_norm.act\n",
            "stages.0.blocks.1.conv.down\n",
            "stages.0.blocks.1.conv.conv1_1x1\n",
            "stages.0.blocks.1.conv.norm1.drop\n",
            "stages.0.blocks.1.conv.norm1.act\n",
            "stages.0.blocks.1.conv.conv2_kxk\n",
            "stages.0.blocks.1.conv.norm2.drop\n",
            "stages.0.blocks.1.conv.norm2.act\n",
            "stages.0.blocks.1.conv.se.fc1\n",
            "stages.0.blocks.1.conv.se.bn\n",
            "stages.0.blocks.1.conv.se.act\n",
            "stages.0.blocks.1.conv.se.fc2\n",
            "stages.0.blocks.1.conv.se.gate\n",
            "stages.0.blocks.1.conv.conv3_1x1\n",
            "stages.0.blocks.1.conv.drop_path\n",
            "stages.0.blocks.1.attn_block.norm1\n",
            "stages.0.blocks.1.attn_block.attn.qkv\n",
            "stages.0.blocks.1.attn_block.attn.rel_pos\n",
            "stages.0.blocks.1.attn_block.attn.attn_drop\n",
            "stages.0.blocks.1.attn_block.attn.proj\n",
            "stages.0.blocks.1.attn_block.attn.proj_drop\n",
            "stages.0.blocks.1.attn_block.ls1\n",
            "stages.0.blocks.1.attn_block.drop_path1\n",
            "stages.0.blocks.1.attn_block.norm2\n",
            "stages.0.blocks.1.attn_block.mlp.fc1\n",
            "stages.0.blocks.1.attn_block.mlp.act\n",
            "stages.0.blocks.1.attn_block.mlp.drop1\n",
            "stages.0.blocks.1.attn_block.mlp.norm\n",
            "stages.0.blocks.1.attn_block.mlp.fc2\n",
            "stages.0.blocks.1.attn_block.mlp.drop2\n",
            "stages.0.blocks.1.attn_block.ls2\n",
            "stages.0.blocks.1.attn_block.drop_path2\n",
            "stages.0.blocks.1.attn_grid.norm1\n",
            "stages.0.blocks.1.attn_grid.attn.qkv\n",
            "stages.0.blocks.1.attn_grid.attn.rel_pos\n",
            "stages.0.blocks.1.attn_grid.attn.attn_drop\n",
            "stages.0.blocks.1.attn_grid.attn.proj\n",
            "stages.0.blocks.1.attn_grid.attn.proj_drop\n",
            "stages.0.blocks.1.attn_grid.ls1\n",
            "stages.0.blocks.1.attn_grid.drop_path1\n",
            "stages.0.blocks.1.attn_grid.norm2\n",
            "stages.0.blocks.1.attn_grid.mlp.fc1\n",
            "stages.0.blocks.1.attn_grid.mlp.act\n",
            "stages.0.blocks.1.attn_grid.mlp.drop1\n",
            "stages.0.blocks.1.attn_grid.mlp.norm\n",
            "stages.0.blocks.1.attn_grid.mlp.fc2\n",
            "stages.0.blocks.1.attn_grid.mlp.drop2\n",
            "stages.0.blocks.1.attn_grid.ls2\n",
            "stages.0.blocks.1.attn_grid.drop_path2\n",
            "stages.1.blocks.0.conv.shortcut.pool\n",
            "stages.1.blocks.0.conv.shortcut.expand\n",
            "stages.1.blocks.0.conv.pre_norm.drop\n",
            "stages.1.blocks.0.conv.pre_norm.act\n",
            "stages.1.blocks.0.conv.down\n",
            "stages.1.blocks.0.conv.conv1_1x1\n",
            "stages.1.blocks.0.conv.norm1.drop\n",
            "stages.1.blocks.0.conv.norm1.act\n",
            "stages.1.blocks.0.conv.conv2_kxk\n",
            "stages.1.blocks.0.conv.norm2.drop\n",
            "stages.1.blocks.0.conv.norm2.act\n",
            "stages.1.blocks.0.conv.se.fc1\n",
            "stages.1.blocks.0.conv.se.bn\n",
            "stages.1.blocks.0.conv.se.act\n",
            "stages.1.blocks.0.conv.se.fc2\n",
            "stages.1.blocks.0.conv.se.gate\n",
            "stages.1.blocks.0.conv.conv3_1x1\n",
            "stages.1.blocks.0.conv.drop_path\n",
            "stages.1.blocks.0.attn_block.norm1\n",
            "stages.1.blocks.0.attn_block.attn.qkv\n",
            "stages.1.blocks.0.attn_block.attn.rel_pos\n",
            "stages.1.blocks.0.attn_block.attn.attn_drop\n",
            "stages.1.blocks.0.attn_block.attn.proj\n",
            "stages.1.blocks.0.attn_block.attn.proj_drop\n",
            "stages.1.blocks.0.attn_block.ls1\n",
            "stages.1.blocks.0.attn_block.drop_path1\n",
            "stages.1.blocks.0.attn_block.norm2\n",
            "stages.1.blocks.0.attn_block.mlp.fc1\n",
            "stages.1.blocks.0.attn_block.mlp.act\n",
            "stages.1.blocks.0.attn_block.mlp.drop1\n",
            "stages.1.blocks.0.attn_block.mlp.norm\n",
            "stages.1.blocks.0.attn_block.mlp.fc2\n",
            "stages.1.blocks.0.attn_block.mlp.drop2\n",
            "stages.1.blocks.0.attn_block.ls2\n",
            "stages.1.blocks.0.attn_block.drop_path2\n",
            "stages.1.blocks.0.attn_grid.norm1\n",
            "stages.1.blocks.0.attn_grid.attn.qkv\n",
            "stages.1.blocks.0.attn_grid.attn.rel_pos\n",
            "stages.1.blocks.0.attn_grid.attn.attn_drop\n",
            "stages.1.blocks.0.attn_grid.attn.proj\n",
            "stages.1.blocks.0.attn_grid.attn.proj_drop\n",
            "stages.1.blocks.0.attn_grid.ls1\n",
            "stages.1.blocks.0.attn_grid.drop_path1\n",
            "stages.1.blocks.0.attn_grid.norm2\n",
            "stages.1.blocks.0.attn_grid.mlp.fc1\n",
            "stages.1.blocks.0.attn_grid.mlp.act\n",
            "stages.1.blocks.0.attn_grid.mlp.drop1\n",
            "stages.1.blocks.0.attn_grid.mlp.norm\n",
            "stages.1.blocks.0.attn_grid.mlp.fc2\n",
            "stages.1.blocks.0.attn_grid.mlp.drop2\n",
            "stages.1.blocks.0.attn_grid.ls2\n",
            "stages.1.blocks.0.attn_grid.drop_path2\n",
            "stages.1.blocks.1.conv.shortcut\n",
            "stages.1.blocks.1.conv.pre_norm.drop\n",
            "stages.1.blocks.1.conv.pre_norm.act\n",
            "stages.1.blocks.1.conv.down\n",
            "stages.1.blocks.1.conv.conv1_1x1\n",
            "stages.1.blocks.1.conv.norm1.drop\n",
            "stages.1.blocks.1.conv.norm1.act\n",
            "stages.1.blocks.1.conv.conv2_kxk\n",
            "stages.1.blocks.1.conv.norm2.drop\n",
            "stages.1.blocks.1.conv.norm2.act\n",
            "stages.1.blocks.1.conv.se.fc1\n",
            "stages.1.blocks.1.conv.se.bn\n",
            "stages.1.blocks.1.conv.se.act\n",
            "stages.1.blocks.1.conv.se.fc2\n",
            "stages.1.blocks.1.conv.se.gate\n",
            "stages.1.blocks.1.conv.conv3_1x1\n",
            "stages.1.blocks.1.conv.drop_path\n",
            "stages.1.blocks.1.attn_block.norm1\n",
            "stages.1.blocks.1.attn_block.attn.qkv\n",
            "stages.1.blocks.1.attn_block.attn.rel_pos\n",
            "stages.1.blocks.1.attn_block.attn.attn_drop\n",
            "stages.1.blocks.1.attn_block.attn.proj\n",
            "stages.1.blocks.1.attn_block.attn.proj_drop\n",
            "stages.1.blocks.1.attn_block.ls1\n",
            "stages.1.blocks.1.attn_block.drop_path1\n",
            "stages.1.blocks.1.attn_block.norm2\n",
            "stages.1.blocks.1.attn_block.mlp.fc1\n",
            "stages.1.blocks.1.attn_block.mlp.act\n",
            "stages.1.blocks.1.attn_block.mlp.drop1\n",
            "stages.1.blocks.1.attn_block.mlp.norm\n",
            "stages.1.blocks.1.attn_block.mlp.fc2\n",
            "stages.1.blocks.1.attn_block.mlp.drop2\n",
            "stages.1.blocks.1.attn_block.ls2\n",
            "stages.1.blocks.1.attn_block.drop_path2\n",
            "stages.1.blocks.1.attn_grid.norm1\n",
            "stages.1.blocks.1.attn_grid.attn.qkv\n",
            "stages.1.blocks.1.attn_grid.attn.rel_pos\n",
            "stages.1.blocks.1.attn_grid.attn.attn_drop\n",
            "stages.1.blocks.1.attn_grid.attn.proj\n",
            "stages.1.blocks.1.attn_grid.attn.proj_drop\n",
            "stages.1.blocks.1.attn_grid.ls1\n",
            "stages.1.blocks.1.attn_grid.drop_path1\n",
            "stages.1.blocks.1.attn_grid.norm2\n",
            "stages.1.blocks.1.attn_grid.mlp.fc1\n",
            "stages.1.blocks.1.attn_grid.mlp.act\n",
            "stages.1.blocks.1.attn_grid.mlp.drop1\n",
            "stages.1.blocks.1.attn_grid.mlp.norm\n",
            "stages.1.blocks.1.attn_grid.mlp.fc2\n",
            "stages.1.blocks.1.attn_grid.mlp.drop2\n",
            "stages.1.blocks.1.attn_grid.ls2\n",
            "stages.1.blocks.1.attn_grid.drop_path2\n",
            "stages.2.blocks.0.conv.shortcut.pool\n",
            "stages.2.blocks.0.conv.shortcut.expand\n",
            "stages.2.blocks.0.conv.pre_norm.drop\n",
            "stages.2.blocks.0.conv.pre_norm.act\n",
            "stages.2.blocks.0.conv.down\n",
            "stages.2.blocks.0.conv.conv1_1x1\n",
            "stages.2.blocks.0.conv.norm1.drop\n",
            "stages.2.blocks.0.conv.norm1.act\n",
            "stages.2.blocks.0.conv.conv2_kxk\n",
            "stages.2.blocks.0.conv.norm2.drop\n",
            "stages.2.blocks.0.conv.norm2.act\n",
            "stages.2.blocks.0.conv.se.fc1\n",
            "stages.2.blocks.0.conv.se.bn\n",
            "stages.2.blocks.0.conv.se.act\n",
            "stages.2.blocks.0.conv.se.fc2\n",
            "stages.2.blocks.0.conv.se.gate\n",
            "stages.2.blocks.0.conv.conv3_1x1\n",
            "stages.2.blocks.0.conv.drop_path\n",
            "stages.2.blocks.0.attn_block.norm1\n",
            "stages.2.blocks.0.attn_block.attn.qkv\n",
            "stages.2.blocks.0.attn_block.attn.rel_pos\n",
            "stages.2.blocks.0.attn_block.attn.attn_drop\n",
            "stages.2.blocks.0.attn_block.attn.proj\n",
            "stages.2.blocks.0.attn_block.attn.proj_drop\n",
            "stages.2.blocks.0.attn_block.ls1\n",
            "stages.2.blocks.0.attn_block.drop_path1\n",
            "stages.2.blocks.0.attn_block.norm2\n",
            "stages.2.blocks.0.attn_block.mlp.fc1\n",
            "stages.2.blocks.0.attn_block.mlp.act\n",
            "stages.2.blocks.0.attn_block.mlp.drop1\n",
            "stages.2.blocks.0.attn_block.mlp.norm\n",
            "stages.2.blocks.0.attn_block.mlp.fc2\n",
            "stages.2.blocks.0.attn_block.mlp.drop2\n",
            "stages.2.blocks.0.attn_block.ls2\n",
            "stages.2.blocks.0.attn_block.drop_path2\n",
            "stages.2.blocks.0.attn_grid.norm1\n",
            "stages.2.blocks.0.attn_grid.attn.qkv\n",
            "stages.2.blocks.0.attn_grid.attn.rel_pos\n",
            "stages.2.blocks.0.attn_grid.attn.attn_drop\n",
            "stages.2.blocks.0.attn_grid.attn.proj\n",
            "stages.2.blocks.0.attn_grid.attn.proj_drop\n",
            "stages.2.blocks.0.attn_grid.ls1\n",
            "stages.2.blocks.0.attn_grid.drop_path1\n",
            "stages.2.blocks.0.attn_grid.norm2\n",
            "stages.2.blocks.0.attn_grid.mlp.fc1\n",
            "stages.2.blocks.0.attn_grid.mlp.act\n",
            "stages.2.blocks.0.attn_grid.mlp.drop1\n",
            "stages.2.blocks.0.attn_grid.mlp.norm\n",
            "stages.2.blocks.0.attn_grid.mlp.fc2\n",
            "stages.2.blocks.0.attn_grid.mlp.drop2\n",
            "stages.2.blocks.0.attn_grid.ls2\n",
            "stages.2.blocks.0.attn_grid.drop_path2\n",
            "stages.2.blocks.1.conv.shortcut\n",
            "stages.2.blocks.1.conv.pre_norm.drop\n",
            "stages.2.blocks.1.conv.pre_norm.act\n",
            "stages.2.blocks.1.conv.down\n",
            "stages.2.blocks.1.conv.conv1_1x1\n",
            "stages.2.blocks.1.conv.norm1.drop\n",
            "stages.2.blocks.1.conv.norm1.act\n",
            "stages.2.blocks.1.conv.conv2_kxk\n",
            "stages.2.blocks.1.conv.norm2.drop\n",
            "stages.2.blocks.1.conv.norm2.act\n",
            "stages.2.blocks.1.conv.se.fc1\n",
            "stages.2.blocks.1.conv.se.bn\n",
            "stages.2.blocks.1.conv.se.act\n",
            "stages.2.blocks.1.conv.se.fc2\n",
            "stages.2.blocks.1.conv.se.gate\n",
            "stages.2.blocks.1.conv.conv3_1x1\n",
            "stages.2.blocks.1.conv.drop_path\n",
            "stages.2.blocks.1.attn_block.norm1\n",
            "stages.2.blocks.1.attn_block.attn.qkv\n",
            "stages.2.blocks.1.attn_block.attn.rel_pos\n",
            "stages.2.blocks.1.attn_block.attn.attn_drop\n",
            "stages.2.blocks.1.attn_block.attn.proj\n",
            "stages.2.blocks.1.attn_block.attn.proj_drop\n",
            "stages.2.blocks.1.attn_block.ls1\n",
            "stages.2.blocks.1.attn_block.drop_path1\n",
            "stages.2.blocks.1.attn_block.norm2\n",
            "stages.2.blocks.1.attn_block.mlp.fc1\n",
            "stages.2.blocks.1.attn_block.mlp.act\n",
            "stages.2.blocks.1.attn_block.mlp.drop1\n",
            "stages.2.blocks.1.attn_block.mlp.norm\n",
            "stages.2.blocks.1.attn_block.mlp.fc2\n",
            "stages.2.blocks.1.attn_block.mlp.drop2\n",
            "stages.2.blocks.1.attn_block.ls2\n",
            "stages.2.blocks.1.attn_block.drop_path2\n",
            "stages.2.blocks.1.attn_grid.norm1\n",
            "stages.2.blocks.1.attn_grid.attn.qkv\n",
            "stages.2.blocks.1.attn_grid.attn.rel_pos\n",
            "stages.2.blocks.1.attn_grid.attn.attn_drop\n",
            "stages.2.blocks.1.attn_grid.attn.proj\n",
            "stages.2.blocks.1.attn_grid.attn.proj_drop\n",
            "stages.2.blocks.1.attn_grid.ls1\n",
            "stages.2.blocks.1.attn_grid.drop_path1\n",
            "stages.2.blocks.1.attn_grid.norm2\n",
            "stages.2.blocks.1.attn_grid.mlp.fc1\n",
            "stages.2.blocks.1.attn_grid.mlp.act\n",
            "stages.2.blocks.1.attn_grid.mlp.drop1\n",
            "stages.2.blocks.1.attn_grid.mlp.norm\n",
            "stages.2.blocks.1.attn_grid.mlp.fc2\n",
            "stages.2.blocks.1.attn_grid.mlp.drop2\n",
            "stages.2.blocks.1.attn_grid.ls2\n",
            "stages.2.blocks.1.attn_grid.drop_path2\n",
            "stages.2.blocks.2.conv.shortcut\n",
            "stages.2.blocks.2.conv.pre_norm.drop\n",
            "stages.2.blocks.2.conv.pre_norm.act\n",
            "stages.2.blocks.2.conv.down\n",
            "stages.2.blocks.2.conv.conv1_1x1\n",
            "stages.2.blocks.2.conv.norm1.drop\n",
            "stages.2.blocks.2.conv.norm1.act\n",
            "stages.2.blocks.2.conv.conv2_kxk\n",
            "stages.2.blocks.2.conv.norm2.drop\n",
            "stages.2.blocks.2.conv.norm2.act\n",
            "stages.2.blocks.2.conv.se.fc1\n",
            "stages.2.blocks.2.conv.se.bn\n",
            "stages.2.blocks.2.conv.se.act\n",
            "stages.2.blocks.2.conv.se.fc2\n",
            "stages.2.blocks.2.conv.se.gate\n",
            "stages.2.blocks.2.conv.conv3_1x1\n",
            "stages.2.blocks.2.conv.drop_path\n",
            "stages.2.blocks.2.attn_block.norm1\n",
            "stages.2.blocks.2.attn_block.attn.qkv\n",
            "stages.2.blocks.2.attn_block.attn.rel_pos\n",
            "stages.2.blocks.2.attn_block.attn.attn_drop\n",
            "stages.2.blocks.2.attn_block.attn.proj\n",
            "stages.2.blocks.2.attn_block.attn.proj_drop\n",
            "stages.2.blocks.2.attn_block.ls1\n",
            "stages.2.blocks.2.attn_block.drop_path1\n",
            "stages.2.blocks.2.attn_block.norm2\n",
            "stages.2.blocks.2.attn_block.mlp.fc1\n",
            "stages.2.blocks.2.attn_block.mlp.act\n",
            "stages.2.blocks.2.attn_block.mlp.drop1\n",
            "stages.2.blocks.2.attn_block.mlp.norm\n",
            "stages.2.blocks.2.attn_block.mlp.fc2\n",
            "stages.2.blocks.2.attn_block.mlp.drop2\n",
            "stages.2.blocks.2.attn_block.ls2\n",
            "stages.2.blocks.2.attn_block.drop_path2\n",
            "stages.2.blocks.2.attn_grid.norm1\n",
            "stages.2.blocks.2.attn_grid.attn.qkv\n",
            "stages.2.blocks.2.attn_grid.attn.rel_pos\n",
            "stages.2.blocks.2.attn_grid.attn.attn_drop\n",
            "stages.2.blocks.2.attn_grid.attn.proj\n",
            "stages.2.blocks.2.attn_grid.attn.proj_drop\n",
            "stages.2.blocks.2.attn_grid.ls1\n",
            "stages.2.blocks.2.attn_grid.drop_path1\n",
            "stages.2.blocks.2.attn_grid.norm2\n",
            "stages.2.blocks.2.attn_grid.mlp.fc1\n",
            "stages.2.blocks.2.attn_grid.mlp.act\n",
            "stages.2.blocks.2.attn_grid.mlp.drop1\n",
            "stages.2.blocks.2.attn_grid.mlp.norm\n",
            "stages.2.blocks.2.attn_grid.mlp.fc2\n",
            "stages.2.blocks.2.attn_grid.mlp.drop2\n",
            "stages.2.blocks.2.attn_grid.ls2\n",
            "stages.2.blocks.2.attn_grid.drop_path2\n",
            "stages.2.blocks.3.conv.shortcut\n",
            "stages.2.blocks.3.conv.pre_norm.drop\n",
            "stages.2.blocks.3.conv.pre_norm.act\n",
            "stages.2.blocks.3.conv.down\n",
            "stages.2.blocks.3.conv.conv1_1x1\n",
            "stages.2.blocks.3.conv.norm1.drop\n",
            "stages.2.blocks.3.conv.norm1.act\n",
            "stages.2.blocks.3.conv.conv2_kxk\n",
            "stages.2.blocks.3.conv.norm2.drop\n",
            "stages.2.blocks.3.conv.norm2.act\n",
            "stages.2.blocks.3.conv.se.fc1\n",
            "stages.2.blocks.3.conv.se.bn\n",
            "stages.2.blocks.3.conv.se.act\n",
            "stages.2.blocks.3.conv.se.fc2\n",
            "stages.2.blocks.3.conv.se.gate\n",
            "stages.2.blocks.3.conv.conv3_1x1\n",
            "stages.2.blocks.3.conv.drop_path\n",
            "stages.2.blocks.3.attn_block.norm1\n",
            "stages.2.blocks.3.attn_block.attn.qkv\n",
            "stages.2.blocks.3.attn_block.attn.rel_pos\n",
            "stages.2.blocks.3.attn_block.attn.attn_drop\n",
            "stages.2.blocks.3.attn_block.attn.proj\n",
            "stages.2.blocks.3.attn_block.attn.proj_drop\n",
            "stages.2.blocks.3.attn_block.ls1\n",
            "stages.2.blocks.3.attn_block.drop_path1\n",
            "stages.2.blocks.3.attn_block.norm2\n",
            "stages.2.blocks.3.attn_block.mlp.fc1\n",
            "stages.2.blocks.3.attn_block.mlp.act\n",
            "stages.2.blocks.3.attn_block.mlp.drop1\n",
            "stages.2.blocks.3.attn_block.mlp.norm\n",
            "stages.2.blocks.3.attn_block.mlp.fc2\n",
            "stages.2.blocks.3.attn_block.mlp.drop2\n",
            "stages.2.blocks.3.attn_block.ls2\n",
            "stages.2.blocks.3.attn_block.drop_path2\n",
            "stages.2.blocks.3.attn_grid.norm1\n",
            "stages.2.blocks.3.attn_grid.attn.qkv\n",
            "stages.2.blocks.3.attn_grid.attn.rel_pos\n",
            "stages.2.blocks.3.attn_grid.attn.attn_drop\n",
            "stages.2.blocks.3.attn_grid.attn.proj\n",
            "stages.2.blocks.3.attn_grid.attn.proj_drop\n",
            "stages.2.blocks.3.attn_grid.ls1\n",
            "stages.2.blocks.3.attn_grid.drop_path1\n",
            "stages.2.blocks.3.attn_grid.norm2\n",
            "stages.2.blocks.3.attn_grid.mlp.fc1\n",
            "stages.2.blocks.3.attn_grid.mlp.act\n",
            "stages.2.blocks.3.attn_grid.mlp.drop1\n",
            "stages.2.blocks.3.attn_grid.mlp.norm\n",
            "stages.2.blocks.3.attn_grid.mlp.fc2\n",
            "stages.2.blocks.3.attn_grid.mlp.drop2\n",
            "stages.2.blocks.3.attn_grid.ls2\n",
            "stages.2.blocks.3.attn_grid.drop_path2\n",
            "stages.2.blocks.4.conv.shortcut\n",
            "stages.2.blocks.4.conv.pre_norm.drop\n",
            "stages.2.blocks.4.conv.pre_norm.act\n",
            "stages.2.blocks.4.conv.down\n",
            "stages.2.blocks.4.conv.conv1_1x1\n",
            "stages.2.blocks.4.conv.norm1.drop\n",
            "stages.2.blocks.4.conv.norm1.act\n",
            "stages.2.blocks.4.conv.conv2_kxk\n",
            "stages.2.blocks.4.conv.norm2.drop\n",
            "stages.2.blocks.4.conv.norm2.act\n",
            "stages.2.blocks.4.conv.se.fc1\n",
            "stages.2.blocks.4.conv.se.bn\n",
            "stages.2.blocks.4.conv.se.act\n",
            "stages.2.blocks.4.conv.se.fc2\n",
            "stages.2.blocks.4.conv.se.gate\n",
            "stages.2.blocks.4.conv.conv3_1x1\n",
            "stages.2.blocks.4.conv.drop_path\n",
            "stages.2.blocks.4.attn_block.norm1\n",
            "stages.2.blocks.4.attn_block.attn.qkv\n",
            "stages.2.blocks.4.attn_block.attn.rel_pos\n",
            "stages.2.blocks.4.attn_block.attn.attn_drop\n",
            "stages.2.blocks.4.attn_block.attn.proj\n",
            "stages.2.blocks.4.attn_block.attn.proj_drop\n",
            "stages.2.blocks.4.attn_block.ls1\n",
            "stages.2.blocks.4.attn_block.drop_path1\n",
            "stages.2.blocks.4.attn_block.norm2\n",
            "stages.2.blocks.4.attn_block.mlp.fc1\n",
            "stages.2.blocks.4.attn_block.mlp.act\n",
            "stages.2.blocks.4.attn_block.mlp.drop1\n",
            "stages.2.blocks.4.attn_block.mlp.norm\n",
            "stages.2.blocks.4.attn_block.mlp.fc2\n",
            "stages.2.blocks.4.attn_block.mlp.drop2\n",
            "stages.2.blocks.4.attn_block.ls2\n",
            "stages.2.blocks.4.attn_block.drop_path2\n",
            "stages.2.blocks.4.attn_grid.norm1\n",
            "stages.2.blocks.4.attn_grid.attn.qkv\n",
            "stages.2.blocks.4.attn_grid.attn.rel_pos\n",
            "stages.2.blocks.4.attn_grid.attn.attn_drop\n",
            "stages.2.blocks.4.attn_grid.attn.proj\n",
            "stages.2.blocks.4.attn_grid.attn.proj_drop\n",
            "stages.2.blocks.4.attn_grid.ls1\n",
            "stages.2.blocks.4.attn_grid.drop_path1\n",
            "stages.2.blocks.4.attn_grid.norm2\n",
            "stages.2.blocks.4.attn_grid.mlp.fc1\n",
            "stages.2.blocks.4.attn_grid.mlp.act\n",
            "stages.2.blocks.4.attn_grid.mlp.drop1\n",
            "stages.2.blocks.4.attn_grid.mlp.norm\n",
            "stages.2.blocks.4.attn_grid.mlp.fc2\n",
            "stages.2.blocks.4.attn_grid.mlp.drop2\n",
            "stages.2.blocks.4.attn_grid.ls2\n",
            "stages.2.blocks.4.attn_grid.drop_path2\n",
            "stages.3.blocks.0.conv.shortcut.pool\n",
            "stages.3.blocks.0.conv.shortcut.expand\n",
            "stages.3.blocks.0.conv.pre_norm.drop\n",
            "stages.3.blocks.0.conv.pre_norm.act\n",
            "stages.3.blocks.0.conv.down\n",
            "stages.3.blocks.0.conv.conv1_1x1\n",
            "stages.3.blocks.0.conv.norm1.drop\n",
            "stages.3.blocks.0.conv.norm1.act\n",
            "stages.3.blocks.0.conv.conv2_kxk\n",
            "stages.3.blocks.0.conv.norm2.drop\n",
            "stages.3.blocks.0.conv.norm2.act\n",
            "stages.3.blocks.0.conv.se.fc1\n",
            "stages.3.blocks.0.conv.se.bn\n",
            "stages.3.blocks.0.conv.se.act\n",
            "stages.3.blocks.0.conv.se.fc2\n",
            "stages.3.blocks.0.conv.se.gate\n",
            "stages.3.blocks.0.conv.conv3_1x1\n",
            "stages.3.blocks.0.conv.drop_path\n",
            "stages.3.blocks.0.attn_block.norm1\n",
            "stages.3.blocks.0.attn_block.attn.qkv\n",
            "stages.3.blocks.0.attn_block.attn.rel_pos\n",
            "stages.3.blocks.0.attn_block.attn.attn_drop\n",
            "stages.3.blocks.0.attn_block.attn.proj\n",
            "stages.3.blocks.0.attn_block.attn.proj_drop\n",
            "stages.3.blocks.0.attn_block.ls1\n",
            "stages.3.blocks.0.attn_block.drop_path1\n",
            "stages.3.blocks.0.attn_block.norm2\n",
            "stages.3.blocks.0.attn_block.mlp.fc1\n",
            "stages.3.blocks.0.attn_block.mlp.act\n",
            "stages.3.blocks.0.attn_block.mlp.drop1\n",
            "stages.3.blocks.0.attn_block.mlp.norm\n",
            "stages.3.blocks.0.attn_block.mlp.fc2\n",
            "stages.3.blocks.0.attn_block.mlp.drop2\n",
            "stages.3.blocks.0.attn_block.ls2\n",
            "stages.3.blocks.0.attn_block.drop_path2\n",
            "stages.3.blocks.0.attn_grid.norm1\n",
            "stages.3.blocks.0.attn_grid.attn.qkv\n",
            "stages.3.blocks.0.attn_grid.attn.rel_pos\n",
            "stages.3.blocks.0.attn_grid.attn.attn_drop\n",
            "stages.3.blocks.0.attn_grid.attn.proj\n",
            "stages.3.blocks.0.attn_grid.attn.proj_drop\n",
            "stages.3.blocks.0.attn_grid.ls1\n",
            "stages.3.blocks.0.attn_grid.drop_path1\n",
            "stages.3.blocks.0.attn_grid.norm2\n",
            "stages.3.blocks.0.attn_grid.mlp.fc1\n",
            "stages.3.blocks.0.attn_grid.mlp.act\n",
            "stages.3.blocks.0.attn_grid.mlp.drop1\n",
            "stages.3.blocks.0.attn_grid.mlp.norm\n",
            "stages.3.blocks.0.attn_grid.mlp.fc2\n",
            "stages.3.blocks.0.attn_grid.mlp.drop2\n",
            "stages.3.blocks.0.attn_grid.ls2\n",
            "stages.3.blocks.0.attn_grid.drop_path2\n",
            "stages.3.blocks.1.conv.shortcut\n",
            "stages.3.blocks.1.conv.pre_norm.drop\n",
            "stages.3.blocks.1.conv.pre_norm.act\n",
            "stages.3.blocks.1.conv.down\n",
            "stages.3.blocks.1.conv.conv1_1x1\n",
            "stages.3.blocks.1.conv.norm1.drop\n",
            "stages.3.blocks.1.conv.norm1.act\n",
            "stages.3.blocks.1.conv.conv2_kxk\n",
            "stages.3.blocks.1.conv.norm2.drop\n",
            "stages.3.blocks.1.conv.norm2.act\n",
            "stages.3.blocks.1.conv.se.fc1\n",
            "stages.3.blocks.1.conv.se.bn\n",
            "stages.3.blocks.1.conv.se.act\n",
            "stages.3.blocks.1.conv.se.fc2\n",
            "stages.3.blocks.1.conv.se.gate\n",
            "stages.3.blocks.1.conv.conv3_1x1\n",
            "stages.3.blocks.1.conv.drop_path\n",
            "stages.3.blocks.1.attn_block.norm1\n",
            "stages.3.blocks.1.attn_block.attn.qkv\n",
            "stages.3.blocks.1.attn_block.attn.rel_pos\n",
            "stages.3.blocks.1.attn_block.attn.attn_drop\n",
            "stages.3.blocks.1.attn_block.attn.proj\n",
            "stages.3.blocks.1.attn_block.attn.proj_drop\n",
            "stages.3.blocks.1.attn_block.ls1\n",
            "stages.3.blocks.1.attn_block.drop_path1\n",
            "stages.3.blocks.1.attn_block.norm2\n",
            "stages.3.blocks.1.attn_block.mlp.fc1\n",
            "stages.3.blocks.1.attn_block.mlp.act\n",
            "stages.3.blocks.1.attn_block.mlp.drop1\n",
            "stages.3.blocks.1.attn_block.mlp.norm\n",
            "stages.3.blocks.1.attn_block.mlp.fc2\n",
            "stages.3.blocks.1.attn_block.mlp.drop2\n",
            "stages.3.blocks.1.attn_block.ls2\n",
            "stages.3.blocks.1.attn_block.drop_path2\n",
            "stages.3.blocks.1.attn_grid.norm1\n",
            "stages.3.blocks.1.attn_grid.attn.qkv\n",
            "stages.3.blocks.1.attn_grid.attn.rel_pos\n",
            "stages.3.blocks.1.attn_grid.attn.attn_drop\n",
            "stages.3.blocks.1.attn_grid.attn.proj\n",
            "stages.3.blocks.1.attn_grid.attn.proj_drop\n",
            "stages.3.blocks.1.attn_grid.ls1\n",
            "stages.3.blocks.1.attn_grid.drop_path1\n",
            "stages.3.blocks.1.attn_grid.norm2\n",
            "stages.3.blocks.1.attn_grid.mlp.fc1\n",
            "stages.3.blocks.1.attn_grid.mlp.act\n",
            "stages.3.blocks.1.attn_grid.mlp.drop1\n",
            "stages.3.blocks.1.attn_grid.mlp.norm\n",
            "stages.3.blocks.1.attn_grid.mlp.fc2\n",
            "stages.3.blocks.1.attn_grid.mlp.drop2\n",
            "stages.3.blocks.1.attn_grid.ls2\n",
            "stages.3.blocks.1.attn_grid.drop_path2\n",
            "norm\n",
            "head.global_pool.pool\n",
            "head.global_pool.flatten\n",
            "head.norm\n",
            "head.flatten\n",
            "head.pre_logits.fc\n",
            "head.pre_logits.act\n",
            "head.drop\n",
            "head.fc\n"
          ]
        }
      ]
    }
  ]
}