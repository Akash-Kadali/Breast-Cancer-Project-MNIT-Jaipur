{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsGEVLyzK1t3",
        "outputId": "617ceff6-17e8-44f0-da20-1ca2075f551c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
        "import timm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "import timm\n",
        "\n",
        "model = timm.create_model(\"hf_hub:timm/maxvit_tiny_tf_224.in1k\", pretrained=True)\n",
        "\n",
        "model.to(device)  # Move model to GPU\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SSA(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SSA, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, channels, height, width = inputs.size()\n",
        "        q = k = v = self.conv(inputs.permute(0, 3, 1, 2))\n",
        "        Qshape = q.size()\n",
        "        Kshape = k.size()\n",
        "        Vshape = v.size()\n",
        "        a = Qshape[2] * Qshape[3]\n",
        "        q = q.view(batch_size, Qshape[1], a)\n",
        "        k = k.view(batch_size, Kshape[1], a).permute(0, 2, 1)\n",
        "        qk = torch.matmul(q, k)\n",
        "        qk = F.softmax(qk, dim=-1)\n",
        "        v = v.view(batch_size, Vshape[1], a)\n",
        "        qkv = torch.matmul(qk, v)\n",
        "        qkv = qkv.view(batch_size, Vshape[1], Vshape[2], Vshape[3])\n",
        "        qkv = qkv.permute(0, 1, 2, 3)\n",
        "        qkv = self.conv(qkv)\n",
        "        qkv = qkv.permute(0,3,2,1)\n",
        "        return qkv\n",
        "\n",
        "# Example usage:\n",
        "inputs = torch.randn(1, 14, 14, 128)  # Assuming batch size of 1\n",
        "ssa = SSA(in_channels=128, out_channels=128)\n",
        "print(\"Input Shape\", inputs.shape)\n",
        "output = ssa(inputs)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "jC6JwVZQLI4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6824ce28-3579-4bb9-cf85-d6748a424534"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape torch.Size([1, 14, 14, 128])\n",
            "Output shape: torch.Size([1, 14, 14, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CDSA(nn.Module):\n",
        "    def __init__(self, fltr, nh):\n",
        "        super(CDSA, self).__init__()\n",
        "        self.fltr = fltr\n",
        "        self.nh = nh\n",
        "        self.attn = nn.ModuleList([SSA(fltr // nh, fltr // nh) for _ in range(nh)])  # Self-Attention modules\n",
        "        self.conv = nn.Conv2d(fltr, fltr, kernel_size=1, stride=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        attn = []\n",
        "        feature_split = torch.chunk(input_tensor, self.nh, dim=3)  # Split input tensor along channel dimension\n",
        "        shape = feature_split[0].shape\n",
        "\n",
        "        # Apply self-attention to each split of the input tensor\n",
        "        x = self.attn[0](feature_split[0])\n",
        "        attn.append(x)\n",
        "\n",
        "        for i in range(1, self.nh):\n",
        "            x = feature_split[i] + x  # Residual connection\n",
        "            x = self.attn[i](x)  # Apply self-attention\n",
        "            attn.append(x)\n",
        "\n",
        "        # Concatenate the outputs of self-attention along the channel dimension\n",
        "        mh_lka_attn = torch.cat(attn, dim=3)\n",
        "\n",
        "        # Apply 1x1 convolution followed by ReLU activation\n",
        "        mh_lka_attn = mh_lka_attn.permute(0,3,2,1)\n",
        "        mh_lka_attn = self.conv(mh_lka_attn)\n",
        "        mh_lka_attn = self.relu(mh_lka_attn)\n",
        "        mh_lka_attn = mh_lka_attn.permute(0,3,2,1)\n",
        "        return mh_lka_attn"
      ],
      "metadata": {
        "id": "OLr1AQ_XLejT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchsummary import summary\n",
        "\n",
        "# Define the CAL class\n",
        "class CAL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CAL, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        fltr = 256\n",
        "        nh = 2\n",
        "        rs1 = x = torch.add(x, x)\n",
        "        layer_norm1 = nn.LayerNorm(x.size()[1:], eps=1e-6).to(x.device)  # Move to the same device as x\n",
        "        x = layer_norm1(x)\n",
        "\n",
        "        # Assuming CDSA is defined elsewhere\n",
        "        cdsa = CDSA(fltr, nh).to(x.device)  # Move to the same device as x\n",
        "        cdsa_output = cdsa(x)\n",
        "        x = cdsa_output\n",
        "\n",
        "        rs2 = x = rs1 + x\n",
        "        layer_norm2 = nn.LayerNorm(x.size()[1:], eps=1e-6).to(x.device)  # Move to the same device as x\n",
        "        x = layer_norm2(x)\n",
        "\n",
        "        x = x.permute(0, 3, 2, 1)\n",
        "        conv = nn.Conv2d(fltr, fltr, kernel_size=1, padding=0).to(x.device)  # Move to the same device as x\n",
        "        x = conv(x)\n",
        "        x = x.permute(0, 3, 2, 1)\n",
        "\n",
        "        output = rs2 + x\n",
        "        return output"
      ],
      "metadata": {
        "id": "yXGr0Wj8bf6O"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun= 'categorical_crossentropy'\n",
        "gpu_num=1\n",
        "k=5\n",
        "lr1=0.005\n",
        "lr2=0.0001\n",
        "image_size=224\n",
        "classes=8\n",
        "ratio=8\n",
        "fltr=256\n",
        "nh=2  # number of splits\n",
        "mag='40'"
      ],
      "metadata": {
        "id": "7t5UYdkyOLxd"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define the MyModel class\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes, fltr=256, nh=2):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.model1 = timm.create_model(\"hf_hub:timm/maxvit_tiny_tf_224.in1k\", pretrained=True).to(device)\n",
        "\n",
        "        self.activation = {}\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activation[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        layer_name = 'stages.2.blocks.4.attn_grid.drop_path2'\n",
        "        desired_layer = self.model1.stages[2].blocks[4].attn_grid.drop_path2\n",
        "        desired_layer.register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "        self.conv = nn.Conv2d(256, fltr, 1).to(device)\n",
        "        self.CAL_out = CAL().to(device)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1)).to(device)\n",
        "        self.fc = nn.Linear(fltr, num_classes).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model1(x)\n",
        "        activation_output = self.activation['stages.2.blocks.4.attn_grid.drop_path2']\n",
        "        print(activation_output.shape)\n",
        "        activation_output = activation_output.permute(0, 3, 2, 1)\n",
        "        mn_output = self.conv(activation_output)\n",
        "        mn_output = mn_output.permute(0, 3, 2, 1)\n",
        "        print(mn_output.shape)\n",
        "        mn_output = self.CAL_out(mn_output)\n",
        "        print(mn_output.shape)\n",
        "        mn_output= mn_output.permute(0, 3, 2, 1)\n",
        "        # Apply adaptive average pooling\n",
        "        adaptive_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        mn_output = adaptive_pool(mn_output)\n",
        "        print(mn_output.shape)\n",
        "        output = mn_output.view(m.size(0), -1)\n",
        "        print(\"final\",output.shape)\n",
        "        return output\n",
        "\n",
        "# Instantiate MyModel\n",
        "classes = 8\n",
        "model = MyModel(num_classes=classes)\n",
        "\n",
        "# Move the model1 to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Print model1 summary\n",
        "summary(model, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HxM--P0NhM4",
        "outputId": "f2900854-8678-4254-d45c-fb43c32674d7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "torch.Size([2, 14, 14, 256])\n",
            "torch.Size([2, 14, 14, 256])\n",
            "torch.Size([2, 14, 14, 256])\n",
            "torch.Size([2, 256, 1, 1])\n",
            "final torch.Size([2, 256])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "        Conv2dSame-1         [-1, 64, 112, 112]           1,792\n",
            "          Identity-2         [-1, 64, 112, 112]               0\n",
            "          GELUTanh-3         [-1, 64, 112, 112]               0\n",
            "    BatchNormAct2d-4         [-1, 64, 112, 112]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,928\n",
            "              Stem-6         [-1, 64, 112, 112]               0\n",
            "     AvgPool2dSame-7           [-1, 64, 56, 56]               0\n",
            "          Identity-8           [-1, 64, 56, 56]               0\n",
            "      Downsample2d-9           [-1, 64, 56, 56]               0\n",
            "         Identity-10         [-1, 64, 112, 112]               0\n",
            "         Identity-11         [-1, 64, 112, 112]               0\n",
            "   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n",
            "         Identity-13         [-1, 64, 112, 112]               0\n",
            "           Conv2d-14        [-1, 256, 112, 112]          16,384\n",
            "         Identity-15        [-1, 256, 112, 112]               0\n",
            "         GELUTanh-16        [-1, 256, 112, 112]               0\n",
            "   BatchNormAct2d-17        [-1, 256, 112, 112]             512\n",
            "       Conv2dSame-18          [-1, 256, 56, 56]           2,304\n",
            "         Identity-19          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-20          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-21          [-1, 256, 56, 56]             512\n",
            "           Conv2d-22             [-1, 16, 1, 1]           4,112\n",
            "         Identity-23             [-1, 16, 1, 1]               0\n",
            "             SiLU-24             [-1, 16, 1, 1]               0\n",
            "           Conv2d-25            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-26            [-1, 256, 1, 1]               0\n",
            "         SEModule-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28           [-1, 64, 56, 56]          16,448\n",
            "         Identity-29           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-30           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-31           [-1, 56, 56, 64]             128\n",
            "           Linear-32            [-1, 7, 7, 192]          12,480\n",
            "           Linear-33             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-34             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-35             [-1, 7, 7, 64]               0\n",
            "         Identity-36           [-1, 56, 56, 64]               0\n",
            "         Identity-37           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-38           [-1, 56, 56, 64]             128\n",
            "           Linear-39          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-40          [-1, 56, 56, 256]               0\n",
            "          Dropout-41          [-1, 56, 56, 256]               0\n",
            "         Identity-42          [-1, 56, 56, 256]               0\n",
            "           Linear-43           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-44           [-1, 56, 56, 64]               0\n",
            "              Mlp-45           [-1, 56, 56, 64]               0\n",
            "         Identity-46           [-1, 56, 56, 64]               0\n",
            "         Identity-47           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-48           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 64]             128\n",
            "           Linear-50            [-1, 7, 7, 192]          12,480\n",
            "           Linear-51             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-52             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-53             [-1, 7, 7, 64]               0\n",
            "         Identity-54           [-1, 56, 56, 64]               0\n",
            "         Identity-55           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-56           [-1, 56, 56, 64]             128\n",
            "           Linear-57          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-58          [-1, 56, 56, 256]               0\n",
            "          Dropout-59          [-1, 56, 56, 256]               0\n",
            "         Identity-60          [-1, 56, 56, 256]               0\n",
            "           Linear-61           [-1, 56, 56, 64]          16,448\n",
            "          Dropout-62           [-1, 56, 56, 64]               0\n",
            "              Mlp-63           [-1, 56, 56, 64]               0\n",
            "         Identity-64           [-1, 56, 56, 64]               0\n",
            "         Identity-65           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-66           [-1, 56, 56, 64]               0\n",
            "     MaxxVitBlock-67           [-1, 64, 56, 56]               0\n",
            "         Identity-68           [-1, 64, 56, 56]               0\n",
            "         Identity-69           [-1, 64, 56, 56]               0\n",
            "         Identity-70           [-1, 64, 56, 56]               0\n",
            "   BatchNormAct2d-71           [-1, 64, 56, 56]             128\n",
            "         Identity-72           [-1, 64, 56, 56]               0\n",
            "           Conv2d-73          [-1, 256, 56, 56]          16,384\n",
            "         Identity-74          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-75          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-76          [-1, 256, 56, 56]             512\n",
            "           Conv2d-77          [-1, 256, 56, 56]           2,304\n",
            "         Identity-78          [-1, 256, 56, 56]               0\n",
            "         GELUTanh-79          [-1, 256, 56, 56]               0\n",
            "   BatchNormAct2d-80          [-1, 256, 56, 56]             512\n",
            "           Conv2d-81             [-1, 16, 1, 1]           4,112\n",
            "         Identity-82             [-1, 16, 1, 1]               0\n",
            "             SiLU-83             [-1, 16, 1, 1]               0\n",
            "           Conv2d-84            [-1, 256, 1, 1]           4,352\n",
            "          Sigmoid-85            [-1, 256, 1, 1]               0\n",
            "         SEModule-86          [-1, 256, 56, 56]               0\n",
            "           Conv2d-87           [-1, 64, 56, 56]          16,448\n",
            "         Identity-88           [-1, 64, 56, 56]               0\n",
            "      MbConvBlock-89           [-1, 64, 56, 56]               0\n",
            "        LayerNorm-90           [-1, 56, 56, 64]             128\n",
            "           Linear-91            [-1, 7, 7, 192]          12,480\n",
            "           Linear-92             [-1, 7, 7, 64]           4,160\n",
            "          Dropout-93             [-1, 7, 7, 64]               0\n",
            "      AttentionCl-94             [-1, 7, 7, 64]               0\n",
            "         Identity-95           [-1, 56, 56, 64]               0\n",
            "         Identity-96           [-1, 56, 56, 64]               0\n",
            "        LayerNorm-97           [-1, 56, 56, 64]             128\n",
            "           Linear-98          [-1, 56, 56, 256]          16,640\n",
            "         GELUTanh-99          [-1, 56, 56, 256]               0\n",
            "         Dropout-100          [-1, 56, 56, 256]               0\n",
            "        Identity-101          [-1, 56, 56, 256]               0\n",
            "          Linear-102           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-103           [-1, 56, 56, 64]               0\n",
            "             Mlp-104           [-1, 56, 56, 64]               0\n",
            "        Identity-105           [-1, 56, 56, 64]               0\n",
            "        Identity-106           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-107           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-108           [-1, 56, 56, 64]             128\n",
            "          Linear-109            [-1, 7, 7, 192]          12,480\n",
            "          Linear-110             [-1, 7, 7, 64]           4,160\n",
            "         Dropout-111             [-1, 7, 7, 64]               0\n",
            "     AttentionCl-112             [-1, 7, 7, 64]               0\n",
            "        Identity-113           [-1, 56, 56, 64]               0\n",
            "        Identity-114           [-1, 56, 56, 64]               0\n",
            "       LayerNorm-115           [-1, 56, 56, 64]             128\n",
            "          Linear-116          [-1, 56, 56, 256]          16,640\n",
            "        GELUTanh-117          [-1, 56, 56, 256]               0\n",
            "         Dropout-118          [-1, 56, 56, 256]               0\n",
            "        Identity-119          [-1, 56, 56, 256]               0\n",
            "          Linear-120           [-1, 56, 56, 64]          16,448\n",
            "         Dropout-121           [-1, 56, 56, 64]               0\n",
            "             Mlp-122           [-1, 56, 56, 64]               0\n",
            "        Identity-123           [-1, 56, 56, 64]               0\n",
            "        Identity-124           [-1, 56, 56, 64]               0\n",
            "PartitionAttentionCl-125           [-1, 56, 56, 64]               0\n",
            "    MaxxVitBlock-126           [-1, 64, 56, 56]               0\n",
            "    MaxxVitStage-127           [-1, 64, 56, 56]               0\n",
            "   AvgPool2dSame-128           [-1, 64, 28, 28]               0\n",
            "          Conv2d-129          [-1, 128, 28, 28]           8,320\n",
            "    Downsample2d-130          [-1, 128, 28, 28]               0\n",
            "        Identity-131           [-1, 64, 56, 56]               0\n",
            "        Identity-132           [-1, 64, 56, 56]               0\n",
            "  BatchNormAct2d-133           [-1, 64, 56, 56]             128\n",
            "        Identity-134           [-1, 64, 56, 56]               0\n",
            "          Conv2d-135          [-1, 512, 56, 56]          32,768\n",
            "        Identity-136          [-1, 512, 56, 56]               0\n",
            "        GELUTanh-137          [-1, 512, 56, 56]               0\n",
            "  BatchNormAct2d-138          [-1, 512, 56, 56]           1,024\n",
            "      Conv2dSame-139          [-1, 512, 28, 28]           4,608\n",
            "        Identity-140          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-141          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-142          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-143             [-1, 32, 1, 1]          16,416\n",
            "        Identity-144             [-1, 32, 1, 1]               0\n",
            "            SiLU-145             [-1, 32, 1, 1]               0\n",
            "          Conv2d-146            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-147            [-1, 512, 1, 1]               0\n",
            "        SEModule-148          [-1, 512, 28, 28]               0\n",
            "          Conv2d-149          [-1, 128, 28, 28]          65,664\n",
            "        Identity-150          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-151          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-152          [-1, 28, 28, 128]             256\n",
            "          Linear-153            [-1, 7, 7, 384]          49,536\n",
            "          Linear-154            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-155            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-156            [-1, 7, 7, 128]               0\n",
            "        Identity-157          [-1, 28, 28, 128]               0\n",
            "        Identity-158          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-159          [-1, 28, 28, 128]             256\n",
            "          Linear-160          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-161          [-1, 28, 28, 512]               0\n",
            "         Dropout-162          [-1, 28, 28, 512]               0\n",
            "        Identity-163          [-1, 28, 28, 512]               0\n",
            "          Linear-164          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-165          [-1, 28, 28, 128]               0\n",
            "             Mlp-166          [-1, 28, 28, 128]               0\n",
            "        Identity-167          [-1, 28, 28, 128]               0\n",
            "        Identity-168          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-169          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-170          [-1, 28, 28, 128]             256\n",
            "          Linear-171            [-1, 7, 7, 384]          49,536\n",
            "          Linear-172            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-173            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-174            [-1, 7, 7, 128]               0\n",
            "        Identity-175          [-1, 28, 28, 128]               0\n",
            "        Identity-176          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-177          [-1, 28, 28, 128]             256\n",
            "          Linear-178          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-179          [-1, 28, 28, 512]               0\n",
            "         Dropout-180          [-1, 28, 28, 512]               0\n",
            "        Identity-181          [-1, 28, 28, 512]               0\n",
            "          Linear-182          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-183          [-1, 28, 28, 128]               0\n",
            "             Mlp-184          [-1, 28, 28, 128]               0\n",
            "        Identity-185          [-1, 28, 28, 128]               0\n",
            "        Identity-186          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-187          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-188          [-1, 128, 28, 28]               0\n",
            "        Identity-189          [-1, 128, 28, 28]               0\n",
            "        Identity-190          [-1, 128, 28, 28]               0\n",
            "        Identity-191          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-192          [-1, 128, 28, 28]             256\n",
            "        Identity-193          [-1, 128, 28, 28]               0\n",
            "          Conv2d-194          [-1, 512, 28, 28]          65,536\n",
            "        Identity-195          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-196          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-197          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-198          [-1, 512, 28, 28]           4,608\n",
            "        Identity-199          [-1, 512, 28, 28]               0\n",
            "        GELUTanh-200          [-1, 512, 28, 28]               0\n",
            "  BatchNormAct2d-201          [-1, 512, 28, 28]           1,024\n",
            "          Conv2d-202             [-1, 32, 1, 1]          16,416\n",
            "        Identity-203             [-1, 32, 1, 1]               0\n",
            "            SiLU-204             [-1, 32, 1, 1]               0\n",
            "          Conv2d-205            [-1, 512, 1, 1]          16,896\n",
            "         Sigmoid-206            [-1, 512, 1, 1]               0\n",
            "        SEModule-207          [-1, 512, 28, 28]               0\n",
            "          Conv2d-208          [-1, 128, 28, 28]          65,664\n",
            "        Identity-209          [-1, 128, 28, 28]               0\n",
            "     MbConvBlock-210          [-1, 128, 28, 28]               0\n",
            "       LayerNorm-211          [-1, 28, 28, 128]             256\n",
            "          Linear-212            [-1, 7, 7, 384]          49,536\n",
            "          Linear-213            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-214            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-215            [-1, 7, 7, 128]               0\n",
            "        Identity-216          [-1, 28, 28, 128]               0\n",
            "        Identity-217          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-218          [-1, 28, 28, 128]             256\n",
            "          Linear-219          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-220          [-1, 28, 28, 512]               0\n",
            "         Dropout-221          [-1, 28, 28, 512]               0\n",
            "        Identity-222          [-1, 28, 28, 512]               0\n",
            "          Linear-223          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-224          [-1, 28, 28, 128]               0\n",
            "             Mlp-225          [-1, 28, 28, 128]               0\n",
            "        Identity-226          [-1, 28, 28, 128]               0\n",
            "        Identity-227          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-228          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-229          [-1, 28, 28, 128]             256\n",
            "          Linear-230            [-1, 7, 7, 384]          49,536\n",
            "          Linear-231            [-1, 7, 7, 128]          16,512\n",
            "         Dropout-232            [-1, 7, 7, 128]               0\n",
            "     AttentionCl-233            [-1, 7, 7, 128]               0\n",
            "        Identity-234          [-1, 28, 28, 128]               0\n",
            "        Identity-235          [-1, 28, 28, 128]               0\n",
            "       LayerNorm-236          [-1, 28, 28, 128]             256\n",
            "          Linear-237          [-1, 28, 28, 512]          66,048\n",
            "        GELUTanh-238          [-1, 28, 28, 512]               0\n",
            "         Dropout-239          [-1, 28, 28, 512]               0\n",
            "        Identity-240          [-1, 28, 28, 512]               0\n",
            "          Linear-241          [-1, 28, 28, 128]          65,664\n",
            "         Dropout-242          [-1, 28, 28, 128]               0\n",
            "             Mlp-243          [-1, 28, 28, 128]               0\n",
            "        Identity-244          [-1, 28, 28, 128]               0\n",
            "        Identity-245          [-1, 28, 28, 128]               0\n",
            "PartitionAttentionCl-246          [-1, 28, 28, 128]               0\n",
            "    MaxxVitBlock-247          [-1, 128, 28, 28]               0\n",
            "    MaxxVitStage-248          [-1, 128, 28, 28]               0\n",
            "   AvgPool2dSame-249          [-1, 128, 14, 14]               0\n",
            "          Conv2d-250          [-1, 256, 14, 14]          33,024\n",
            "    Downsample2d-251          [-1, 256, 14, 14]               0\n",
            "        Identity-252          [-1, 128, 28, 28]               0\n",
            "        Identity-253          [-1, 128, 28, 28]               0\n",
            "  BatchNormAct2d-254          [-1, 128, 28, 28]             256\n",
            "        Identity-255          [-1, 128, 28, 28]               0\n",
            "          Conv2d-256         [-1, 1024, 28, 28]         131,072\n",
            "        Identity-257         [-1, 1024, 28, 28]               0\n",
            "        GELUTanh-258         [-1, 1024, 28, 28]               0\n",
            "  BatchNormAct2d-259         [-1, 1024, 28, 28]           2,048\n",
            "      Conv2dSame-260         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-261         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-262         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-263         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-264             [-1, 64, 1, 1]          65,600\n",
            "        Identity-265             [-1, 64, 1, 1]               0\n",
            "            SiLU-266             [-1, 64, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-268           [-1, 1024, 1, 1]               0\n",
            "        SEModule-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "        Identity-271          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-272          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-273          [-1, 14, 14, 256]             512\n",
            "          Linear-274            [-1, 7, 7, 768]         197,376\n",
            "          Linear-275            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-276            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-277            [-1, 7, 7, 256]               0\n",
            "        Identity-278          [-1, 14, 14, 256]               0\n",
            "        Identity-279          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-280          [-1, 14, 14, 256]             512\n",
            "          Linear-281         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-282         [-1, 14, 14, 1024]               0\n",
            "         Dropout-283         [-1, 14, 14, 1024]               0\n",
            "        Identity-284         [-1, 14, 14, 1024]               0\n",
            "          Linear-285          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-286          [-1, 14, 14, 256]               0\n",
            "             Mlp-287          [-1, 14, 14, 256]               0\n",
            "        Identity-288          [-1, 14, 14, 256]               0\n",
            "        Identity-289          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-290          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-291          [-1, 14, 14, 256]             512\n",
            "          Linear-292            [-1, 7, 7, 768]         197,376\n",
            "          Linear-293            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-294            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-295            [-1, 7, 7, 256]               0\n",
            "        Identity-296          [-1, 14, 14, 256]               0\n",
            "        Identity-297          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-298          [-1, 14, 14, 256]             512\n",
            "          Linear-299         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-300         [-1, 14, 14, 1024]               0\n",
            "         Dropout-301         [-1, 14, 14, 1024]               0\n",
            "        Identity-302         [-1, 14, 14, 1024]               0\n",
            "          Linear-303          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-304          [-1, 14, 14, 256]               0\n",
            "             Mlp-305          [-1, 14, 14, 256]               0\n",
            "        Identity-306          [-1, 14, 14, 256]               0\n",
            "        Identity-307          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-308          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-309          [-1, 256, 14, 14]               0\n",
            "        Identity-310          [-1, 256, 14, 14]               0\n",
            "        Identity-311          [-1, 256, 14, 14]               0\n",
            "        Identity-312          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-313          [-1, 256, 14, 14]             512\n",
            "        Identity-314          [-1, 256, 14, 14]               0\n",
            "          Conv2d-315         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-316         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-317         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-318         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-319         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-320         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-321         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-322         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-323             [-1, 64, 1, 1]          65,600\n",
            "        Identity-324             [-1, 64, 1, 1]               0\n",
            "            SiLU-325             [-1, 64, 1, 1]               0\n",
            "          Conv2d-326           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-327           [-1, 1024, 1, 1]               0\n",
            "        SEModule-328         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-329          [-1, 256, 14, 14]         262,400\n",
            "        Identity-330          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-331          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-332          [-1, 14, 14, 256]             512\n",
            "          Linear-333            [-1, 7, 7, 768]         197,376\n",
            "          Linear-334            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-335            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-336            [-1, 7, 7, 256]               0\n",
            "        Identity-337          [-1, 14, 14, 256]               0\n",
            "        Identity-338          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-339          [-1, 14, 14, 256]             512\n",
            "          Linear-340         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-341         [-1, 14, 14, 1024]               0\n",
            "         Dropout-342         [-1, 14, 14, 1024]               0\n",
            "        Identity-343         [-1, 14, 14, 1024]               0\n",
            "          Linear-344          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-345          [-1, 14, 14, 256]               0\n",
            "             Mlp-346          [-1, 14, 14, 256]               0\n",
            "        Identity-347          [-1, 14, 14, 256]               0\n",
            "        Identity-348          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-349          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-350          [-1, 14, 14, 256]             512\n",
            "          Linear-351            [-1, 7, 7, 768]         197,376\n",
            "          Linear-352            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-353            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-354            [-1, 7, 7, 256]               0\n",
            "        Identity-355          [-1, 14, 14, 256]               0\n",
            "        Identity-356          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 256]             512\n",
            "          Linear-358         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-359         [-1, 14, 14, 1024]               0\n",
            "         Dropout-360         [-1, 14, 14, 1024]               0\n",
            "        Identity-361         [-1, 14, 14, 1024]               0\n",
            "          Linear-362          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-363          [-1, 14, 14, 256]               0\n",
            "             Mlp-364          [-1, 14, 14, 256]               0\n",
            "        Identity-365          [-1, 14, 14, 256]               0\n",
            "        Identity-366          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-367          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-368          [-1, 256, 14, 14]               0\n",
            "        Identity-369          [-1, 256, 14, 14]               0\n",
            "        Identity-370          [-1, 256, 14, 14]               0\n",
            "        Identity-371          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-372          [-1, 256, 14, 14]             512\n",
            "        Identity-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-375         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-376         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-377         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-378         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-379         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-380         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-381         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-382             [-1, 64, 1, 1]          65,600\n",
            "        Identity-383             [-1, 64, 1, 1]               0\n",
            "            SiLU-384             [-1, 64, 1, 1]               0\n",
            "          Conv2d-385           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-386           [-1, 1024, 1, 1]               0\n",
            "        SEModule-387         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-388          [-1, 256, 14, 14]         262,400\n",
            "        Identity-389          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-390          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-391          [-1, 14, 14, 256]             512\n",
            "          Linear-392            [-1, 7, 7, 768]         197,376\n",
            "          Linear-393            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-394            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-395            [-1, 7, 7, 256]               0\n",
            "        Identity-396          [-1, 14, 14, 256]               0\n",
            "        Identity-397          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-398          [-1, 14, 14, 256]             512\n",
            "          Linear-399         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-400         [-1, 14, 14, 1024]               0\n",
            "         Dropout-401         [-1, 14, 14, 1024]               0\n",
            "        Identity-402         [-1, 14, 14, 1024]               0\n",
            "          Linear-403          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-404          [-1, 14, 14, 256]               0\n",
            "             Mlp-405          [-1, 14, 14, 256]               0\n",
            "        Identity-406          [-1, 14, 14, 256]               0\n",
            "        Identity-407          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-408          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-409          [-1, 14, 14, 256]             512\n",
            "          Linear-410            [-1, 7, 7, 768]         197,376\n",
            "          Linear-411            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-412            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-413            [-1, 7, 7, 256]               0\n",
            "        Identity-414          [-1, 14, 14, 256]               0\n",
            "        Identity-415          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-416          [-1, 14, 14, 256]             512\n",
            "          Linear-417         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-418         [-1, 14, 14, 1024]               0\n",
            "         Dropout-419         [-1, 14, 14, 1024]               0\n",
            "        Identity-420         [-1, 14, 14, 1024]               0\n",
            "          Linear-421          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-422          [-1, 14, 14, 256]               0\n",
            "             Mlp-423          [-1, 14, 14, 256]               0\n",
            "        Identity-424          [-1, 14, 14, 256]               0\n",
            "        Identity-425          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-426          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-427          [-1, 256, 14, 14]               0\n",
            "        Identity-428          [-1, 256, 14, 14]               0\n",
            "        Identity-429          [-1, 256, 14, 14]               0\n",
            "        Identity-430          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-431          [-1, 256, 14, 14]             512\n",
            "        Identity-432          [-1, 256, 14, 14]               0\n",
            "          Conv2d-433         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-434         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-435         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-436         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-437         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-438         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-439         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-440         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-441             [-1, 64, 1, 1]          65,600\n",
            "        Identity-442             [-1, 64, 1, 1]               0\n",
            "            SiLU-443             [-1, 64, 1, 1]               0\n",
            "          Conv2d-444           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-445           [-1, 1024, 1, 1]               0\n",
            "        SEModule-446         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-447          [-1, 256, 14, 14]         262,400\n",
            "        Identity-448          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-449          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-450          [-1, 14, 14, 256]             512\n",
            "          Linear-451            [-1, 7, 7, 768]         197,376\n",
            "          Linear-452            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-453            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-454            [-1, 7, 7, 256]               0\n",
            "        Identity-455          [-1, 14, 14, 256]               0\n",
            "        Identity-456          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-457          [-1, 14, 14, 256]             512\n",
            "          Linear-458         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-459         [-1, 14, 14, 1024]               0\n",
            "         Dropout-460         [-1, 14, 14, 1024]               0\n",
            "        Identity-461         [-1, 14, 14, 1024]               0\n",
            "          Linear-462          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-463          [-1, 14, 14, 256]               0\n",
            "             Mlp-464          [-1, 14, 14, 256]               0\n",
            "        Identity-465          [-1, 14, 14, 256]               0\n",
            "        Identity-466          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-467          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-468          [-1, 14, 14, 256]             512\n",
            "          Linear-469            [-1, 7, 7, 768]         197,376\n",
            "          Linear-470            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-471            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-472            [-1, 7, 7, 256]               0\n",
            "        Identity-473          [-1, 14, 14, 256]               0\n",
            "        Identity-474          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-475          [-1, 14, 14, 256]             512\n",
            "          Linear-476         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-477         [-1, 14, 14, 1024]               0\n",
            "         Dropout-478         [-1, 14, 14, 1024]               0\n",
            "        Identity-479         [-1, 14, 14, 1024]               0\n",
            "          Linear-480          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-481          [-1, 14, 14, 256]               0\n",
            "             Mlp-482          [-1, 14, 14, 256]               0\n",
            "        Identity-483          [-1, 14, 14, 256]               0\n",
            "        Identity-484          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-485          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-486          [-1, 256, 14, 14]               0\n",
            "        Identity-487          [-1, 256, 14, 14]               0\n",
            "        Identity-488          [-1, 256, 14, 14]               0\n",
            "        Identity-489          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-490          [-1, 256, 14, 14]             512\n",
            "        Identity-491          [-1, 256, 14, 14]               0\n",
            "          Conv2d-492         [-1, 1024, 14, 14]         262,144\n",
            "        Identity-493         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-494         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-495         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-496         [-1, 1024, 14, 14]           9,216\n",
            "        Identity-497         [-1, 1024, 14, 14]               0\n",
            "        GELUTanh-498         [-1, 1024, 14, 14]               0\n",
            "  BatchNormAct2d-499         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-500             [-1, 64, 1, 1]          65,600\n",
            "        Identity-501             [-1, 64, 1, 1]               0\n",
            "            SiLU-502             [-1, 64, 1, 1]               0\n",
            "          Conv2d-503           [-1, 1024, 1, 1]          66,560\n",
            "         Sigmoid-504           [-1, 1024, 1, 1]               0\n",
            "        SEModule-505         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-506          [-1, 256, 14, 14]         262,400\n",
            "        Identity-507          [-1, 256, 14, 14]               0\n",
            "     MbConvBlock-508          [-1, 256, 14, 14]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 256]             512\n",
            "          Linear-510            [-1, 7, 7, 768]         197,376\n",
            "          Linear-511            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-512            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-513            [-1, 7, 7, 256]               0\n",
            "        Identity-514          [-1, 14, 14, 256]               0\n",
            "        Identity-515          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-516          [-1, 14, 14, 256]             512\n",
            "          Linear-517         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-518         [-1, 14, 14, 1024]               0\n",
            "         Dropout-519         [-1, 14, 14, 1024]               0\n",
            "        Identity-520         [-1, 14, 14, 1024]               0\n",
            "          Linear-521          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-522          [-1, 14, 14, 256]               0\n",
            "             Mlp-523          [-1, 14, 14, 256]               0\n",
            "        Identity-524          [-1, 14, 14, 256]               0\n",
            "        Identity-525          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-526          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-527          [-1, 14, 14, 256]             512\n",
            "          Linear-528            [-1, 7, 7, 768]         197,376\n",
            "          Linear-529            [-1, 7, 7, 256]          65,792\n",
            "         Dropout-530            [-1, 7, 7, 256]               0\n",
            "     AttentionCl-531            [-1, 7, 7, 256]               0\n",
            "        Identity-532          [-1, 14, 14, 256]               0\n",
            "        Identity-533          [-1, 14, 14, 256]               0\n",
            "       LayerNorm-534          [-1, 14, 14, 256]             512\n",
            "          Linear-535         [-1, 14, 14, 1024]         263,168\n",
            "        GELUTanh-536         [-1, 14, 14, 1024]               0\n",
            "         Dropout-537         [-1, 14, 14, 1024]               0\n",
            "        Identity-538         [-1, 14, 14, 1024]               0\n",
            "          Linear-539          [-1, 14, 14, 256]         262,400\n",
            "         Dropout-540          [-1, 14, 14, 256]               0\n",
            "             Mlp-541          [-1, 14, 14, 256]               0\n",
            "        Identity-542          [-1, 14, 14, 256]               0\n",
            "        Identity-543          [-1, 14, 14, 256]               0\n",
            "PartitionAttentionCl-544          [-1, 14, 14, 256]               0\n",
            "    MaxxVitBlock-545          [-1, 256, 14, 14]               0\n",
            "    MaxxVitStage-546          [-1, 256, 14, 14]               0\n",
            "   AvgPool2dSame-547            [-1, 256, 7, 7]               0\n",
            "          Conv2d-548            [-1, 512, 7, 7]         131,584\n",
            "    Downsample2d-549            [-1, 512, 7, 7]               0\n",
            "        Identity-550          [-1, 256, 14, 14]               0\n",
            "        Identity-551          [-1, 256, 14, 14]               0\n",
            "  BatchNormAct2d-552          [-1, 256, 14, 14]             512\n",
            "        Identity-553          [-1, 256, 14, 14]               0\n",
            "          Conv2d-554         [-1, 2048, 14, 14]         524,288\n",
            "        Identity-555         [-1, 2048, 14, 14]               0\n",
            "        GELUTanh-556         [-1, 2048, 14, 14]               0\n",
            "  BatchNormAct2d-557         [-1, 2048, 14, 14]           4,096\n",
            "      Conv2dSame-558           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-559           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-560           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-561           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-562            [-1, 128, 1, 1]         262,272\n",
            "        Identity-563            [-1, 128, 1, 1]               0\n",
            "            SiLU-564            [-1, 128, 1, 1]               0\n",
            "          Conv2d-565           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-566           [-1, 2048, 1, 1]               0\n",
            "        SEModule-567           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-568            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-569            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-570            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-571            [-1, 7, 7, 512]           1,024\n",
            "          Linear-572           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-573            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-574            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-575            [-1, 7, 7, 512]               0\n",
            "        Identity-576            [-1, 7, 7, 512]               0\n",
            "        Identity-577            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-578            [-1, 7, 7, 512]           1,024\n",
            "          Linear-579           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-580           [-1, 7, 7, 2048]               0\n",
            "         Dropout-581           [-1, 7, 7, 2048]               0\n",
            "        Identity-582           [-1, 7, 7, 2048]               0\n",
            "          Linear-583            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-584            [-1, 7, 7, 512]               0\n",
            "             Mlp-585            [-1, 7, 7, 512]               0\n",
            "        Identity-586            [-1, 7, 7, 512]               0\n",
            "        Identity-587            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-588            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-589            [-1, 7, 7, 512]           1,024\n",
            "          Linear-590           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-591            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-592            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-593            [-1, 7, 7, 512]               0\n",
            "        Identity-594            [-1, 7, 7, 512]               0\n",
            "        Identity-595            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-596            [-1, 7, 7, 512]           1,024\n",
            "          Linear-597           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-598           [-1, 7, 7, 2048]               0\n",
            "         Dropout-599           [-1, 7, 7, 2048]               0\n",
            "        Identity-600           [-1, 7, 7, 2048]               0\n",
            "          Linear-601            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-602            [-1, 7, 7, 512]               0\n",
            "             Mlp-603            [-1, 7, 7, 512]               0\n",
            "        Identity-604            [-1, 7, 7, 512]               0\n",
            "        Identity-605            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-606            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-607            [-1, 512, 7, 7]               0\n",
            "        Identity-608            [-1, 512, 7, 7]               0\n",
            "        Identity-609            [-1, 512, 7, 7]               0\n",
            "        Identity-610            [-1, 512, 7, 7]               0\n",
            "  BatchNormAct2d-611            [-1, 512, 7, 7]           1,024\n",
            "        Identity-612            [-1, 512, 7, 7]               0\n",
            "          Conv2d-613           [-1, 2048, 7, 7]       1,048,576\n",
            "        Identity-614           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-615           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-616           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-617           [-1, 2048, 7, 7]          18,432\n",
            "        Identity-618           [-1, 2048, 7, 7]               0\n",
            "        GELUTanh-619           [-1, 2048, 7, 7]               0\n",
            "  BatchNormAct2d-620           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-621            [-1, 128, 1, 1]         262,272\n",
            "        Identity-622            [-1, 128, 1, 1]               0\n",
            "            SiLU-623            [-1, 128, 1, 1]               0\n",
            "          Conv2d-624           [-1, 2048, 1, 1]         264,192\n",
            "         Sigmoid-625           [-1, 2048, 1, 1]               0\n",
            "        SEModule-626           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-627            [-1, 512, 7, 7]       1,049,088\n",
            "        Identity-628            [-1, 512, 7, 7]               0\n",
            "     MbConvBlock-629            [-1, 512, 7, 7]               0\n",
            "       LayerNorm-630            [-1, 7, 7, 512]           1,024\n",
            "          Linear-631           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-632            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-633            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-634            [-1, 7, 7, 512]               0\n",
            "        Identity-635            [-1, 7, 7, 512]               0\n",
            "        Identity-636            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-637            [-1, 7, 7, 512]           1,024\n",
            "          Linear-638           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-639           [-1, 7, 7, 2048]               0\n",
            "         Dropout-640           [-1, 7, 7, 2048]               0\n",
            "        Identity-641           [-1, 7, 7, 2048]               0\n",
            "          Linear-642            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-643            [-1, 7, 7, 512]               0\n",
            "             Mlp-644            [-1, 7, 7, 512]               0\n",
            "        Identity-645            [-1, 7, 7, 512]               0\n",
            "        Identity-646            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-647            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-648            [-1, 7, 7, 512]           1,024\n",
            "          Linear-649           [-1, 7, 7, 1536]         787,968\n",
            "          Linear-650            [-1, 7, 7, 512]         262,656\n",
            "         Dropout-651            [-1, 7, 7, 512]               0\n",
            "     AttentionCl-652            [-1, 7, 7, 512]               0\n",
            "        Identity-653            [-1, 7, 7, 512]               0\n",
            "        Identity-654            [-1, 7, 7, 512]               0\n",
            "       LayerNorm-655            [-1, 7, 7, 512]           1,024\n",
            "          Linear-656           [-1, 7, 7, 2048]       1,050,624\n",
            "        GELUTanh-657           [-1, 7, 7, 2048]               0\n",
            "         Dropout-658           [-1, 7, 7, 2048]               0\n",
            "        Identity-659           [-1, 7, 7, 2048]               0\n",
            "          Linear-660            [-1, 7, 7, 512]       1,049,088\n",
            "         Dropout-661            [-1, 7, 7, 512]               0\n",
            "             Mlp-662            [-1, 7, 7, 512]               0\n",
            "        Identity-663            [-1, 7, 7, 512]               0\n",
            "        Identity-664            [-1, 7, 7, 512]               0\n",
            "PartitionAttentionCl-665            [-1, 7, 7, 512]               0\n",
            "    MaxxVitBlock-666            [-1, 512, 7, 7]               0\n",
            "    MaxxVitStage-667            [-1, 512, 7, 7]               0\n",
            "        Identity-668            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-669            [-1, 512, 1, 1]               0\n",
            "        Identity-670            [-1, 512, 1, 1]               0\n",
            "SelectAdaptivePool2d-671            [-1, 512, 1, 1]               0\n",
            "     LayerNorm2d-672            [-1, 512, 1, 1]           1,024\n",
            "         Flatten-673                  [-1, 512]               0\n",
            "          Linear-674                  [-1, 512]         262,656\n",
            "            Tanh-675                  [-1, 512]               0\n",
            "         Dropout-676                  [-1, 512]               0\n",
            "          Linear-677                 [-1, 1000]         513,000\n",
            "NormMlpClassifierHead-678                 [-1, 1000]               0\n",
            "         MaxxVit-679                 [-1, 1000]               0\n",
            "          Conv2d-680          [-1, 256, 14, 14]          65,792\n",
            "             CAL-681          [-1, 14, 14, 256]               0\n",
            "================================================================\n",
            "Total params: 30,953,928\n",
            "Trainable params: 30,953,928\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 887.88\n",
            "Params size (MB): 118.08\n",
            "Estimated Total Size (MB): 1006.53\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LoOh8RTkSa1o"
      },
      "execution_count": 87,
      "outputs": []
    }
  ]
}