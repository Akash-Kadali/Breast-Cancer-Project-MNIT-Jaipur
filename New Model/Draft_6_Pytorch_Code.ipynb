{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "class Conv2dSamePadding(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Conv2dSamePadding, self).__init__(*args, **kwargs)\n",
        "        self.zero_pad_2d = nn.ZeroPad2d(functools.reduce(operator.__add__,\n",
        "                  [(k // 2, k // 2) for k in self.kernel_size[::-1]]))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(self.zero_pad_2d(input), self.weight, self.bias)\n",
        "\n",
        "class DepthwiseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu'):\n",
        "        super(DepthwiseConv2d, self).__init__()\n",
        "\n",
        "        if padding == 'same':\n",
        "            self.depthwise_conv = Conv2dSamePadding(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                groups=in_channels\n",
        "            )\n",
        "        else:\n",
        "            self.depthwise_conv = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                groups=in_channels\n",
        "            )\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'relu6':\n",
        "            self.activation = nn.ReLU6()\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "in_channels = 32\n",
        "depth_multiplier = 1\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 'same'\n",
        "activation = 'relu'\n",
        "\n",
        "# Create an instance of the DepthwiseConv2d layer\n",
        "depthwise_conv_layer = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "\n",
        "# Example input: batch size of 1, 32 input channels, and 112x112 spatial dimensions\n",
        "input_tensor = torch.randn(1, in_channels, 112, 112)\n",
        "\n",
        "# Apply depthwise convolution\n",
        "output_tensor = depthwise_conv_layer(input_tensor)\n",
        "\n",
        "print(f\"Input shape: {input_tensor.shape}\")\n",
        "print(f\"Output shape: {output_tensor.shape}\")  # Should be (1, 96, 112, 112)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn1h10s73m2L",
        "outputId": "6ea501c6-30f8-440b-8757-794c7df8b128"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 32, 112, 112])\n",
            "Output shape: torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem, self).__init__()\n",
        "        self.conv1 = Conv2dSamePadding(in_channels=3, out_channels=32, kernel_size=3, stride=2)  # Changed stride to 2\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = Conv2dSamePadding(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of Input of stem:\", x.shape)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        print(\"Shape of Output of stem:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(1, 3, 224, 224)  # Batch size 1, 3 channels, 224x224 image\n",
        "model = Stem()\n",
        "output = model(input_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCluBkEe3s6x",
        "outputId": "64cd0c2e-900c-43c4-e0ab-dd02c5bb89df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input of stem: torch.Size([1, 3, 224, 224])\n",
            "Shape of Output of stem: torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LKA(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, depth_multiplier=1):\n",
        "        super(LKA, self).__init__()\n",
        "        self.depthwise1 = DepthwiseConv2d(in_channels, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.depthwise2 = DepthwiseConv2d(in_channels * depth_multiplier, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.conv = Conv2dSamePadding(in_channels * depth_multiplier, fltr, kernel_size=1, stride=1)  # Output channels set to in_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of Input of lka:\", x.shape)\n",
        "        x1 = F.relu(self.bn1(self.depthwise1(x)))\n",
        "        print(\"Shape of x1\", x1.shape)\n",
        "        x2 = F.relu(self.bn2(self.depthwise2(x1)))\n",
        "        print(\"Shape of x2\", x2.shape)\n",
        "        x3 = F.relu(self.conv(x2))\n",
        "        print(\"Shape of x3\", x3.shape)\n",
        "        out = x * x3\n",
        "        print(\"Shape of Output of lka:\", out.shape)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(1, 32, 224, 224)  # Batch size 1, 3 channels, 224x224 image\n",
        "model = LKA(in_channels=32, fltr=32, depth_multiplier=1)\n",
        "output = model(input_tensor)\n",
        "print(f\"Input shape: {input_tensor.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVbZKia938uL",
        "outputId": "6761500b-cd71-44b6-977a-0512237e3ee6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input of lka: torch.Size([1, 32, 224, 224])\n",
            "Shape of x1 torch.Size([1, 32, 224, 224])\n",
            "Shape of x2 torch.Size([1, 32, 224, 224])\n",
            "Shape of x3 torch.Size([1, 32, 224, 224])\n",
            "Shape of Output of lka: torch.Size([1, 32, 224, 224])\n",
            "Input shape: torch.Size([1, 32, 224, 224])\n",
            "Output shape: torch.Size([1, 32, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GAB(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(GAB, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        print(\"Shape of Input of GAB:\", inputs.shape)\n",
        "\n",
        "        # Channel attention mechanism\n",
        "        x = self.avg_pool(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        C_A = x * inputs\n",
        "\n",
        "        # Spatial attention mechanism\n",
        "        x = torch.mean(C_A, dim=1, keepdim=True)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        S_A = x * C_A\n",
        "        print(\"Shape of Output of GAB:\", S_A.shape)\n",
        "\n",
        "        return S_A\n",
        "\n",
        "# Example usage:\n",
        "inputs = torch.randn(1, 32, 112, 112)  # Example input tensor with shape (batch_size, channels, height, width)\n",
        "model = GAB(in_channels=32)\n",
        "output = model(inputs)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-UXgvuK4NLr",
        "outputId": "c33ee127-5041-4c70-d927-de2f57bf0c08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MSConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, strides, dm):\n",
        "        super(MSConv, self).__init__()\n",
        "\n",
        "        self.depth_conv1 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.depth_conv2 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.depth_conv4 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn4 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.depth_conv3 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.depth_conv5 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn5 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.depth_conv6 = DepthwiseConv2d(in_channels, depth_multiplier, kernel_size, stride, padding, activation)\n",
        "        self.bn6 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels * 3, out_channels, kernel_size=1)\n",
        "        self.bn7 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of MSConv:\", input.shape)\n",
        "\n",
        "        x = self.depth_conv1(input)\n",
        "        x = self.bn1(x)\n",
        "        shape = x.shape\n",
        "\n",
        "        y = self.depth_conv2(input)\n",
        "        y = self.bn2(y)\n",
        "        y = self.depth_conv4(y)\n",
        "        y = self.bn4(y)\n",
        "\n",
        "        z = self.depth_conv3(input)\n",
        "        z = self.bn3(z)\n",
        "        z = self.depth_conv5(z)\n",
        "        z = self.bn5(z)\n",
        "        z = self.depth_conv6(z)\n",
        "        z = self.bn6(z)\n",
        "\n",
        "        yr = F.interpolate(y, size=(shape[2], shape[3]), mode='bilinear', align_corners=True)\n",
        "        zr = F.interpolate(z, size=(shape[2], shape[3]), mode='bilinear', align_corners=True)\n",
        "\n",
        "        ot = torch.cat([x, yr, zr], dim=1)\n",
        "        ot = self.conv(ot)\n",
        "        ot = F.relu(ot)\n",
        "        ot = self.bn7(ot)\n",
        "\n",
        "        inr = F.interpolate(input, size=(ot.shape[2], ot.shape[3]), mode='bilinear', align_corners=True)\n",
        "        out = ot + inr\n",
        "\n",
        "        print(\"Shape of Output of MSConv:\", out.shape)\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "inputs = torch.randn(1, 32, 112, 112)  # Example input tensor with shape (batch_size, channels, height, width)\n",
        "model = MSConv(in_channels=32, out_channels=32, strides=1, dm=1)\n",
        "output = model(inputs)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcVsBrcb4SpY",
        "outputId": "a4dffbf0-4dd6-4a85-fa19-69796436910a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of MSConv: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DefConv(nn.Module):\n",
        "    def __init__(self, fltr, strides, dm=1):\n",
        "        super(DefConv, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(fltr)\n",
        "        self.conv1 = nn.Conv2d(fltr, fltr, kernel_size=1, padding=0)\n",
        "        self.msconv = MSConv(fltr, fltr, strides, dm)\n",
        "        self.gab = GAB(fltr)\n",
        "        self.conv2 = nn.Conv2d(fltr, fltr, kernel_size=1, padding=0)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.strides = strides\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of defconv:\", input.shape)\n",
        "        x = self.bn1(input)\n",
        "        x = self.conv1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.msconv(x)\n",
        "        x = self.gab(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        if self.strides == 2:\n",
        "            input = self.pool(input)\n",
        "            input = self.conv2(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "        else:\n",
        "            input = self.conv2(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "\n",
        "        print(\"Shape of output of defconv:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = DefConv(fltr=32, strides=1, dm=1)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkCgwXf-4U99",
        "outputId": "b707c9e9-a22a-428f-9e82-1e5adb0f77ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransEnc(nn.Module):\n",
        "    def __init__(self, fltr, nl, nh):\n",
        "        super(TransEnc, self).__init__()\n",
        "        self.nl = nl\n",
        "        self.norm1 = nn.LayerNorm(fltr)\n",
        "        self.norm2 = nn.LayerNorm(fltr)\n",
        "        self.conv = nn.Conv2d(fltr, fltr, kernel_size=1, padding=0)\n",
        "        self.lka_attn = LKA(fltr, fltr, depth_multiplier=nh)  # Assuming `LKA` is your custom large kernel attention\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of trans_enc:\", input.shape)\n",
        "        x = input\n",
        "        for i in range(self.nl):\n",
        "            x = x.permute(0, 2, 3, 1)  # Change shape from (N, C, H, W) to (N, H, W, C) for LayerNorm\n",
        "            x = self.norm1(x)\n",
        "            x = x.permute(0, 3, 1, 2)  # Change back to (N, C, H, W)\n",
        "            x = self.lka_attn(x)\n",
        "            x = x + input\n",
        "\n",
        "            y = x.permute(0, 2, 3, 1)\n",
        "            y = self.norm2(y)\n",
        "            y = y.permute(0, 3, 1, 2)\n",
        "            y = self.conv(y)\n",
        "            y = F.relu(y)\n",
        "            x = x + y\n",
        "\n",
        "        print(\"Shape of output of trans_enc:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = TransEnc(fltr=32, nl=3, nh=1)  # Adjust the number of layers (nl) and heads (nh) as required\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJR_D2nh5Gd8",
        "outputId": "04291c9b-51c2-46d7-ed99-d60d31d88de6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LKA_Attn(nn.Module):\n",
        "    def __init__(self, fltr, nh):\n",
        "        super(LKA_Attn, self).__init__()\n",
        "        self.nh = nh\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(fltr, fltr, kernel_size=1, padding=0) for _ in range(nh)\n",
        "        ])\n",
        "        self.activations = [F.relu, F.gelu, torch.sigmoid]\n",
        "        self.lka_layers = nn.ModuleList([LKA(fltr, fltr) for _ in range(nh)])\n",
        "        self.final_convs = nn.ModuleList([\n",
        "            nn.Conv2d(fltr, fltr, kernel_size=1, padding=0) for _ in range(nh)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of lka_attn:\", input.shape)\n",
        "        attn = []\n",
        "        y = input\n",
        "        for i in range(self.nh):\n",
        "            x = self.convs[i](input)\n",
        "            x = self.activations[i](x)\n",
        "            x = self.lka_layers[i](x)\n",
        "            x = self.final_convs[i](x)\n",
        "            x = self.activations[i](x)\n",
        "            x = x + y\n",
        "            attn.append(x)\n",
        "\n",
        "        mh_lka_attn = attn[0]\n",
        "        for i in range(1, len(attn)):\n",
        "            mh_lka_attn = mh_lka_attn + attn[i]\n",
        "\n",
        "        print(\"Shape of output of lka_attn:\", mh_lka_attn.shape)\n",
        "        return mh_lka_attn\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = LKA_Attn(fltr=32, nh=3)  # Adjust the number of heads (nh) as required\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpr410M55ItH",
        "outputId": "49fc8bdc-87b4-4238-fa6f-813039568a35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input of lka_attn: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of lka_attn: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DefVitBlock(nn.Module):\n",
        "    def __init__(self, fltr, strides, nl, nh, dm=1):\n",
        "        super(DefVitBlock, self).__init__()\n",
        "        self.defconv = DefConv(fltr, strides, dm)\n",
        "        self.trans_enc = TransEnc(fltr, nl, nh)\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of DefVitBlock:\", input.shape)\n",
        "        x = self.defconv(input)\n",
        "        x = self.trans_enc(x)\n",
        "        print(\"Shape of output of DefVitBlock:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = DefVitBlock(fltr=32, strides=1, nl=1, nh=1)  # Adjust the parameters as required\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tl2W3wm5QjJ",
        "outputId": "244943ce-e56b-436d-db44-806903014812"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DefVit model\n",
        "class DefVit(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(DefVit, self).__init__()\n",
        "        self.stem = Stem()\n",
        "        self.stage1_block1 = DefVitBlock(32, strides=1, nl=2, nh=3)\n",
        "        self.stage1_block2 = DefVitBlock(32, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block1 = DefVitBlock(64, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block2 = DefVitBlock(64, strides=1, nl=2, nh=3)\n",
        "        self.stage3_block1 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block2 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block3 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block4 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block5 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage4_block1 = DefVitBlock(256, strides=1, nl=2, nh=3)\n",
        "        self.stage4_block2 = DefVitBlock(256, strides=1, nl=2, nh=3)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        print('After stage-0', x.shape)\n",
        "\n",
        "        x = self.stage1_block1(x)\n",
        "        x = self.stage1_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-1', x.shape)\n",
        "\n",
        "        x = self.stage2_block1(x)\n",
        "        x = self.stage2_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-2', x.shape)\n",
        "\n",
        "        x = self.stage3_block1(x)\n",
        "        x = self.stage3_block2(x)\n",
        "        x = self.stage3_block3(x)\n",
        "        x = self.stage3_block4(x)\n",
        "        x = self.stage3_block5(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-3', x.shape)\n",
        "\n",
        "        x = self.stage4_block1(x)\n",
        "        x = self.stage4_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-4', x.shape)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "num_classes = 8\n",
        "model = DefVit(input_shape=(3, 224, 224), num_classes=num_classes)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "10G-_hhe8D_q",
        "outputId": "67d0ac3c-3e0c-4ef5-b022-76b5f733958f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input of stem: torch.Size([1, 3, 224, 224])\n",
            "Shape of Output of stem: torch.Size([1, 32, 112, 112])\n",
            "After stage-0 torch.Size([1, 32, 112, 112])\n",
            "Shape of input of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of MSConv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 96, 112, 112])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 288 elements not 96",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6bcfed523477>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefVit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-6bcfed523477>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After stage-0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-ab1fc79f0297>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of input of DefVitBlock:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of output of DefVitBlock:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b55d65ecb31e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Change back to (N, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlka_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-934692aaf862>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of x1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of x2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2510\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 288 elements not 96"
          ]
        }
      ]
    }
  ]
}