{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NUI49EivLap",
        "outputId": "01c510a9-6081-4d6a-c38f-25fb68990f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stage-0 torch.Size([1, 32, 112, 112])\n",
            "After stage-1 torch.Size([1, 32, 56, 56])\n",
            "After stage-2 torch.Size([1, 64, 28, 28])\n",
            "After stage-3 torch.Size([1, 128, 14, 14])\n",
            "After stage-4 torch.Size([1, 128, 14, 14])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "class Conv2dSamePadding(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Conv2dSamePadding, self).__init__(*args, **kwargs)\n",
        "        self.zero_pad_2d = nn.ZeroPad2d(functools.reduce(operator.__add__,\n",
        "                  [(k // 2, k // 2) for k in self.kernel_size[::-1]]))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(self.zero_pad_2d(input), self.weight, self.bias)\n",
        "\n",
        "class DepthwiseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu'):\n",
        "        super(DepthwiseConv2d, self).__init__()\n",
        "\n",
        "        if padding == 'same':\n",
        "            self.depthwise_conv = Conv2dSamePadding(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                groups=in_channels\n",
        "            )\n",
        "        else:\n",
        "            self.depthwise_conv = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                groups=in_channels\n",
        "            )\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'relu6':\n",
        "            self.activation = nn.ReLU6()\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class MSConv(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, strides, dm):\n",
        "        super(MSConv, self).__init__()\n",
        "        self.depthwise1 = DepthwiseConv2d(in_channels, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.depthwise2 = DepthwiseConv2d(in_channels * dm, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.depthwise3 = DepthwiseConv2d(in_channels * dm, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.conv = Conv2dSamePadding(in_channels * dm * 3, fltr, kernel_size=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.depthwise1(input)\n",
        "        x = self.bn1(x)\n",
        "        y = self.depthwise2(x)\n",
        "        y = self.bn2(y)\n",
        "        z = self.depthwise3(y)\n",
        "        z = self.bn3(z)\n",
        "        z = torch.cat([x, y, z], dim=1)\n",
        "        z = self.conv(z)\n",
        "        z = F.relu(z)\n",
        "        return z\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem, self).__init__()\n",
        "        self.conv1 = Conv2dSamePadding(in_channels=3, out_channels=32, kernel_size=3, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = Conv2dSamePadding(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "class LKA(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, depth_multiplier=1):\n",
        "        super(LKA, self).__init__()\n",
        "        self.depthwise1 = DepthwiseConv2d(in_channels, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.depthwise2 = DepthwiseConv2d(in_channels * depth_multiplier, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.conv = Conv2dSamePadding(in_channels * depth_multiplier, fltr, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.bn1(self.depthwise1(x)))\n",
        "        x2 = F.relu(self.bn2(self.depthwise2(x1)))\n",
        "        x3 = F.relu(self.conv(x2))\n",
        "        out = x * x3\n",
        "        return out\n",
        "\n",
        "class GAB(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(GAB, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv1 = Conv2dSamePadding(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv2 = Conv2dSamePadding(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.avg_pool(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        C_A = x * inputs\n",
        "        x = torch.mean(C_A, dim=1, keepdim=True)\n",
        "        x = torch.sigmoid(x)\n",
        "        S_A = x * C_A\n",
        "        return S_A\n",
        "\n",
        "class DefConv(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, strides, dm=1):\n",
        "        super(DefConv, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = Conv2dSamePadding(in_channels, fltr, kernel_size=1)\n",
        "        self.msconv = MSConv(fltr, fltr, strides, dm)\n",
        "        self.gab = GAB(fltr)\n",
        "        self.conv2 = Conv2dSamePadding(fltr, fltr, kernel_size=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.strides = strides\n",
        "        self.conv_residual = Conv2dSamePadding(in_channels, fltr, kernel_size=1)  # Add this line\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.bn1(input)\n",
        "        x = self.conv1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.msconv(x)\n",
        "        x = self.gab(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        if self.strides == 2:\n",
        "            input = self.pool(input)\n",
        "            input = self.conv_residual(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "        else:\n",
        "            input = self.conv_residual(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransEnc(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, nl, nh):\n",
        "        super(TransEnc, self).__init__()\n",
        "        self.nl = nl\n",
        "        self.norm1 = nn.LayerNorm(fltr)\n",
        "        self.norm2 = nn.LayerNorm(fltr)\n",
        "        self.conv = Conv2dSamePadding(fltr, fltr, kernel_size=1)\n",
        "        self.lka_attn = LKA(fltr, fltr)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        for _ in range(self.nl):\n",
        "            residual = x  # Store the input to add it later\n",
        "            x = x.permute(0, 2, 3, 1)  # Change shape to (B, H, W, C) for LayerNorm\n",
        "            x = self.norm1(x)\n",
        "            x = x.permute(0, 3, 1, 2)  # Change back to (B, C, H, W) for Conv2d\n",
        "            x = self.lka_attn(x)\n",
        "            x = x + residual  # Add the residual connection\n",
        "            y = x.permute(0, 2, 3, 1)  # Change shape to (B, H, W, C) for LayerNorm\n",
        "            y = self.norm2(y)\n",
        "            y = y.permute(0, 3, 1, 2)  # Change back to (B, C, H, W) for Conv2d\n",
        "            y = self.conv(y)\n",
        "            y = F.relu(y)\n",
        "            x = x + y  # Add the output of the convolution\n",
        "        return x\n",
        "\n",
        "class LKA_Attn(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, nh):\n",
        "        super(LKA_Attn, self).__init__()\n",
        "        self.nh = nh\n",
        "        self.convs = nn.ModuleList([\n",
        "            Conv2dSamePadding(fltr, fltr, kernel_size=1) for _ in range(nh)\n",
        "        ])\n",
        "        self.activations = [F.relu, F.gelu, torch.sigmoid]\n",
        "        self.lka_layers = nn.ModuleList([LKA(fltr, fltr) for _ in range(nh)])\n",
        "        self.final_convs = nn.ModuleList([\n",
        "            Conv2dSamePadding(fltr, fltr, kernel_size=1) for _ in range(nh)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        attn = []\n",
        "        y = input\n",
        "        for i in range(self.nh):\n",
        "            x = self.convs[i](input)\n",
        "            x = self.activations[i](x)\n",
        "            x = self.lka_layers[i](x)\n",
        "            x = self.final_convs[i](x)\n",
        "            x = self.activations[i](x)\n",
        "            x = x + y\n",
        "            attn.append(x)\n",
        "\n",
        "        mh_lka_attn = attn[0]\n",
        "        for i in range(1, len(attn)):\n",
        "            mh_lka_attn = mh_lka_attn + attn[i]\n",
        "        return mh_lka_attn\n",
        "\n",
        "class DefVitBlock(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, strides, nl, nh, dm=1):\n",
        "        super(DefVitBlock, self).__init__()\n",
        "        self.defconv = DefConv(in_channels, fltr, strides, dm)\n",
        "        self.trans_enc = TransEnc(in_channels, fltr, nl, nh)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.defconv(input)\n",
        "        x = self.trans_enc(x)\n",
        "        return x\n",
        "\n",
        "class DefVit(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(DefVit, self).__init__()\n",
        "        self.stem = Stem()\n",
        "        self.stage1_block1 = DefVitBlock(32, 32, strides=1, nl=2, nh=3)\n",
        "        self.stage1_block2 = DefVitBlock(32, 32, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block1 = DefVitBlock(32, 64, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block2 = DefVitBlock(64, 64, strides=1, nl=2, nh=3)\n",
        "        self.stage3_block1 = DefVitBlock(64, 128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block2 = DefVitBlock(128, 128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block3 = DefVitBlock(128, 128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block4 = DefVitBlock(128, 128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block5 = DefVitBlock(128, 128, strides=1, nl=4, nh=3)\n",
        "        self.stage4_block1 = DefVitBlock(128, 128, strides=1, nl=2, nh=3)\n",
        "        self.stage4_block2 = DefVitBlock(128, 128, strides=1, nl=2, nh=3)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        print('After stage-0', x.shape)\n",
        "\n",
        "        x = self.stage1_block1(x)\n",
        "        x = self.stage1_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-1', x.shape)\n",
        "\n",
        "        x = self.stage2_block1(x)\n",
        "        x = self.stage2_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-2', x.shape)\n",
        "\n",
        "        x = self.stage3_block1(x)\n",
        "        x = self.stage3_block2(x)\n",
        "        x = self.stage3_block3(x)\n",
        "        x = self.stage3_block4(x)\n",
        "        x = self.stage3_block5(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-3', x.shape)\n",
        "\n",
        "        x = self.stage4_block1(x)\n",
        "        x = self.stage4_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=1)\n",
        "        print('After stage-4', x.shape)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "num_classes = 8\n",
        "model = DefVit(input_shape=(3, 224, 224), num_classes=num_classes)\n",
        "output = model(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0m3L1eCCOr-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}