{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d4Z0W122LmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83a08de-d317-42c5-f6b4-f20ed5d2bd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.17.0)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEBIE9z72LmY",
        "outputId": "e42fdbb5-c865-494e-de6a-da7b3b6968f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmaszhvR2LmY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "98e112ed-6c07-4f09-c8cc-13ab3034ecb4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cc35d27baf2d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, utils\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht481_PZ2LmZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntHewN6POVkj"
      },
      "outputs": [],
      "source": [
        "# pip install tensorflow==2.11.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o91Tp0x_sHTl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import statistics\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Dense, DepthwiseConv2D,AveragePooling2D, Concatenate, Dropout, Permute,Reshape,Lambda,Activation, Add,Multiply, MaxPooling2D, Conv2D, Flatten, BatchNormalization, GlobalAveragePooling2D,LayerNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16,ConvNeXtTiny,ResNet50, MobileNet, Xception, EfficientNetB0 , DenseNet169, DenseNet201, DenseNet121, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile\n",
        "from tensorflow.keras.optimizers.legacy import Adam, SGD\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import gc\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "\n",
        "from keras.models import load_model\n",
        "#from keras.models import Sequential\n",
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "base_data_dir = ''\n",
        "out_loc = os.path.join(base_data_dir, '/content/drive/MyDrive/')\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cDysEGQOVkl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-4YPF2w26ll"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhoPeOAex6Y5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EWX3Gd8r32a"
      },
      "outputs": [],
      "source": [
        "mag = '40'\n",
        "# benign = '/content/drive/MyDrive/BreakHis_data/40/B'\n",
        "# malignant = '/content/drive/MyDrive/BreakHis_data/40/M'\n",
        "A = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/A'\n",
        "F = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/F'\n",
        "PT = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/PT'\n",
        "TA = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/TA'\n",
        "DC = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/DC'\n",
        "LC = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/LC'\n",
        "MC = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/MC'\n",
        "PC = '/content/drive/MyDrive/Breast Cancer Project/IW/'+mag+'/PC'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNZkBeucr9Mh"
      },
      "outputs": [],
      "source": [
        "dirlist=[A, F, PT, TA, DC, LC, MC, PC]\n",
        "classes=['A','F', 'PT', 'TA', 'DC', 'LC', 'MC', 'PC']\n",
        "# dirlist=[benign, malignant]\n",
        "# classes=['B','M']\n",
        "filepaths=[]\n",
        "labels=[]\n",
        "for i,j in zip(dirlist, classes):\n",
        "    filelist=os.listdir(i)\n",
        "    for f in filelist:\n",
        "        filepath=os.path.join (i,f)\n",
        "        filepaths.append(filepath)\n",
        "        labels.append(j)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVgOHhygsAlZ"
      },
      "outputs": [],
      "source": [
        "Files=pd.Series(filepaths, name='filepaths')\n",
        "Label=pd.Series(labels, name='labels')\n",
        "df=pd.concat([Files,Label], axis=1)\n",
        "df=pd.DataFrame(np.array(df).reshape(len(filepaths),8), columns = ['filepaths', 'labels'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uQkt3MTsD_o"
      },
      "outputs": [],
      "source": [
        "print(df['labels'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfI-lh99sGr7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, train_size=0.70, random_state=0)\n",
        "#train_new, valid = train_test_split(train, train_size=0.90, random_state=0)\n",
        "\n",
        "print(f\"train set shape: {train.shape}\")\n",
        "print(f\"test set shape: {test.shape}\")\n",
        "#print(f\"validation set shape: {valid.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5LJCbZ2IooS"
      },
      "outputs": [],
      "source": [
        "# train = '/content/drive/MyDrive/fold5/40/train'\n",
        "# test = '/content/drive/MyDrive/fold5/40/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF_mNPBOsQPL"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, vertical_flip =True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1.0/255.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gl5R4EJsRbN"
      },
      "outputs": [],
      "source": [
        "train_gen = train_datagen.flow_from_dataframe(dataframe=train,\n",
        "                                              x_col = 'filepaths', y_col ='labels',\n",
        "                                              target_size = (224,224), batch_size = 8,\n",
        "                                              class_mode = 'categorical', shuffle = True)\n",
        "\n",
        "test_gen = test_datagen.flow_from_dataframe(test,\n",
        "                                            target_size = (224,224),   x_col = 'filepaths', y_col ='labels',\n",
        "                                             class_mode = 'categorical',\n",
        "                                            batch_size = 4, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJBdug_F2Lmc"
      },
      "outputs": [],
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "    smoothed_points = []\n",
        "    for point in points:\n",
        "        if smoothed_points:\n",
        "            previous = smoothed_points[-1]\n",
        "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "        else:\n",
        "            smoothed_points.append(point)\n",
        "    return smoothed_points\n",
        "\n",
        "def plotmodel(history,name):\n",
        "\n",
        "    acc = history.history['acc']\n",
        "    #val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    #val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(epochs,smooth_curve(acc))\n",
        "    #plt.plot(epochs,smooth_curve(val_acc))\n",
        "    plt.ylabel('acc')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_acc'], loc='upper left')\n",
        "    #plt.savefig('acc_'+name+'_'+mag+'_'+fold+'.png')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(epochs,smooth_curve(loss))\n",
        "    #plt.plot(epochs,smooth_curve(val_loss))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss'], loc='upper right')\n",
        "   # plt.savefig('loss_'+name+'_'+mag+'_'+fold+'.png')\n",
        "\n",
        "def label_smooth(y_true, y_pred):\n",
        "    y_true=((1-0.1)*y_true+0.05)\n",
        "    return K.categorical_crossentropy(y_true, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeIVRsgY2Lmc"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 3.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"green\" if cm[i, j] > thresh else \"red\", fontdict={'fontsize':'x-large'})\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQqoWozCTBhK"
      },
      "outputs": [],
      "source": [
        "loss_fun= 'categorical_crossentropy'\n",
        "gpu_num=1\n",
        "k=5\n",
        "lr1=0.005\n",
        "lr2=0.0001\n",
        "image_size=224\n",
        "classes=2\n",
        "ratio=8\n",
        "fold='fold5'\n",
        "mag='40'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDj2Er1H2Lmf"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_gen,test_gen,mag,save_name,lr1,lr2,Epochs1,Epochs2):\n",
        "\n",
        "    lr_decay=ReduceLROnPlateau(monitor='loss', factor=0.8, patience=3, verbose=1)\n",
        "    save_model=ModelCheckpoint('effnetB6/'+mag+'/'+save_name+'/'+save_name+'{epoch:02d}.h5', monitor='loss',period=10,save_best_only=True)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-5,decay=1e-6),loss=loss_fun,metrics=['acc'])\n",
        "    model.fit(train_gen,\n",
        "                        #steps_per_epoch=train_num/batch_size,\n",
        "                        #validation_data=test_data,\n",
        "                        #validation_steps=test_num/batch_size,\n",
        "                        epochs=Epochs1,\n",
        "                        workers=4,\n",
        "                        callbacks=[lr_decay,save_model])\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=lr2,decay=0.00001),loss=loss_fun,metrics=['acc'])\n",
        "    history = model.fit(train_gen,\n",
        "                        #steps_per_epoch=train_num/batch_size,\n",
        "                        #validation_data=test_data,\n",
        "                        #validation_steps=test_num/batch_size,\n",
        "                        epochs=Epochs2,\n",
        "                        workers=4,\n",
        "                        callbacks=[lr_decay,save_model])\n",
        "\n",
        "    results =model.evaluate(test_gen)\n",
        "    print('Test loss and accuracy: ',results)\n",
        "    predictions = model.predict(test_gen)\n",
        "    rounded_pred = np.argmax(predictions, axis=-1)\n",
        "    cm = confusion_matrix(y_true=test_gen.classes, y_pred=rounded_pred)\n",
        "    cm_plot_labels = ['A','F', 'PT', 'TA', 'DC', 'LC', 'MC', 'PC']\n",
        "    plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n",
        "    print(classification_report(y_true=test_gen.classes, y_pred=rounded_pred, target_names=cm_plot_labels))\n",
        "\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJuXLmmFz0G3"
      },
      "outputs": [],
      "source": [
        "def stem(input,stride):\n",
        "    x = Conv2D(32, 3, strides=stride, padding='same', activation='relu')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, 3, strides=1, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpsqYghjzgw3"
      },
      "outputs": [],
      "source": [
        "def lka(input,fltr,dm=1):\n",
        "    x = DepthwiseConv2D(5, strides=1, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = DepthwiseConv2D(7, strides=1, padding='same', depth_multiplier=dm, activation='relu',dilation_rate=3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(fltr,1,padding='same',activation='relu')(x)\n",
        "    x = Multiply()([x,input])\n",
        "    # return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAVDGGXqOVku"
      },
      "outputs": [],
      "source": [
        "def lka(input,fltr,dm=1):\n",
        "#     shape=K.int_shape(input)\n",
        "#     q=Conv2D(fltr,1,padding='same',activation='relu')(input)\n",
        "#     k=Conv2D(fltr,1,padding='same',activation='relu')(input)\n",
        "#     v=Conv2D(fltr,1,padding='same',activation='relu')(input)\n",
        "#   #q=Conv2D(fltr,3, padding=\"same\", activation='relu')(li)\n",
        "#   # dfc_li=Conv2D(1,1,padding='same',activation='relu')(li)\n",
        "#   # k=v=DefConv_full(dfc_li, filters=fltr, kernel_size=3, strides=1, name='def_conv'+str(i)) # Key and Value\n",
        "#   #   #print(q.shape)\n",
        "#     Qshape=K.int_shape(q)\n",
        "#     Kshape= K.int_shape(k)\n",
        "#     Vshape= K.int_shape(v)\n",
        "#     #print(Qshape,Kshape,Vshape)\n",
        "#     a=Qshape[1]*Qshape[2]\n",
        "#     q=Reshape((a,Qshape[3]))(q)\n",
        "#     #print(q.shape)\n",
        "#     k=Reshape((a,Kshape[3]))(k)\n",
        "#     k=Permute((2,1))(k)\n",
        "#     #print(k.shape)\n",
        "#     qk=tf.matmul(q,k)\n",
        "#     qk=Activation('softmax')(qk)\n",
        "#     #print(qk.shape)\n",
        "#     v=Reshape((a,Vshape[3]))(v)\n",
        "#     #print(v.shape)\n",
        "#     qkv=tf.matmul(qk,v)\n",
        "#     #print(qkv.shape)\n",
        "#     qkv=Activation('softmax')(qkv)\n",
        "#     qkv=Reshape((Vshape[1],Vshape[2],Vshape[3]))(qkv)\n",
        "  #print(qkv.shape)\n",
        "  #return qkv\n",
        "    x = DepthwiseConv2D(3, strides=1, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = DepthwiseConv2D(3, strides=1, padding='same', depth_multiplier=dm, activation='relu',dilation_rate=3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(fltr,1,padding='same',activation='relu')(x)\n",
        "    x = Multiply()([x,input])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_az0NUaOVku"
      },
      "outputs": [],
      "source": [
        "# def cascaded_lka_attn(input,fltr,nh):\n",
        "#     attn = []\n",
        "#     num_splits = nh\n",
        "#     split_size = input.shape[2] // num_splits\n",
        "\n",
        "#   # Split the tensor into smaller tensors along the channel dimension (axis=2)\n",
        "#     feature_split = tf.split(input, num_or_size_splits= num_splits, axis=2)\n",
        "\n",
        "#   # First head\n",
        "#     x = Conv2D(fltr, 1, strides=1, padding='same', activation=tf.nn.gelu)(feature_split[0])\n",
        "#     x = lka(x,fltr)\n",
        "#     x = Conv2D(fltr, 1, strides=1, padding='same', activation=tf.nn.gelu)(x)\n",
        "#     x = Add()([feature_split[0],x])\n",
        "#     attn.append(x)\n",
        "\n",
        "#     for i in range(1,nh):\n",
        "#         x = Add()([feature_split[i],x])\n",
        "#         x = Conv2D(fltr, 1, strides=1, padding='same', activation=tf.nn.gelu)(x)\n",
        "#         x = lka(x,fltr)\n",
        "#         x = Conv2D(fltr, 1, strides=1, padding='same', activation=tf.nn.gelu)(x)\n",
        "#         x = Add()([feature_split[i],x])\n",
        "#         attn.append(x)\n",
        "#     mh_lka_attn = Concatenate(axis=-1)(attn)\n",
        "#     mh_lka_attn = Conv2D(fltr,1, strides=1, padding='same', activation='relu')(mh_lka_attn)\n",
        "#     return mh_lka_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM8eHn6r5c1b"
      },
      "outputs": [],
      "source": [
        "def lka_attn(input,fltr,nh):\n",
        "    attn = []\n",
        "    act = ['relu',tf.nn.gelu,'sigmoid']\n",
        "    for i in range(nh):\n",
        "        x = Conv2D(fltr, 1, strides=1, padding='same', activation=act[i])(input)\n",
        "        x = lka(x,fltr)\n",
        "        x = Conv2D(fltr, 1, strides=1, padding='same', activation=act[i])(x)\n",
        "        attn.append(x)\n",
        "    mh_lka_attn = Add()([attn[0],attn[1],attn[2]])\n",
        "    return mh_lka_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSzjRxS4JFI"
      },
      "outputs": [],
      "source": [
        "def trans_enc(input,fltr,nl,nh):\n",
        "    x = input\n",
        "    for i in range(nl):\n",
        "        x1 = x\n",
        "        x = LayerNormalization(epsilon=1e-6)(x1)\n",
        "        x = lka_attn(x,fltr,nh)\n",
        "        x = Add()([x1,x])\n",
        "        y = LayerNormalization(epsilon=1e-6)(x)\n",
        "        y = Conv2D(fltr, 1, strides=1, padding='same', activation='relu')(y)\n",
        "        x = Add()([x,y])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3pF2dx-zSe4"
      },
      "outputs": [],
      "source": [
        "# def MSConv(input,fltr,strides,dm):\n",
        "#     x = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     shape = K.int_shape(x)\n",
        "\n",
        "#     y = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "#     y = BatchNormalization()(y)\n",
        "#     y = DepthwiseConv2D(3, padding='same',  depth_multiplier=fltr, activation='relu')(y)\n",
        "#     y = BatchNormalization()(y)\n",
        "\n",
        "#     z = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "#     z = BatchNormalization()(z)\n",
        "#     z = DepthwiseConv2D(3,  padding='same', depth_multiplier=dm, activation='relu')(z)\n",
        "#     z = BatchNormalization()(z)\n",
        "#     z = DepthwiseConv2D(3, padding='same', depth_multiplier=dm, activation='relu')(z)\n",
        "#     z = BatchNormalization()(z)\n",
        "\n",
        "#     yr = tf.image.resize(y, size=[shape[1], shape[2]], method=tf.image.ResizeMethod.BILINEAR)\n",
        "#     zr = tf.image.resize(z, size=[shape[1], shape[2]], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "#     ot = Concatenate()([x,yr,zr])\n",
        "#     ot = Conv2D(fltr, 1, padding='same', activation='relu')(ot)\n",
        "#     ot = BatchNormalization()(ot)\n",
        "#     inr = tf.image.resize(input, size=[ot.shape[1], ot.shape[2]], method=tf.image.ResizeMethod.BILINEAR)\n",
        "#     out = Add()([inr,ot])\n",
        "\n",
        "# #     out = DefConv_full(input, fltr,strides, kernel_size=3)\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9RO49OAtpH_"
      },
      "outputs": [],
      "source": [
        "def MSConv(input,fltr,strides,dm):\n",
        "    shape = K.int_shape(input)\n",
        "    x = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    y = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    z = DepthwiseConv2D(3, strides=strides, padding='same', depth_multiplier=dm, activation='relu')(y)\n",
        "    z = BatchNormalization()(z)\n",
        "\n",
        "    z = Concatenate()([x,y,z])\n",
        "    z = Conv2D(shape[3], 1, padding='same', activation='relu')(z)\n",
        "    z = Add()([input,z])\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4pQiZ5VbBpL"
      },
      "outputs": [],
      "source": [
        "def GAB(inputs):\n",
        "    shape=K.int_shape(inputs)\n",
        "    x=AveragePooling2D(pool_size=(shape[1],shape[2])) (inputs)\n",
        "    x=Conv2D(shape[3],1, padding='same') (x)\n",
        "    x=Activation('relu') (x)\n",
        "    x=Conv2D(shape[3],1, padding='same') (x)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    C_A=Multiply()([x,inputs])\n",
        "\n",
        "    x=Lambda(lambda x: K.mean(x,axis=-1,keepdims=True))  (C_A)\n",
        "    x=Activation('sigmoid') (x)\n",
        "    S_A=Multiply()([x,C_A])\n",
        "    return S_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oguuSiZ795lf"
      },
      "outputs": [],
      "source": [
        "def defconv(input,fltr,strides,dm=1):\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv2D(fltr, 1, padding='same', activation=tf.nn.gelu)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MSConv(x,fltr,strides,dm)\n",
        "    x = GAB(x)\n",
        "    x = Conv2D(fltr, 1, padding='same', activation=tf.nn.gelu)(x)\n",
        "\n",
        "\n",
        "    if strides==2:\n",
        "        input = MaxPooling2D()(input)\n",
        "        input = Conv2D(fltr, 1, padding='same', activation=tf.nn.gelu)(input)\n",
        "        x = Add()([input,x])\n",
        "    else:\n",
        "        input = Conv2D(fltr, 1, padding='same', activation=tf.nn.gelu)(input)\n",
        "        x = Add()([input,x])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1XDuxJPzSZD"
      },
      "outputs": [],
      "source": [
        "def Defvit_block(input, fltr,strides,nl,nh,dm=1):\n",
        "    x = defconv(input,fltr,strides,dm)\n",
        "    x = trans_enc(x,fltr,nl,nh)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHvuooLjzSTj"
      },
      "outputs": [],
      "source": [
        "def build_defvit(input_shape,classes):\n",
        "  # S-0 STEM stage\n",
        "    input = Input(input_shape)\n",
        "    x = stem(input,stride=2)\n",
        "#     x = stem(x,stride=1)\n",
        "    print('After stage-0',x.shape)\n",
        "\n",
        "  # S-1 stage\n",
        "    x = Defvit_block(x,32,strides=1,nl=2,nh=3)\n",
        "    x = Defvit_block(x,32,strides=1,nl=2,nh=3)\n",
        "    x = MaxPooling2D()(x)\n",
        "    print('After stage-1',x.shape)\n",
        "\n",
        "    # S-2 stage\n",
        "    x = Defvit_block(x,64,strides=1,nl=2,nh=3)\n",
        "    x = Defvit_block(x,64,strides=1,nl=2,nh=3)\n",
        "    x = MaxPooling2D()(x)\n",
        "    print('After stage-2',x.shape)\n",
        "\n",
        "    # S-3 stage\n",
        "    x = Defvit_block(x,128,strides=1,nl=4,nh=3)\n",
        "    x = Defvit_block(x,128,strides=1,nl=4,nh=3)\n",
        "    x = Defvit_block(x,128,strides=1,nl=4,nh=3)\n",
        "    x = Defvit_block(x,128,strides=1,nl=4,nh=3)\n",
        "    x = Defvit_block(x,128,strides=1,nl=4,nh=3)\n",
        "    x = MaxPooling2D()(x)\n",
        "    print('After stage-3',x.shape)\n",
        "\n",
        "    # S-4 stage\n",
        "\n",
        "    # x = Defvit_block(x,256,strides=1,nl=2,nh=3)\n",
        "    # x = Defvit_block(x,256,strides=1,nl=2,nh=3)\n",
        "    # x = MaxPooling2D()(x)\n",
        "    # print('After stage-4',x.shape)\n",
        "\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    out = Dense(classes,activation='softmax')(x)\n",
        "    model = Model(input,out)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqb54ydjnFAz"
      },
      "outputs": [],
      "source": [
        "model_vit = build_defvit((224,224,3),8)\n",
        "model_vit.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQEFMkYAzSQK"
      },
      "outputs": [],
      "source": [
        "lr_decay=ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n",
        "model_vit.compile(optimizer=Adam(lr=0.001,decay=0.00001),loss=loss_fun,metrics=['acc'])\n",
        "history = model_vit.fit(train_gen,\n",
        "                        #steps_per_epoch=train_num/batch_size,\n",
        "                        #validation_data=test_data,\n",
        "                        #validation_steps=test_num/batch_size,\n",
        "                        epochs=200,\n",
        "                        workers=4,\n",
        "                        callbacks=[lr_decay])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i7l8M1MzSK5"
      },
      "outputs": [],
      "source": [
        "results = model_vit.evaluate(test_gen)\n",
        "print('Test loss and accuracy: ',results)\n",
        "predictions = model_vit.predict(test_gen)\n",
        "rounded_pred = np.argmax(predictions, axis=-1)\n",
        "cm = confusion_matrix(y_true=test_gen.classes, y_pred=rounded_pred)\n",
        "cm_plot_labels = ['B','M']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n",
        "print(classification_report(y_true=test_gen.classes, y_pred=rounded_pred, target_names=cm_plot_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGpMYKP4zSCc"
      },
      "outputs": [],
      "source": [
        "plotmodel(history,'defvit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7flHr0FOVk8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}