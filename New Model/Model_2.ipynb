{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MTd7pe6avXrH",
        "outputId": "26033b97-de71-4b69-a6ea-a4214032636e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input of stem: torch.Size([1, 3, 224, 224])\n",
            "Shape of Output of stem: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 224, 224])\n",
            "Shape of x1 torch.Size([1, 32, 224, 224])\n",
            "Shape of x2 torch.Size([1, 32, 224, 224])\n",
            "Shape of x3 torch.Size([1, 32, 224, 224])\n",
            "Shape of Output of lka: torch.Size([1, 32, 224, 224])\n",
            "Input shape: torch.Size([1, 32, 224, 224])\n",
            "Output shape: torch.Size([1, 32, 224, 224])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n",
            "Shape of input of lka_attn: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of lka_attn: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n",
            "Shape of input of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of stem: torch.Size([1, 3, 224, 224])\n",
            "Shape of Output of stem: torch.Size([1, 32, 112, 112])\n",
            "After stage-0 torch.Size([1, 32, 112, 112])\n",
            "Shape of input of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of GAB: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of defconv: torch.Size([1, 32, 112, 112])\n",
            "Shape of input of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of Input of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of x1 torch.Size([1, 32, 112, 112])\n",
            "Shape of x2 torch.Size([1, 32, 112, 112])\n",
            "Shape of x3 torch.Size([1, 32, 112, 112])\n",
            "Shape of Output of lka: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of trans_enc: torch.Size([1, 32, 112, 112])\n",
            "Shape of output of DefVitBlock: torch.Size([1, 32, 112, 112])\n",
            "After stage-1 torch.Size([1, 32, 56, 56])\n",
            "Shape of input of DefVitBlock: torch.Size([1, 32, 56, 56])\n",
            "Shape of input of defconv: torch.Size([1, 32, 56, 56])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 32 elements not 64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-46fe098fc91c>\u001b[0m in \u001b[0;36m<cell line: 337>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefVit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46fe098fc91c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After stage-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46fe098fc91c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of input of DefVitBlock:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of output of DefVitBlock:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46fe098fc91c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of input of defconv:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2510\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 32 elements not 64"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "class Conv2dSamePadding(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Conv2dSamePadding, self).__init__(*args, **kwargs)\n",
        "        self.zero_pad_2d = nn.ZeroPad2d(functools.reduce(operator.__add__,\n",
        "                  [(k // 2, k // 2) for k in self.kernel_size[::-1]]))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self._conv_forward(self.zero_pad_2d(input), self.weight, self.bias)\n",
        "\n",
        "class DepthwiseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu'):\n",
        "        super(DepthwiseConv2d, self).__init__()\n",
        "\n",
        "        if padding == 'same':\n",
        "            self.depthwise_conv = Conv2dSamePadding(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                groups=in_channels\n",
        "            )\n",
        "        else:\n",
        "            self.depthwise_conv = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels * depth_multiplier,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                groups=in_channels\n",
        "            )\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'relu6':\n",
        "            self.activation = nn.ReLU6()\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise_conv(x)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class MSConv(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, strides, dm):\n",
        "        super(MSConv, self).__init__()\n",
        "        self.depthwise1 = DepthwiseConv2d(in_channels, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.depthwise2 = DepthwiseConv2d(in_channels * dm, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.depthwise3 = DepthwiseConv2d(in_channels * dm, depth_multiplier=dm, kernel_size=3, stride=strides, padding='same', activation='relu')\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels * dm)\n",
        "        self.conv = Conv2dSamePadding(in_channels * dm * 3, fltr, kernel_size=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.depthwise1(input)\n",
        "        x = self.bn1(x)\n",
        "        y = self.depthwise2(x)\n",
        "        y = self.bn2(y)\n",
        "        z = self.depthwise3(y)\n",
        "        z = self.bn3(z)\n",
        "        z = torch.cat([x, y, z], dim=1)\n",
        "        z = self.conv(z)\n",
        "        z = F.relu(z)\n",
        "        z = z + input\n",
        "        return z\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stem, self).__init__()\n",
        "        self.conv1 = Conv2dSamePadding(in_channels=3, out_channels=32, kernel_size=3, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = Conv2dSamePadding(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of Input of stem:\", x.shape)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        print(\"Shape of Output of stem:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "model = Stem()\n",
        "output = model(input_tensor)\n",
        "\n",
        "class LKA(nn.Module):\n",
        "    def __init__(self, in_channels, fltr, depth_multiplier=1):\n",
        "        super(LKA, self).__init__()\n",
        "        self.depthwise1 = DepthwiseConv2d(in_channels, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.depthwise2 = DepthwiseConv2d(in_channels * depth_multiplier, depth_multiplier=depth_multiplier, kernel_size=3, stride=1, padding='same', activation='relu')\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels * depth_multiplier)\n",
        "        self.conv = Conv2dSamePadding(in_channels * depth_multiplier, fltr, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of Input of lka:\", x.shape)\n",
        "        x1 = F.relu(self.bn1(self.depthwise1(x)))\n",
        "        print(\"Shape of x1\", x1.shape)\n",
        "        x2 = F.relu(self.bn2(self.depthwise2(x1)))\n",
        "        print(\"Shape of x2\", x2.shape)\n",
        "        x3 = F.relu(self.conv(x2))\n",
        "        print(\"Shape of x3\", x3.shape)\n",
        "        out = x * x3\n",
        "        print(\"Shape of Output of lka:\", out.shape)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(1, 32, 224, 224)\n",
        "model = LKA(in_channels=32, fltr=32, depth_multiplier=1)\n",
        "output = model(input_tensor)\n",
        "print(f\"Input shape: {input_tensor.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "class GAB(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(GAB, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv1 = Conv2dSamePadding(in_channels, in_channels, kernel_size=1)\n",
        "        self.conv2 = Conv2dSamePadding(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        print(\"Shape of Input of GAB:\", inputs.shape)\n",
        "        x = self.avg_pool(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        C_A = x * inputs\n",
        "        x = torch.mean(C_A, dim=1, keepdim=True)\n",
        "        x = torch.sigmoid(x)\n",
        "        S_A = x * C_A\n",
        "        print(\"Shape of Output of GAB:\", S_A.shape)\n",
        "        return S_A\n",
        "\n",
        "# Example usage:\n",
        "inputs = torch.randn(1, 32, 112, 112)\n",
        "model = GAB(in_channels=32)\n",
        "output = model(inputs)\n",
        "print(output.shape)\n",
        "\n",
        "class DefConv(nn.Module):\n",
        "    def __init__(self, fltr, strides, dm=1):\n",
        "        super(DefConv, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(fltr)\n",
        "        self.conv1 = Conv2dSamePadding(fltr, fltr, kernel_size=1)\n",
        "        self.msconv = MSConv(fltr, fltr, strides, dm)\n",
        "        self.gab = GAB(fltr)\n",
        "        self.conv2 = Conv2dSamePadding(fltr, fltr, kernel_size=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.strides = strides\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of defconv:\", input.shape)\n",
        "        x = self.bn1(input)\n",
        "        x = self.conv1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.msconv(x)\n",
        "        x = self.gab(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        if self.strides == 2:\n",
        "            input = self.pool(input)\n",
        "            input = self.conv2(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "        else:\n",
        "            input = self.conv2(input)\n",
        "            input = F.gelu(input)\n",
        "            x = x + input\n",
        "\n",
        "        print(\"Shape of output of defconv:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = DefConv(fltr=32, strides=1, dm=1)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n",
        "\n",
        "class TransEnc(nn.Module):\n",
        "    def __init__(self, fltr, nl, nh):\n",
        "        super(TransEnc, self).__init__()\n",
        "        self.nl = nl\n",
        "        self.norm1 = nn.LayerNorm(fltr)\n",
        "        self.norm2 = nn.LayerNorm(fltr)\n",
        "        self.conv = Conv2dSamePadding(fltr, fltr, kernel_size=1)\n",
        "        self.lka_attn = LKA(fltr, fltr)\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of trans_enc:\", input.shape)\n",
        "        x = input\n",
        "        for i in range(self.nl):\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "            x = self.norm1(x)\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "            x = self.lka_attn(x)\n",
        "            x = x + input\n",
        "            y = x.permute(0, 2, 3, 1)\n",
        "            y = self.norm2(y)\n",
        "            y = y.permute(0, 3, 1, 2)\n",
        "            y = self.conv(y)\n",
        "            y = F.relu(y)\n",
        "            x = x + y\n",
        "\n",
        "        print(\"Shape of output of trans_enc:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = TransEnc(fltr=32, nl=3, nh=1)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n",
        "\n",
        "class LKA_Attn(nn.Module):\n",
        "    def __init__(self, fltr, nh):\n",
        "        super(LKA_Attn, self).__init__()\n",
        "        self.nh = nh\n",
        "        self.convs = nn.ModuleList([\n",
        "            Conv2dSamePadding(fltr, fltr, kernel_size=1) for _ in range(nh)\n",
        "        ])\n",
        "        self.activations = [F.relu, F.gelu, torch.sigmoid]\n",
        "        self.lka_layers = nn.ModuleList([LKA(fltr, fltr) for _ in range(nh)])\n",
        "        self.final_convs = nn.ModuleList([\n",
        "            Conv2dSamePadding(fltr, fltr, kernel_size=1) for _ in range(nh)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of lka_attn:\", input.shape)\n",
        "        attn = []\n",
        "        y = input\n",
        "        for i in range(self.nh):\n",
        "            x = self.convs[i](input)\n",
        "            x = self.activations[i](x)\n",
        "            x = self.lka_layers[i](x)\n",
        "            x = self.final_convs[i](x)\n",
        "            x = self.activations[i](x)\n",
        "            x = x + y\n",
        "            attn.append(x)\n",
        "\n",
        "        mh_lka_attn = attn[0]\n",
        "        for i in range(1, len(attn)):\n",
        "            mh_lka_attn = mh_lka_attn + attn[i]\n",
        "\n",
        "        print(\"Shape of output of lka_attn:\", mh_lka_attn.shape)\n",
        "        return mh_lka_attn\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = LKA_Attn(fltr=32, nh=3)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n",
        "\n",
        "class DefVitBlock(nn.Module):\n",
        "    def __init__(self, fltr, strides, nl, nh, dm=1):\n",
        "        super(DefVitBlock, self).__init__()\n",
        "        self.defconv = DefConv(fltr, strides, dm)\n",
        "        self.trans_enc = TransEnc(fltr, nl, nh)\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(\"Shape of input of DefVitBlock:\", input.shape)\n",
        "        x = self.defconv(input)\n",
        "        x = self.trans_enc(x)\n",
        "        print(\"Shape of output of DefVitBlock:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 32, 112, 112)\n",
        "model = DefVitBlock(fltr=32, strides=1, nl=1, nh=1)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n",
        "\n",
        "class DefVit(nn.Module):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        super(DefVit, self).__init__()\n",
        "        self.stem = Stem()\n",
        "        self.stage1_block1 = DefVitBlock(32, strides=1, nl=2, nh=3)\n",
        "        self.stage1_block2 = DefVitBlock(32, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block1 = DefVitBlock(64, strides=1, nl=2, nh=3)\n",
        "        self.stage2_block2 = DefVitBlock(64, strides=1, nl=2, nh=3)\n",
        "        self.stage3_block1 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block2 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block3 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block4 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage3_block5 = DefVitBlock(128, strides=1, nl=4, nh=3)\n",
        "        self.stage4_block1 = DefVitBlock(256, strides=1, nl=2, nh=3)\n",
        "        self.stage4_block2 = DefVitBlock(256, strides=1, nl=2, nh=3)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        print('After stage-0', x.shape)\n",
        "\n",
        "        x = self.stage1_block1(x)\n",
        "        x = self.stage1_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-1', x.shape)\n",
        "\n",
        "        x = self.stage2_block1(x)\n",
        "        x = self.stage2_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-2', x.shape)\n",
        "\n",
        "        x = self.stage3_block1(x)\n",
        "        x = self.stage3_block2(x)\n",
        "        x = self.stage3_block3(x)\n",
        "        x = self.stage3_block4(x)\n",
        "        x = self.stage3_block5(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-3', x.shape)\n",
        "\n",
        "        x = self.stage4_block1(x)\n",
        "        x = self.stage4_block2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        print('After stage-4', x.shape)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "num_classes = 8\n",
        "model = DefVit(input_shape=(3, 224, 224), num_classes=num_classes)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)\n"
      ]
    }
  ]
}