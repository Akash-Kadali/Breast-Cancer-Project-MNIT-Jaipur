{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Breast Cancer Project/new_master_dataset.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "reWGdqaeA4R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train_100 = pd.read_csv(\"/content/drive/MyDrive/Breast Cancer Project/Mag100/train_df_100.csv\")\n",
        "# df_test_100 = pd.read_csv(\"/content/drive/MyDrive/Breast Cancer Project/Mag100/test_df_100.csv\")\n",
        "# df_val_100 = pd.read_csv(\"/content/drive/MyDrive/Breast Cancer Project/Mag100/val_df_100.csv\")"
      ],
      "metadata": {
        "id": "_P4q8DLy9zFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define constants\n",
        "data_dirs = [\"TA\", \"PT\", \"PC\", \"MC\", \"LC\", \"F\", \"DC\", \"A\"]\n",
        "data_root = \"/content/drive/MyDrive/Breast Cancer Project/IW/100\"  # Replace with the root directory of your data\n",
        "train_split = 0.7\n",
        "\n",
        "# Create a list to store the paths and labels of all images\n",
        "all_data = []\n",
        "\n",
        "# Populate the list with paths and labels\n",
        "for label, folder in enumerate(data_dirs):\n",
        "    folder_path = os.path.join(data_root, folder)\n",
        "    image_files = os.listdir(folder_path)\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        all_data.append((image_path, label))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = train_test_split(all_data, train_size=train_split, shuffle=True, random_state=42)\n",
        "\n",
        "# Define custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        img = Image.open(img_path).convert('RGB')  # Open image and convert to RGB mode\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
        "        return img, label_tensor\n",
        "\n",
        "# Image preprocessing with augmentation for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((700, 460)),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Image preprocessing without augmentation for testing and validation\n",
        "test_val_transform = transforms.Compose([\n",
        "    transforms.Resize((700, 460)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create custom datasets\n",
        "train_dataset = CustomDataset(train_data, transform=train_transform)\n",
        "test_dataset = CustomDataset(test_data, transform=test_val_transform)\n",
        "\n",
        "# DataLoaders for batching and shuffling\n",
        "batch_size = 2 # Define the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load model directly\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/efficientnet-b6\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"google/efficientnet-b6\").to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        # Ensure the input tensor is passed correctly\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        progress_bar.set_postfix({'Loss': train_loss / total, 'Accuracy': 100 * correct / total})\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_accuracy = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(test_loader.dataset)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step(val_loss)"
      ],
      "metadata": {
        "id": "mApVVh3M_jcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Dictionary to store accuracies for each patient ID\n",
        "accuracy_per_patient = defaultdict(list)\n",
        "\n",
        "# Iterate through the images in the folder\n",
        "for image_file in image_files:\n",
        "    # Extract the 5-digit patient ID from the filename\n",
        "    patient_id = image_file.split('-')[2]  # Assuming patient ID is the third part split by '-'\n",
        "\n",
        "    # Load the image and preprocess it if needed\n",
        "    image = load_and_preprocess_image(image_file)\n",
        "\n",
        "    # Perform inference to get the predicted label for the image using the model\n",
        "    predicted_label = model.predict(image)\n",
        "\n",
        "    # Get the actual label from the filename or elsewhere if available\n",
        "    actual_label = get_actual_label(image_file)\n",
        "\n",
        "    # Check if the predicted label matches the actual label\n",
        "    is_correct = predicted_label == actual_label\n",
        "\n",
        "    # Update the accuracy for this patient ID\n",
        "    accuracy_per_patient[patient_id].append(is_correct)\n",
        "\n",
        "# Calculate accuracy for each patient ID\n",
        "average_accuracy = 0.0\n",
        "for patient_id, accuracies in accuracy_per_patient.items():\n",
        "    accuracy = sum(accuracies) / len(accuracies)\n",
        "    print(f\"Accuracy for patient ID {patient_id}: {accuracy:.2f}\")\n",
        "    average_accuracy += accuracy\n",
        "\n",
        "# Calculate average accuracy across all patient IDs\n",
        "average_accuracy /= len(accuracy_per_patient)\n",
        "print(f\"Average accuracy across all patient IDs: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "96Qy0EDxAvTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}